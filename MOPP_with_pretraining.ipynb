{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2996e99b",
   "metadata": {
    "papermill": {
     "duration": 0.009971,
     "end_time": "2025-01-17T21:28:55.080061",
     "exception": false,
     "start_time": "2025-01-17T21:28:55.070090",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **0. Install Mujoco Physics Engine on the Kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5144a9ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:28:55.099072Z",
     "iopub.status.busy": "2025-01-17T21:28:55.098746Z",
     "iopub.status.idle": "2025-01-17T21:29:27.314301Z",
     "shell.execute_reply": "2025-01-17T21:29:27.313241Z"
    },
    "papermill": {
     "duration": 32.226892,
     "end_time": "2025-01-17T21:29:27.316017",
     "exception": false,
     "start_time": "2025-01-17T21:28:55.089125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "software-properties-common is already the newest version (0.99.22.9).\r\n",
      "libgl1-mesa-glx is already the newest version (23.0.4-0ubuntu1~22.04.1).\r\n",
      "The following additional packages will be installed:\r\n",
      "  libegl-dev libgl-dev libgles-dev libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev\r\n",
      "  libglvnd-dev libglx-dev libopengl-dev libosmesa6\r\n",
      "The following NEW packages will be installed:\r\n",
      "  libegl-dev libgl-dev libgl1-mesa-dev libgles-dev libgles1 libglew-dev libglu1-mesa\r\n",
      "  libglu1-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev libopengl-dev libosmesa6\r\n",
      "  libosmesa6-dev\r\n",
      "0 upgraded, 14 newly installed, 0 to remove and 62 not upgraded.\r\n",
      "Need to get 4,008 kB of archives.\r\n",
      "After this operation, 19.3 MB of additional disk space will be used.\r\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\r\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\r\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\r\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\r\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\r\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\r\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\r\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\r\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dev amd64 23.2.1-1ubuntu3.1~22.04.3 [6,848 B]\r\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\r\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\r\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libglew-dev amd64 2.2.0-4 [287 kB]\r\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libosmesa6 amd64 23.2.1-1ubuntu3.1~22.04.3 [3,115 kB]\r\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libosmesa6-dev amd64 23.2.1-1ubuntu3.1~22.04.3 [8,984 B]\r\n",
      "Fetched 4,008 kB in 1s (5,710 kB/s)\r\n",
      "Selecting previously unselected package libglx-dev:amd64.\r\n",
      "(Reading database ... 127400 files and directories currently installed.)\r\n",
      "Preparing to unpack .../00-libglx-dev_1.4.0-1_amd64.deb ...\r\n",
      "Unpacking libglx-dev:amd64 (1.4.0-1) ...\r\n",
      "Selecting previously unselected package libgl-dev:amd64.\r\n",
      "Preparing to unpack .../01-libgl-dev_1.4.0-1_amd64.deb ...\r\n",
      "Unpacking libgl-dev:amd64 (1.4.0-1) ...\r\n",
      "Selecting previously unselected package libegl-dev:amd64.\r\n",
      "Preparing to unpack .../02-libegl-dev_1.4.0-1_amd64.deb ...\r\n",
      "Unpacking libegl-dev:amd64 (1.4.0-1) ...\r\n",
      "Selecting previously unselected package libgles1:amd64.\r\n",
      "Preparing to unpack .../03-libgles1_1.4.0-1_amd64.deb ...\r\n",
      "Unpacking libgles1:amd64 (1.4.0-1) ...\r\n",
      "Selecting previously unselected package libgles-dev:amd64.\r\n",
      "Preparing to unpack .../04-libgles-dev_1.4.0-1_amd64.deb ...\r\n",
      "Unpacking libgles-dev:amd64 (1.4.0-1) ...\r\n",
      "Selecting previously unselected package libglvnd-core-dev:amd64.\r\n",
      "Preparing to unpack .../05-libglvnd-core-dev_1.4.0-1_amd64.deb ...\r\n",
      "Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\r\n",
      "Selecting previously unselected package libopengl-dev:amd64.\r\n",
      "Preparing to unpack .../06-libopengl-dev_1.4.0-1_amd64.deb ...\r\n",
      "Unpacking libopengl-dev:amd64 (1.4.0-1) ...\r\n",
      "Selecting previously unselected package libglvnd-dev:amd64.\r\n",
      "Preparing to unpack .../07-libglvnd-dev_1.4.0-1_amd64.deb ...\r\n",
      "Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\r\n",
      "Selecting previously unselected package libgl1-mesa-dev:amd64.\r\n",
      "Preparing to unpack .../08-libgl1-mesa-dev_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\r\n",
      "Unpacking libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\r\n",
      "Selecting previously unselected package libglu1-mesa:amd64.\r\n",
      "Preparing to unpack .../09-libglu1-mesa_9.0.2-1_amd64.deb ...\r\n",
      "Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\r\n",
      "Selecting previously unselected package libglu1-mesa-dev:amd64.\r\n",
      "Preparing to unpack .../10-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\r\n",
      "Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\r\n",
      "Selecting previously unselected package libglew-dev:amd64.\r\n",
      "Preparing to unpack .../11-libglew-dev_2.2.0-4_amd64.deb ...\r\n",
      "Unpacking libglew-dev:amd64 (2.2.0-4) ...\r\n",
      "Selecting previously unselected package libosmesa6:amd64.\r\n",
      "Preparing to unpack .../12-libosmesa6_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\r\n",
      "Unpacking libosmesa6:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\r\n",
      "Selecting previously unselected package libosmesa6-dev:amd64.\r\n",
      "Preparing to unpack .../13-libosmesa6-dev_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\r\n",
      "Unpacking libosmesa6-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\r\n",
      "Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\r\n",
      "Setting up libgles1:amd64 (1.4.0-1) ...\r\n",
      "Setting up libglx-dev:amd64 (1.4.0-1) ...\r\n",
      "Setting up libglu1-mesa:amd64 (9.0.2-1) ...\r\n",
      "Setting up libopengl-dev:amd64 (1.4.0-1) ...\r\n",
      "Setting up libgl-dev:amd64 (1.4.0-1) ...\r\n",
      "Setting up libosmesa6:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\r\n",
      "Setting up libegl-dev:amd64 (1.4.0-1) ...\r\n",
      "Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\r\n",
      "Setting up libosmesa6-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\r\n",
      "Setting up libgles-dev:amd64 (1.4.0-1) ...\r\n",
      "Setting up libglvnd-dev:amd64 (1.4.0-1) ...\r\n",
      "Setting up libglew-dev:amd64 (2.2.0-4) ...\r\n",
      "Setting up libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\r\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "The following NEW packages will be installed:\r\n",
      "  patchelf\r\n",
      "0 upgraded, 1 newly installed, 0 to remove and 62 not upgraded.\r\n",
      "Need to get 72.1 kB of archives.\r\n",
      "After this operation, 186 kB of additional disk space will be used.\r\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 patchelf amd64 0.14.3-1 [72.1 kB]\r\n",
      "Fetched 72.1 kB in 1s (117 kB/s)\r\n",
      "Selecting previously unselected package patchelf.\r\n",
      "(Reading database ... 127534 files and directories currently installed.)\r\n",
      "Preparing to unpack .../patchelf_0.14.3-1_amd64.deb ...\r\n",
      "Unpacking patchelf (0.14.3-1) ...\r\n",
      "Setting up patchelf (0.14.3-1) ...\r\n",
      "Processing triggers for man-db (2.10.2-1) ...\r\n",
      "Collecting free-mujoco-py\r\n",
      "  Downloading free_mujoco_py-2.1.6-py3-none-any.whl.metadata (586 bytes)\r\n",
      "Collecting Cython<0.30.0,>=0.29.24 (from free-mujoco-py)\r\n",
      "  Downloading Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (3.1 kB)\r\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (1.17.1)\r\n",
      "Collecting fasteners==0.15 (from free-mujoco-py)\r\n",
      "  Downloading fasteners-0.15-py2.py3-none-any.whl.metadata (2.6 kB)\r\n",
      "Collecting glfw<2.0.0,>=1.4.0 (from free-mujoco-py)\r\n",
      "  Downloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl.metadata (4.3 kB)\r\n",
      "Requirement already satisfied: imageio<3.0.0,>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (2.36.1)\r\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (1.26.4)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fasteners==0.15->free-mujoco-py) (1.17.0)\r\n",
      "Collecting monotonic>=0.1 (from fasteners==0.15->free-mujoco-py)\r\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi<2.0.0,>=1.15.0->free-mujoco-py) (2.22)\r\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0.0,>=2.9.0->free-mujoco-py) (11.0.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.21.3->free-mujoco-py) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.21.3->free-mujoco-py) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.21.3->free-mujoco-py) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.21.3->free-mujoco-py) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.21.3->free-mujoco-py) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.21.3->free-mujoco-py) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0,>=1.21.3->free-mujoco-py) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0,>=1.21.3->free-mujoco-py) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.0.0,>=1.21.3->free-mujoco-py) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.0.0,>=1.21.3->free-mujoco-py) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.0.0,>=1.21.3->free-mujoco-py) (2024.2.0)\r\n",
      "Downloading free_mujoco_py-2.1.6-py3-none-any.whl (14.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading fasteners-0.15-py2.py3-none-any.whl (23 kB)\r\n",
      "Downloading Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (203 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\r\n",
      "Installing collected packages: monotonic, glfw, fasteners, Cython, free-mujoco-py\r\n",
      "  Attempting uninstall: Cython\r\n",
      "    Found existing installation: Cython 3.0.11\r\n",
      "    Uninstalling Cython-3.0.11:\r\n",
      "      Successfully uninstalled Cython-3.0.11\r\n",
      "Successfully installed Cython-0.29.37 fasteners-0.15 free-mujoco-py-2.1.6 glfw-1.12.0 monotonic-1.6\r\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y \\\n",
    "   libgl1-mesa-dev \\\n",
    "   libgl1-mesa-glx \\\n",
    "   libglew-dev \\\n",
    "   libosmesa6-dev \\\n",
    "   software-properties-common\n",
    "\n",
    "!apt-get install -y patchelf \n",
    "\n",
    "!pip install free-mujoco-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a7d52c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:29:27.346746Z",
     "iopub.status.busy": "2025-01-17T21:29:27.346490Z",
     "iopub.status.idle": "2025-01-17T21:30:40.585889Z",
     "shell.execute_reply": "2025-01-17T21:30:40.585179Z"
    },
    "papermill": {
     "duration": 73.256374,
     "end_time": "2025-01-17T21:30:40.587613",
     "exception": false,
     "start_time": "2025-01-17T21:29:27.331239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\r\n",
      "Selecting previously unselected package libglfw3:amd64.\r\n",
      "(Reading database ... 127540 files and directories currently installed.)\r\n",
      "Preparing to unpack .../libglfw3_3.3.6-1_amd64.deb ...\r\n",
      "Unpacking libglfw3:amd64 (3.3.6-1) ...\r\n",
      "Setting up libglfw3:amd64 (3.3.6-1) ...\r\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\r\n",
      "\r\n",
      "Collecting mujoco-py<2.2,>=2.1\r\n",
      "  Downloading mujoco_py-2.1.2.14-py3-none-any.whl.metadata (669 bytes)\r\n",
      "Requirement already satisfied: glfw>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mujoco-py<2.2,>=2.1) (1.12.0)\r\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.10/dist-packages (from mujoco-py<2.2,>=2.1) (1.26.4)\r\n",
      "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.10/dist-packages (from mujoco-py<2.2,>=2.1) (0.29.37)\r\n",
      "Requirement already satisfied: imageio>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from mujoco-py<2.2,>=2.1) (2.36.1)\r\n",
      "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.10/dist-packages (from mujoco-py<2.2,>=2.1) (1.17.1)\r\n",
      "Requirement already satisfied: fasteners~=0.15 in /usr/local/lib/python3.10/dist-packages (from mujoco-py<2.2,>=2.1) (0.15)\r\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1) (2.22)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fasteners~=0.15->mujoco-py<2.2,>=2.1) (1.17.0)\r\n",
      "Requirement already satisfied: monotonic>=0.1 in /usr/local/lib/python3.10/dist-packages (from fasteners~=0.15->mujoco-py<2.2,>=2.1) (1.6)\r\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.1.2->mujoco-py<2.2,>=2.1) (11.0.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.11->mujoco-py<2.2,>=2.1) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.11->mujoco-py<2.2,>=2.1) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.11->mujoco-py<2.2,>=2.1) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.11->mujoco-py<2.2,>=2.1) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.11->mujoco-py<2.2,>=2.1) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.11->mujoco-py<2.2,>=2.1) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.11->mujoco-py<2.2,>=2.1) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.11->mujoco-py<2.2,>=2.1) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.11->mujoco-py<2.2,>=2.1) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.11->mujoco-py<2.2,>=2.1) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.11->mujoco-py<2.2,>=2.1) (2024.2.0)\r\n",
      "Downloading mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: mujoco-py\r\n",
      "Successfully installed mujoco-py-2.1.2.14\r\n",
      "Compiling /usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.pyx because it depends on /usr/local/lib/python3.10/dist-packages/mujoco_py/pxd/mujoco.pxd.\n",
      "[1/1] Cythonizing /usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.pyx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#Include this at the top of your colab code\n",
    "if not os.path.exists('.mujoco_setup_complete'):\n",
    "    # Get the prereqs\n",
    "    !apt-get -qq update\n",
    "    !apt-get -qq install -y libosmesa6-dev libgl1-mesa-glx libglfw3 libgl1-mesa-dev libglew-dev patchelf\n",
    "    # Get Mujoco\n",
    "    !mkdir ~/.mujoco\n",
    "    !wget -q https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O mujoco.tar.gz\n",
    "    !tar -zxf mujoco.tar.gz -C \"$HOME/.mujoco\"\n",
    "    !rm mujoco.tar.gz\n",
    "    # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
    "    !echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/.mujoco/mujoco210/bin' >> ~/.bashrc \n",
    "    !echo 'export LD_PRELOAD=$LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libGLEW.so' >> ~/.bashrc \n",
    "    # THE ANNOYING ONE, FORCE IT INTO LDCONFIG SO WE ACTUALLY GET ACCESS TO IT THIS SESSION\n",
    "    !echo \"/root/.mujoco/mujoco210/bin\" > /etc/ld.so.conf.d/mujoco_ld_lib_path.conf\n",
    "    !ldconfig\n",
    "    # Install Mujoco-py\n",
    "    !pip3 install -U 'mujoco-py<2.2,>=2.1'\n",
    "    # run once\n",
    "    !touch .mujoco_setup_complete\n",
    "\n",
    "try:\n",
    "    if _mujoco_run_once:\n",
    "        pass\n",
    "except NameError:\n",
    "    _mujoco_run_once = False\n",
    "if not _mujoco_run_once:\n",
    "  # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
    "    try:\n",
    "        os.environ['LD_LIBRARY_PATH']=os.environ['LD_LIBRARY_PATH'] + ':/root/.mujoco/mujoco210/bin'\n",
    "    except KeyError:\n",
    "        os.environ['LD_LIBRARY_PATH']='/root/.mujoco/mujoco210/bin'\n",
    "    try:\n",
    "        os.environ['LD_PRELOAD']=os.environ['LD_PRELOAD'] + ':/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
    "    except KeyError:\n",
    "        os.environ['LD_PRELOAD']='/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
    "    # presetup so we don't see output on first env initialization\n",
    "    import mujoco_py\n",
    "    _mujoco_run_once = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4f29930",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:30:40.619607Z",
     "iopub.status.busy": "2025-01-17T21:30:40.619360Z",
     "iopub.status.idle": "2025-01-17T21:31:07.107317Z",
     "shell.execute_reply": "2025-01-17T21:31:07.106228Z"
    },
    "papermill": {
     "duration": 26.505266,
     "end_time": "2025-01-17T21:31:07.109009",
     "exception": false,
     "start_time": "2025-01-17T21:30:40.603743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting d4rl\r\n",
      "  Cloning https://github.com/Farama-Foundation/d4rl (to revision master) to /tmp/pip-install-buhkkfxm/d4rl_66895ebb661a4ac5b1fa57f9f1f079d0\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/Farama-Foundation/d4rl /tmp/pip-install-buhkkfxm/d4rl_66895ebb661a4ac5b1fa57f9f1f079d0\r\n",
      "  Resolved https://github.com/Farama-Foundation/d4rl to commit 89141a689b0353b0dac3da5cba60da4b1b16254d\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Collecting mjrl@ git+https://github.com/aravindr93/mjrl@master#egg=mjrl (from d4rl)\r\n",
      "  Cloning https://github.com/aravindr93/mjrl (to revision master) to /tmp/pip-install-buhkkfxm/mjrl_84e4eff5507d4e7cb9deae38faab630c\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/aravindr93/mjrl /tmp/pip-install-buhkkfxm/mjrl_84e4eff5507d4e7cb9deae38faab630c\r\n",
      "  Resolved https://github.com/aravindr93/mjrl to commit 3871d93763d3b49c4741e6daeaebbc605fe140dc\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Collecting gym<0.24.0 (from d4rl)\r\n",
      "  Downloading gym-0.23.1.tar.gz (626 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.2/626.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from d4rl) (1.26.4)\r\n",
      "Requirement already satisfied: mujoco_py in /usr/local/lib/python3.10/dist-packages (from d4rl) (2.1.2.14)\r\n",
      "Collecting pybullet (from d4rl)\r\n",
      "  Downloading pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\r\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from d4rl) (3.12.1)\r\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from d4rl) (2.5.0)\r\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from d4rl) (8.1.7)\r\n",
      "Collecting dm_control>=1.0.3 (from d4rl)\r\n",
      "  Downloading dm_control-1.0.27-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->d4rl) (1.4.0)\r\n",
      "Collecting dm-env (from dm_control>=1.0.3->d4rl)\r\n",
      "  Downloading dm_env-1.6-py3-none-any.whl.metadata (966 bytes)\r\n",
      "Requirement already satisfied: dm-tree!=0.1.2 in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->d4rl) (0.1.8)\r\n",
      "Requirement already satisfied: glfw in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->d4rl) (1.12.0)\r\n",
      "Collecting labmaze (from dm_control>=1.0.3->d4rl)\r\n",
      "  Downloading labmaze-1.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (278 bytes)\r\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->d4rl) (5.3.0)\r\n",
      "Collecting mujoco>=3.2.7 (from dm_control>=1.0.3->d4rl)\r\n",
      "  Downloading mujoco-3.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.19.4 in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->d4rl) (3.20.3)\r\n",
      "Requirement already satisfied: pyopengl>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->d4rl) (3.1.7)\r\n",
      "Requirement already satisfied: pyparsing>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->d4rl) (3.2.0)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->d4rl) (2.32.3)\r\n",
      "Requirement already satisfied: setuptools!=50.0.0 in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->d4rl) (75.1.0)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->d4rl) (1.13.1)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->d4rl) (4.67.1)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym<0.24.0->d4rl) (3.1.0)\r\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<0.24.0->d4rl) (0.0.8)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->d4rl) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->d4rl) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->d4rl) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->d4rl) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->d4rl) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->d4rl) (2.4.1)\r\n",
      "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.10/dist-packages (from mujoco_py->d4rl) (0.29.37)\r\n",
      "Requirement already satisfied: imageio>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from mujoco_py->d4rl) (2.36.1)\r\n",
      "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.10/dist-packages (from mujoco_py->d4rl) (1.17.1)\r\n",
      "Requirement already satisfied: fasteners~=0.15 in /usr/local/lib/python3.10/dist-packages (from mujoco_py->d4rl) (0.15)\r\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.10->mujoco_py->d4rl) (2.22)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fasteners~=0.15->mujoco_py->d4rl) (1.17.0)\r\n",
      "Requirement already satisfied: monotonic>=0.1 in /usr/local/lib/python3.10/dist-packages (from fasteners~=0.15->mujoco_py->d4rl) (1.6)\r\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.1.2->mujoco_py->d4rl) (11.0.0)\r\n",
      "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco>=3.2.7->dm_control>=1.0.3->d4rl) (1.11.0)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->d4rl) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->d4rl) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->d4rl) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->d4rl) (2024.2.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dm_control>=1.0.3->d4rl) (3.4.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dm_control>=1.0.3->d4rl) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dm_control>=1.0.3->d4rl) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dm_control>=1.0.3->d4rl) (2024.12.14)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->d4rl) (2024.2.0)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=3.2.7->dm_control>=1.0.3->d4rl) (2024.9.0)\r\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=3.2.7->dm_control>=1.0.3->d4rl) (6.4.5)\r\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=3.2.7->dm_control>=1.0.3->d4rl) (4.12.2)\r\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=3.2.7->dm_control>=1.0.3->d4rl) (3.21.0)\r\n",
      "Downloading dm_control-1.0.27-py3-none-any.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading mujoco-3.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading dm_env-1.6-py3-none-any.whl (26 kB)\r\n",
      "Downloading labmaze-1.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: d4rl, gym, mjrl\r\n",
      "  Building wheel for d4rl (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for d4rl: filename=D4RL-1.1-py3-none-any.whl size=26431548 sha256=bedc47e33eacb579b0ab8472f7e2857943f57f420a4b44035b2f15aeb42a539c\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-k_kl3wjr/wheels/7a/6e/e1/e5d8e3f6d89271fbc022dc955bb64360665044214be4e17251\r\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for gym: filename=gym-0.23.1-py3-none-any.whl size=701427 sha256=ee09b1ac2b79a8a07431a5a504f239e4ca0a3db43b7a2f03560e59bb81a06211\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/1a/00/fb/fe5cf2860fb9b7bc860e28f00095a1f42c7b726dd6f42d1acc\r\n",
      "  Building wheel for mjrl (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for mjrl: filename=mjrl-1.0.0-py3-none-any.whl size=61936 sha256=28703c9ba4a7d7845631367cc9ce5bd9e455389af7a04c4578b5ebeaaad91d6b\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-k_kl3wjr/wheels/8f/99/f9/efd223b38d503df5eaada10ffe96a869fb0c0f3c92d9e43ed0\r\n",
      "Successfully built d4rl gym mjrl\r\n",
      "Installing collected packages: pybullet, mjrl, mujoco, labmaze, dm-env, gym, dm_control, d4rl\r\n",
      "  Attempting uninstall: gym\r\n",
      "    Found existing installation: gym 0.25.2\r\n",
      "    Uninstalling gym-0.25.2:\r\n",
      "      Successfully uninstalled gym-0.25.2\r\n",
      "Successfully installed d4rl-1.1 dm-env-1.6 dm_control-1.0.27 gym-0.23.1 labmaze-1.0.6 mjrl-1.0.0 mujoco-3.2.7 pybullet-3.2.6\r\n"
     ]
    }
   ],
   "source": [
    "# intall d4rl github repo\n",
    "!pip install git+https://github.com/Farama-Foundation/d4rl@master#egg=d4rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e835ba14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:07.146826Z",
     "iopub.status.busy": "2025-01-17T21:31:07.146575Z",
     "iopub.status.idle": "2025-01-17T21:31:18.910932Z",
     "shell.execute_reply": "2025-01-17T21:31:18.909837Z"
    },
    "papermill": {
     "duration": 11.784873,
     "end_time": "2025-01-17T21:31:18.912776",
     "exception": false,
     "start_time": "2025-01-17T21:31:07.127903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.0)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\r\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.0->gymnasium) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.0->gymnasium) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.0->gymnasium) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.0->gymnasium) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.0->gymnasium) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.0->gymnasium) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.0->gymnasium) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.0->gymnasium) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.0->gymnasium) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.21.0->gymnasium) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.21.0->gymnasium) (2024.2.0)\r\n",
      "Collecting minari\r\n",
      "  Downloading minari-0.5.2-py3-none-any.whl.metadata (5.8 kB)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from minari) (1.26.4)\r\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from minari) (4.12.2)\r\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]>=0.9.0->minari) (0.15.1)\r\n",
      "Requirement already satisfied: gymnasium>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from minari) (0.29.0)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from minari) (24.2)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->minari) (3.1.0)\r\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->minari) (0.0.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.0->minari) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.0->minari) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.0->minari) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.0->minari) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.0->minari) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.0->minari) (2.4.1)\r\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->typer[all]>=0.9.0->minari) (8.1.7)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->typer[all]>=0.9.0->minari) (1.5.4)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->typer[all]>=0.9.0->minari) (13.9.4)\r\n",
      "\u001b[33mWARNING: typer 0.15.1 does not provide the extra 'all'\u001b[0m\u001b[33m\r\n",
      "\u001b[0mRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->typer[all]>=0.9.0->minari) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->typer[all]>=0.9.0->minari) (2.18.0)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.0->minari) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.0->minari) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.0->minari) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.21.0->minari) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.21.0->minari) (2024.2.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->typer[all]>=0.9.0->minari) (0.1.2)\r\n",
      "Downloading minari-0.5.2-py3-none-any.whl (55 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: minari\r\n",
      "Successfully installed minari-0.5.2\r\n",
      "Collecting carla\r\n",
      "  Downloading carla-0.9.15-cp310-cp310-manylinux_2_27_x86_64.whl.metadata (876 bytes)\r\n",
      "Downloading carla-0.9.15-cp310-cp310-manylinux_2_27_x86_64.whl (31.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.9/31.9 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: carla\r\n",
      "Successfully installed carla-0.9.15\r\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install minari\n",
    "!pip install carla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "910ff7ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:18.952112Z",
     "iopub.status.busy": "2025-01-17T21:31:18.951812Z",
     "iopub.status.idle": "2025-01-17T21:31:23.990277Z",
     "shell.execute_reply": "2025-01-17T21:31:23.989323Z"
    },
    "papermill": {
     "duration": 5.059833,
     "end_time": "2025-01-17T21:31:23.991919",
     "exception": false,
     "start_time": "2025-01-17T21:31:18.932086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: All of online environments libraries in D4RL have been moved \u001b]8;;https://github.com/Farama-Foundation/Gymnasium\u001b\\Gymnasium\u001b]8;;\u001b\\, \u001b]8;;https://github.com/Farama-Foundation/MiniGrid\u001b\\MiniGrid\u001b]8;;\u001b\\ and \u001b]8;;https://github.com/Farama-Foundation/Gymnasium-Robotics\u001b\\Gymnasium-Robotics\u001b]8;;\u001b\\, and all offline datasets in D4RL have been moved to \u001b]8;;https://github.com/Farama-Foundation/Minari\u001b\\Minari\u001b]8;;\u001b\\.\n",
      "These new versions include large bug fixes, new versions of Python, and are where all new development will continue. Please upgrade these libraries as soon as you're able to do so.\n",
      "If you'd like to read more about the story behind this switch, please check out \u001b]8;;https://farama.org/Announcing-Minari\u001b\\this blog post\u001b]8;;\u001b\\.\n",
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow'\n",
      "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'dotmap'\n",
      "/usr/local/lib/python3.10/dist-packages/pybullet_envs/env_bases.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import parse_version\n",
      "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n"
     ]
    }
   ],
   "source": [
    "# All necessary imports\n",
    "import os\n",
    "import gym\n",
    "import d4rl # Import required to register environments, you may need to also import the submodule\n",
    "import numpy as np \n",
    "import torch \n",
    "from torch.utils.data import Dataset \n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import collections\n",
    "from torch.cuda.amp import autocast\n",
    "import torch.distributions as dist\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcedf247",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:24.034681Z",
     "iopub.status.busy": "2025-01-17T21:31:24.034217Z",
     "iopub.status.idle": "2025-01-17T21:31:24.038601Z",
     "shell.execute_reply": "2025-01-17T21:31:24.037702Z"
    },
    "papermill": {
     "duration": 0.026291,
     "end_time": "2025-01-17T21:31:24.039766",
     "exception": false,
     "start_time": "2025-01-17T21:31:24.013475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be818fc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:24.078417Z",
     "iopub.status.busy": "2025-01-17T21:31:24.078118Z",
     "iopub.status.idle": "2025-01-17T21:31:24.133735Z",
     "shell.execute_reply": "2025-01-17T21:31:24.132981Z"
    },
    "papermill": {
     "duration": 0.076206,
     "end_time": "2025-01-17T21:31:24.134969",
     "exception": false,
     "start_time": "2025-01-17T21:31:24.058763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36487e5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:24.173673Z",
     "iopub.status.busy": "2025-01-17T21:31:24.173429Z",
     "iopub.status.idle": "2025-01-17T21:31:24.176871Z",
     "shell.execute_reply": "2025-01-17T21:31:24.176074Z"
    },
    "papermill": {
     "duration": 0.024078,
     "end_time": "2025-01-17T21:31:24.178102",
     "exception": false,
     "start_time": "2025-01-17T21:31:24.154024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PRETRAINED_DYNAMICS_MODEL = [\n",
    "    \"/kaggle/input/hopper-medium-pretrain/dynamics_model/dynamics_model_1_final.pth\",\n",
    "    \"/kaggle/input/hopper-medium-pretrain/dynamics_model/dynamics_model_2_final.pth\",\n",
    "    \"/kaggle/input/hopper-medium-pretrain/dynamics_model/dynamics_model_3_final.pth\",\n",
    "]\n",
    "\n",
    "PRETRAINED_BEHAVIOR_MODEL = [\n",
    "    \"/kaggle/input/hopper-medium-pretrain/behavior_model/behavior_model_1_final.pth\",\n",
    "    \"/kaggle/input/hopper-medium-pretrain/behavior_model/behavior_model_2_final.pth\",\n",
    "    \"/kaggle/input/hopper-medium-pretrain/behavior_model/behavior_model_3_final.pth\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f3e61df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:24.216953Z",
     "iopub.status.busy": "2025-01-17T21:31:24.216707Z",
     "iopub.status.idle": "2025-01-17T21:31:24.222486Z",
     "shell.execute_reply": "2025-01-17T21:31:24.221793Z"
    },
    "papermill": {
     "duration": 0.026532,
     "end_time": "2025-01-17T21:31:24.223763",
     "exception": false,
     "start_time": "2025-01-17T21:31:24.197231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories created:\n",
      "- /kaggle/working/dynamics_model\n",
      "- /kaggle/working/behavior_model\n",
      "- /kaggle/working/q_network-\n",
      " /kaggle/working/results\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = \"/kaggle/working/\"\n",
    "DYNAMICS_MODEL = os.path.join(OUTPUT_DIR,\"dynamics_model\")\n",
    "BEHAVIOR_MODEL = os.path.join(OUTPUT_DIR,\"behavior_model\")\n",
    "Q_NETWORK_MODEL = os.path.join(OUTPUT_DIR,\"q_network\")\n",
    "RESULTS = os.path.join(OUTPUT_DIR, \"results\")\n",
    "\n",
    "os.makedirs(DYNAMICS_MODEL, exist_ok=True)\n",
    "os.makedirs(BEHAVIOR_MODEL, exist_ok=True)\n",
    "os.makedirs(Q_NETWORK_MODEL, exist_ok=True)\n",
    "os.makedirs(RESULTS, exist_ok = True)\n",
    "\n",
    "print(f\"Directories created:\\n- {DYNAMICS_MODEL}\\n- {BEHAVIOR_MODEL}\\n- {Q_NETWORK_MODEL}-\\n {RESULTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad18c86",
   "metadata": {
    "papermill": {
     "duration": 0.018796,
     "end_time": "2025-01-17T21:31:24.261763",
     "exception": false,
     "start_time": "2025-01-17T21:31:24.242967",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6190c881",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:24.300831Z",
     "iopub.status.busy": "2025-01-17T21:31:24.300579Z",
     "iopub.status.idle": "2025-01-17T21:31:24.310083Z",
     "shell.execute_reply": "2025-01-17T21:31:24.309266Z"
    },
    "papermill": {
     "duration": 0.030612,
     "end_time": "2025-01-17T21:31:24.311473",
     "exception": false,
     "start_time": "2025-01-17T21:31:24.280861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Environment(Dataset):\n",
    "    def __init__(self, env_name, normalization_states=False, normalization_rewards=False, subset_size=None):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.dataset = self.env.get_dataset()  # Load the D4RL dataset\n",
    "\n",
    "        # Extract data from the D4RL dataset\n",
    "        self.observations = torch.tensor(self.dataset['observations'], dtype=torch.float32)\n",
    "        self.actions = torch.tensor(self.dataset['actions'], dtype=torch.float32)\n",
    "        self.rewards = torch.tensor(self.dataset['rewards'], dtype=torch.float32)\n",
    "        self.next_states = torch.tensor(self.dataset['next_observations'], dtype=torch.float32)\n",
    "        self.dones = torch.tensor(self.dataset['terminals'], dtype=torch.float32)\n",
    "\n",
    "        # Subset the dataset if subset_size is provided\n",
    "        if subset_size is not None:\n",
    "            self.observations = self.observations[:subset_size]\n",
    "            self.actions = self.actions[:subset_size]\n",
    "            self.rewards = self.rewards[:subset_size]\n",
    "            self.next_states = self.next_states[:subset_size]\n",
    "            self.dones = self.dones[:subset_size]\n",
    "\n",
    "        # Normalize data if required\n",
    "        if normalization_states:\n",
    "            self.normalize_states()\n",
    "        if normalization_rewards:\n",
    "            self.normalize_rewards()\n",
    "\n",
    "        # Initialize for episodic simulation\n",
    "        self.num_steps = len(self.observations)\n",
    "        self.current_idx = 0 \n",
    "\n",
    "    def normalize_states(self):\n",
    "        self.shift = -self.observations.mean(dim=0)\n",
    "        self.scale = 1.0 / (self.observations.std(dim=0) + 1e-3)\n",
    "\n",
    "        # Normalize observations and next_states\n",
    "        self.observations = (self.observations + self.shift) * self.scale\n",
    "        self.next_states = (self.next_states + self.shift) * self.scale\n",
    "\n",
    "    def normalize_rewards(self):\n",
    "        self.r_max = self.rewards.max()\n",
    "        self.r_min = self.rewards.min()\n",
    "        self.rewards = (self.rewards - self.r_min) / (self.r_max - self.r_min)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.observations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'observation': self.observations[idx],\n",
    "            'action': self.actions[idx],\n",
    "            'reward': self.rewards[idx],\n",
    "            'next_state': self.next_states[idx],\n",
    "            'done': self.dones[idx]\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_idx = 0\n",
    "        return self.observations[self.current_idx]\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Taking a step in the environment \n",
    "        #print(\"self.current_idx\", self.current_idx)\n",
    "        if self.current_idx >= self.num_steps:\n",
    "            raise IndexError(\"Dataset Exhausted. Call reset() to restart\")\n",
    "\n",
    "        observation = self.observations[self.current_idx]\n",
    "        reward = self.rewards[self.current_idx]\n",
    "        next_state = self.next_states[self.current_idx]\n",
    "        done = self.dones[self.current_idx]\n",
    "\n",
    "        self.current_idx += 1  # Move to the next step\n",
    "        return next_state, reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a36e4d7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:24.350348Z",
     "iopub.status.busy": "2025-01-17T21:31:24.350065Z",
     "iopub.status.idle": "2025-01-17T21:31:28.026960Z",
     "shell.execute_reply": "2025-01-17T21:31:28.025920Z"
    },
    "papermill": {
     "duration": 3.697927,
     "end_time": "2025-01-17T21:31:28.028568",
     "exception": false,
     "start_time": "2025-01-17T21:31:24.330641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset: http://rail.eecs.berkeley.edu/datasets/offline_rl/gym_mujoco_v2/hopper_medium-v2.hdf5 to /root/.d4rl/datasets/hopper_medium-v2.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/spaces/box.py:84: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "load datafile: 100%|██████████| 21/21 [00:01<00:00, 15.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States Shape: torch.Size([1000000, 11])\n",
      "Actions Shape: torch.Size([1000000, 3])\n",
      "Next States Shape: torch.Size([1000000, 11])\n",
      "Rewards Shape: torch.Size([1000000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize dataset\n",
    "env_dataset = Environment(env_name=\"hopper-medium-v2\", normalization_states=False, normalization_rewards=False)\n",
    "\n",
    "# Extract data\n",
    "states = env_dataset.observations\n",
    "actions = env_dataset.actions\n",
    "next_states = env_dataset.next_states\n",
    "rewards = env_dataset.rewards\n",
    "\n",
    "# Optionally, inspect shapes\n",
    "print(\"States Shape:\", states.shape)        # (N, state_dim)\n",
    "print(\"Actions Shape:\", actions.shape)      # (N, action_dim)\n",
    "print(\"Next States Shape:\", next_states.shape)  # (N, state_dim)\n",
    "print(\"Rewards Shape:\", rewards.shape)      # (N,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6220a074",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:28.070091Z",
     "iopub.status.busy": "2025-01-17T21:31:28.069838Z",
     "iopub.status.idle": "2025-01-17T21:31:28.272400Z",
     "shell.execute_reply": "2025-01-17T21:31:28.271564Z"
    },
    "papermill": {
     "duration": 0.2238,
     "end_time": "2025-01-17T21:31:28.273924",
     "exception": false,
     "start_time": "2025-01-17T21:31:28.050124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "states, actions, next_states, rewards = states.to(device), actions.to(device), next_states.to(device), rewards.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecf27e0",
   "metadata": {
    "papermill": {
     "duration": 0.019814,
     "end_time": "2025-01-17T21:31:28.314164",
     "exception": false,
     "start_time": "2025-01-17T21:31:28.294350",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2. Autoregressive Dynamics Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f45ee116",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:28.354524Z",
     "iopub.status.busy": "2025-01-17T21:31:28.354227Z",
     "iopub.status.idle": "2025-01-17T21:31:28.357810Z",
     "shell.execute_reply": "2025-01-17T21:31:28.356880Z"
    },
    "papermill": {
     "duration": 0.02495,
     "end_time": "2025-01-17T21:31:28.359151",
     "exception": false,
     "start_time": "2025-01-17T21:31:28.334201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "572b1c30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:28.398874Z",
     "iopub.status.busy": "2025-01-17T21:31:28.398600Z",
     "iopub.status.idle": "2025-01-17T21:31:28.413197Z",
     "shell.execute_reply": "2025-01-17T21:31:28.412362Z"
    },
    "papermill": {
     "duration": 0.035725,
     "end_time": "2025-01-17T21:31:28.414464",
     "exception": false,
     "start_time": "2025-01-17T21:31:28.378739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ADM(nn.Module):\n",
    "    \"\"\"Unified ADM network: Supports both behavior policy and dynamics model\"\"\"\n",
    "    def __init__(self, output_dim, input_dim, fc_layer_params=(), out_reranking=None, latent_dim=None):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.out_rank = np.arange(output_dim) if out_reranking is None else out_reranking\n",
    "        latent_dim = latent_dim or output_dim * 2\n",
    "        self.shared_layer, self._layers_list = self._initialize_layers(fc_layer_params, latent_dim)\n",
    "\n",
    "    def _initialize_layers(self, fc_layer_params, latent_dim):\n",
    "        shared_layer, layers_list = nn.ModuleList(), nn.ModuleList()\n",
    "        if isinstance(fc_layer_params[0], (list, tuple)):\n",
    "            shared_params, indiv_params = fc_layer_params\n",
    "            # Initialize shared layers\n",
    "            for i, n in enumerate(shared_params):\n",
    "                in_features = self.input_dim if i == 0 else shared_params[i - 1]\n",
    "                shared_layer.append(nn.Linear(in_features, n))\n",
    "            shared_layer.append(nn.Linear(shared_params[-1], latent_dim))\n",
    "        else:\n",
    "            indiv_params = fc_layer_params\n",
    "    \n",
    "        # Initialize individual output layers\n",
    "        for _ in range(self.output_dim):\n",
    "            layers = nn.ModuleList()\n",
    "            for i, n in enumerate(indiv_params):\n",
    "                # Adjust input size for concatenated tensor (latent_dim + mode.size)\n",
    "                in_features = latent_dim + _ if i == 0 else indiv_params[i - 1]\n",
    "                layers.append(nn.Linear(in_features, n))\n",
    "            layers.append(nn.Linear(indiv_params[-1], 2))  # Mean and std\n",
    "            layers_list.append(layers)\n",
    "    \n",
    "        return shared_layer, layers_list\n",
    "\n",
    "    def _forward_layers(self, x, layers):\n",
    "        for layer in layers:\n",
    "            x = torch.relu(layer(x))\n",
    "        return x\n",
    "\n",
    "    def _get_outputs(self, inputs, layers):\n",
    "        h = self._forward_layers(inputs, layers)\n",
    "        mean, log_std = torch.chunk(h, 2, dim=-1)\n",
    "        log_std = torch.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)\n",
    "        std = torch.exp(log_std)\n",
    "        distribution = dist.Normal(mean, std)\n",
    "        return distribution, mean\n",
    "\n",
    "    def _update_tensor(self, a, b, index):\n",
    "        # If `b` is scalar, unsqueeze to make it (1, 1)\n",
    "        if b.dim() == 0:  # If `b` is scalar, make it (1, 1)\n",
    "            b = b.unsqueeze(0).unsqueeze(1)  # Reshape to (1, 1)\n",
    "        elif b.dim() == 1:  # If `b` is 1D, add another dimension\n",
    "            b = b.unsqueeze(1)  # Reshape to (batch_size, 1)\n",
    "    \n",
    "        # Ensure `b` has the same batch size as `a`\n",
    "        if b.size(0) != a.size(0):\n",
    "            raise ValueError(f\"Mismatch in batch size: a has {a.size(0)} rows, but b has {b.size(0)} rows.\")\n",
    "    \n",
    "        # Ensure `b` has the correct dimensions (2D tensor with size [batch_size, 1])\n",
    "        if b.dim() != 2 or b.size(1) != 1:\n",
    "            raise ValueError(f\"Tensor `b` must have 2 dimensions and size (batch_size, 1). Got {b.shape}.\")\n",
    "    \n",
    "        # Split `a` into left and right parts\n",
    "        left = a[:, :index]\n",
    "        right = a[:, index + 1:]\n",
    "    \n",
    "        # Concatenate left, b, and right along dimension 1\n",
    "        return torch.cat([left, b, right], dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h = inputs\n",
    "        if self.shared_layer:\n",
    "            for layer in self.shared_layer:\n",
    "                h = torch.relu(layer(h))\n",
    "    \n",
    "        batch_size = inputs.size(0)\n",
    "        out_mean = torch.zeros(batch_size, self.output_dim, device=inputs.device)\n",
    "        outs_sample = torch.zeros(batch_size, self.output_dim, device=inputs.device)\n",
    "        log_pi_outs = torch.zeros(batch_size, self.output_dim, device=inputs.device)\n",
    "        outs_dist = [None] * self.output_dim\n",
    "    \n",
    "        for i, index in enumerate(self.out_rank):\n",
    "            dist, mode = self._get_outputs(h, self._layers_list[i])\n",
    "    \n",
    "            # Ensure mode has the shape [batch_size, 1] (2D tensor)\n",
    "            if mode.dim() == 3:  # If mode has shape [batch_size, 1, 1], squeeze the extra dimension\n",
    "                mode = mode.squeeze(2)  # This will make it [batch_size, 1]\n",
    "            elif mode.dim() == 1:  # If mode is [batch_size], we need to add an extra dimension\n",
    "                mode = mode.unsqueeze(1)  # This will make it [batch_size, 1]\n",
    "    \n",
    "            # Now, h is [batch_size, feature_size] and mode is [batch_size, 1]\n",
    "            #print(f\"h.shape: {h.shape}\")  # Debug print\n",
    "            #print(f\"mode.shape: {mode.shape}\")  # Debug print\n",
    "    \n",
    "            # Concatenate h and mode\n",
    "            h = torch.cat([h, mode], dim=-1)  # This should now work\n",
    "    \n",
    "            sample = dist.rsample()\n",
    "            log_prob = dist.log_prob(sample)\n",
    "    \n",
    "            out_mean = self._update_tensor(out_mean, mode, index)\n",
    "            outs_sample = self._update_tensor(outs_sample, sample, index)\n",
    "            log_pi_outs = self._update_tensor(log_pi_outs, log_prob, index)\n",
    "            outs_dist[index] = dist\n",
    "    \n",
    "        return out_mean, outs_sample, log_pi_outs, outs_dist\n",
    "\n",
    "\n",
    "    def predict_behavior(self, state):\n",
    "        \"\"\"Predicts behavior policy given the state.\"\"\"\n",
    "        return self.forward(state)\n",
    "\n",
    "    def predict_dynamics(self, state, action):\n",
    "        \"\"\"Predicts next state and reward given state and action.\"\"\"\n",
    "        inputs = torch.cat([state, action], dim=-1)\n",
    "        return self.forward(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da8e0a27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:28.454018Z",
     "iopub.status.busy": "2025-01-17T21:31:28.453803Z",
     "iopub.status.idle": "2025-01-17T21:31:28.459332Z",
     "shell.execute_reply": "2025-01-17T21:31:28.458499Z"
    },
    "papermill": {
     "duration": 0.026775,
     "end_time": "2025-01-17T21:31:28.460437",
     "exception": false,
     "start_time": "2025-01-17T21:31:28.433662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ADMEnsemble:\n",
    "    \"\"\"Ensemble of K ADM models with randomly permuted output orderings.\"\"\"\n",
    "    def __init__(self, num_models, output_dim, input_dim, fc_layer_params=(), latent_dim=None):\n",
    "        self.num_models = num_models\n",
    "        self.models = []\n",
    "        self.output_orderings = []\n",
    "\n",
    "        for _ in range(num_models):\n",
    "            # Generate a random permutation of output dimensions\n",
    "            ordering = np.random.permutation(output_dim)\n",
    "            self.output_orderings.append(ordering)\n",
    "\n",
    "            # Initialize a new ADM model with the permuted ordering\n",
    "            model = ADM(\n",
    "                output_dim=output_dim,\n",
    "                input_dim=input_dim,\n",
    "                fc_layer_params=fc_layer_params,\n",
    "                out_reranking=ordering,\n",
    "                latent_dim=latent_dim\n",
    "            )\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict_behavior(self, state):\n",
    "        \"\"\"Predicts behavior policy using all ensemble members.\"\"\"\n",
    "        predictions = [model.predict_behavior(state) for model in self.models]\n",
    "        return predictions\n",
    "\n",
    "    def predict_dynamics(self, state, action):\n",
    "        \"\"\"Predicts dynamics using all ensemble members.\"\"\"\n",
    "        predictions = [model.predict_dynamics(state, action) for model in self.models]\n",
    "        return predictions\n",
    "\n",
    "    def aggregate_predictions(self, predictions):\n",
    "        \"\"\"Aggregates predictions from the ensemble.\"\"\"\n",
    "        # Extract `out_mean` from each prediction\n",
    "        out_means = [pred[0] for pred in predictions]  # Select the first element of each tuple\n",
    "        aggregated = torch.mean(torch.stack(out_means), dim=0)  # Aggregate over ensemble members\n",
    "        return aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723e4623",
   "metadata": {
    "papermill": {
     "duration": 0.060382,
     "end_time": "2025-01-17T21:31:28.540106",
     "exception": false,
     "start_time": "2025-01-17T21:31:28.479724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Extracting dimensionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c5796c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:28.579830Z",
     "iopub.status.busy": "2025-01-17T21:31:28.579532Z",
     "iopub.status.idle": "2025-01-17T21:31:28.583007Z",
     "shell.execute_reply": "2025-01-17T21:31:28.582375Z"
    },
    "papermill": {
     "duration": 0.024788,
     "end_time": "2025-01-17T21:31:28.584292",
     "exception": false,
     "start_time": "2025-01-17T21:31:28.559504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "state_dim = states.shape[1]\n",
    "action_dim = actions.shape[1]\n",
    "next_state_dim = next_states.shape[1]\n",
    "reward_dim = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf2a044",
   "metadata": {
    "papermill": {
     "duration": 0.019188,
     "end_time": "2025-01-17T21:31:28.622829",
     "exception": false,
     "start_time": "2025-01-17T21:31:28.603641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Define ADM Ensemble Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15a83088",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:28.662445Z",
     "iopub.status.busy": "2025-01-17T21:31:28.662106Z",
     "iopub.status.idle": "2025-01-17T21:31:28.697770Z",
     "shell.execute_reply": "2025-01-17T21:31:28.697129Z"
    },
    "papermill": {
     "duration": 0.056865,
     "end_time": "2025-01-17T21:31:28.698868",
     "exception": false,
     "start_time": "2025-01-17T21:31:28.642003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize ADMEnsemble for dynamics\n",
    "K1 = 3  # Number of dynamics models in the ensemble\n",
    "dynamics_ensemble = ADMEnsemble(\n",
    "    num_models=K1,\n",
    "    output_dim=next_state_dim + reward_dim,\n",
    "    input_dim=state_dim + action_dim,\n",
    "    fc_layer_params=([500], [200,100])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42c09b5",
   "metadata": {
    "papermill": {
     "duration": 0.018931,
     "end_time": "2025-01-17T21:31:28.737252",
     "exception": false,
     "start_time": "2025-01-17T21:31:28.718321",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Define ADM Ensemble Behavior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5365b74d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:28.776640Z",
     "iopub.status.busy": "2025-01-17T21:31:28.776376Z",
     "iopub.status.idle": "2025-01-17T21:31:28.785596Z",
     "shell.execute_reply": "2025-01-17T21:31:28.784930Z"
    },
    "papermill": {
     "duration": 0.030227,
     "end_time": "2025-01-17T21:31:28.786785",
     "exception": false,
     "start_time": "2025-01-17T21:31:28.756558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize ADMEnsemble for behavior\n",
    "K2 = 3 # Number of behavior models in the ensemble\n",
    "behavior_ensemble = ADMEnsemble(\n",
    "    num_models=K2,\n",
    "    output_dim=action_dim,\n",
    "    input_dim=state_dim,\n",
    "    fc_layer_params=([500],[200,100])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b6087d",
   "metadata": {
    "papermill": {
     "duration": 0.019103,
     "end_time": "2025-01-17T21:31:28.825125",
     "exception": false,
     "start_time": "2025-01-17T21:31:28.806022",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a82a76fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:28.864631Z",
     "iopub.status.busy": "2025-01-17T21:31:28.864372Z",
     "iopub.status.idle": "2025-01-17T21:31:32.385947Z",
     "shell.execute_reply": "2025-01-17T21:31:32.385288Z"
    },
    "papermill": {
     "duration": 3.543236,
     "end_time": "2025-01-17T21:31:32.387561",
     "exception": false,
     "start_time": "2025-01-17T21:31:28.844325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optimizers for ensemble models\n",
    "dynamics_optimizers = [torch.optim.Adam(model.parameters(), lr=1e-3) for model in dynamics_ensemble.models]\n",
    "behavior_optimizers = [torch.optim.Adam(model.parameters(), lr=1e-3) for model in behavior_ensemble.models]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d5369f",
   "metadata": {
    "papermill": {
     "duration": 0.019395,
     "end_time": "2025-01-17T21:31:32.427507",
     "exception": false,
     "start_time": "2025-01-17T21:31:32.408112",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Loading Pretrained Ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "697be624",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:32.467736Z",
     "iopub.status.busy": "2025-01-17T21:31:32.467286Z",
     "iopub.status.idle": "2025-01-17T21:31:32.472537Z",
     "shell.execute_reply": "2025-01-17T21:31:32.471570Z"
    },
    "papermill": {
     "duration": 0.026906,
     "end_time": "2025-01-17T21:31:32.473862",
     "exception": false,
     "start_time": "2025-01-17T21:31:32.446956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained dynamics and behavior models\n",
    "def load_pretrained_ensemble(ensemble, pretrained_paths, optimizers, device):\n",
    "    for i, (model, optimizer) in enumerate(zip(ensemble.models, optimizers)):\n",
    "        model.to(device)\n",
    "        if pretrained_paths[i] is not None:  # Check if a path is provided\n",
    "            checkpoint = torch.load(pretrained_paths[i], map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            print(f\"Loaded pretrained model {i + 1} from {pretrained_paths[i]}\")\n",
    "        else:\n",
    "            print(f\"No pretrained model provided for model {i + 1}. Training from scratch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67882877",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:32.513775Z",
     "iopub.status.busy": "2025-01-17T21:31:32.513510Z",
     "iopub.status.idle": "2025-01-17T21:31:33.164241Z",
     "shell.execute_reply": "2025-01-17T21:31:33.163052Z"
    },
    "papermill": {
     "duration": 0.672295,
     "end_time": "2025-01-17T21:31:33.165647",
     "exception": false,
     "start_time": "2025-01-17T21:31:32.493352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-5ca5807f7522>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pretrained_paths[i], map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model 1 from /kaggle/input/hopper-medium-pretrain/dynamics_model/dynamics_model_1_final.pth\n",
      "Loaded pretrained model 2 from /kaggle/input/hopper-medium-pretrain/dynamics_model/dynamics_model_2_final.pth\n",
      "Loaded pretrained model 3 from /kaggle/input/hopper-medium-pretrain/dynamics_model/dynamics_model_3_final.pth\n",
      "Loaded pretrained model 1 from /kaggle/input/hopper-medium-pretrain/behavior_model/behavior_model_1_final.pth\n",
      "Loaded pretrained model 2 from /kaggle/input/hopper-medium-pretrain/behavior_model/behavior_model_2_final.pth\n",
      "Loaded pretrained model 3 from /kaggle/input/hopper-medium-pretrain/behavior_model/behavior_model_3_final.pth\n"
     ]
    }
   ],
   "source": [
    "load_pretrained_ensemble(dynamics_ensemble, PRETRAINED_DYNAMICS_MODEL, dynamics_optimizers, device)\n",
    "load_pretrained_ensemble(behavior_ensemble, PRETRAINED_BEHAVIOR_MODEL, behavior_optimizers, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caafead",
   "metadata": {
    "papermill": {
     "duration": 0.019191,
     "end_time": "2025-01-17T21:31:33.204783",
     "exception": false,
     "start_time": "2025-01-17T21:31:33.185592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Initializing Ensemble with Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1d6c872",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:33.244540Z",
     "iopub.status.busy": "2025-01-17T21:31:33.244252Z",
     "iopub.status.idle": "2025-01-17T21:31:33.248818Z",
     "shell.execute_reply": "2025-01-17T21:31:33.247970Z"
    },
    "papermill": {
     "duration": 0.025684,
     "end_time": "2025-01-17T21:31:33.249979",
     "exception": false,
     "start_time": "2025-01-17T21:31:33.224295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initialize_ensemble_with_pretrained(ensemble, pretrained_paths, optimizers, device):\n",
    "    for i, (model, optimizer, pretrained_path) in enumerate(zip(ensemble.models, optimizers, pretrained_paths)):\n",
    "        model.to(device)\n",
    "        if pretrained_path is not None:  # If pretrained path is provided\n",
    "            checkpoint = torch.load(pretrained_path, map_location=device)  # Load the checkpoint\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            print(f\"Pretrained model {i + 1} loaded from {pretrained_path}.\")\n",
    "        else:\n",
    "            print(f\"No pretrained model provided for model {i + 1}. Training from scratch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22e53abe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:33.290578Z",
     "iopub.status.busy": "2025-01-17T21:31:33.290340Z",
     "iopub.status.idle": "2025-01-17T21:31:33.523489Z",
     "shell.execute_reply": "2025-01-17T21:31:33.522580Z"
    },
    "papermill": {
     "duration": 0.255464,
     "end_time": "2025-01-17T21:31:33.525204",
     "exception": false,
     "start_time": "2025-01-17T21:31:33.269740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-2ce07f5479cc>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pretrained_path, map_location=device)  # Load the checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model 1 loaded from /kaggle/input/hopper-medium-pretrain/dynamics_model/dynamics_model_1_final.pth.\n",
      "Pretrained model 2 loaded from /kaggle/input/hopper-medium-pretrain/dynamics_model/dynamics_model_2_final.pth.\n",
      "Pretrained model 3 loaded from /kaggle/input/hopper-medium-pretrain/dynamics_model/dynamics_model_3_final.pth.\n",
      "Pretrained model 1 loaded from /kaggle/input/hopper-medium-pretrain/behavior_model/behavior_model_1_final.pth.\n",
      "Pretrained model 2 loaded from /kaggle/input/hopper-medium-pretrain/behavior_model/behavior_model_2_final.pth.\n",
      "Pretrained model 3 loaded from /kaggle/input/hopper-medium-pretrain/behavior_model/behavior_model_3_final.pth.\n"
     ]
    }
   ],
   "source": [
    "initialize_ensemble_with_pretrained(dynamics_ensemble, PRETRAINED_DYNAMICS_MODEL, dynamics_optimizers, device)\n",
    "initialize_ensemble_with_pretrained(behavior_ensemble, PRETRAINED_BEHAVIOR_MODEL, behavior_optimizers, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040e28d5",
   "metadata": {
    "papermill": {
     "duration": 0.029793,
     "end_time": "2025-01-17T21:31:33.587747",
     "exception": false,
     "start_time": "2025-01-17T21:31:33.557954",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35660908",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:33.656453Z",
     "iopub.status.busy": "2025-01-17T21:31:33.655937Z",
     "iopub.status.idle": "2025-01-17T21:31:33.660608Z",
     "shell.execute_reply": "2025-01-17T21:31:33.659695Z"
    },
    "papermill": {
     "duration": 0.043851,
     "end_time": "2025-01-17T21:31:33.662695",
     "exception": false,
     "start_time": "2025-01-17T21:31:33.618844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "batch_size = 256\n",
    "num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3f33108",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:33.721871Z",
     "iopub.status.busy": "2025-01-17T21:31:33.721400Z",
     "iopub.status.idle": "2025-01-17T21:31:33.734257Z",
     "shell.execute_reply": "2025-01-17T21:31:33.733262Z"
    },
    "papermill": {
     "duration": 0.044483,
     "end_time": "2025-01-17T21:31:33.735532",
     "exception": false,
     "start_time": "2025-01-17T21:31:33.691049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "subset_size = 100000 # Use 1000 samples instead of 1M\n",
    "dataset_dynamics = torch.utils.data.TensorDataset(\n",
    "    states[:subset_size], actions[:subset_size], next_states[:subset_size], rewards[:subset_size]\n",
    ")\n",
    "dataset_behavior = torch.utils.data.TensorDataset(\n",
    "    states[:subset_size], actions[:subset_size]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a66b52d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:33.776102Z",
     "iopub.status.busy": "2025-01-17T21:31:33.775857Z",
     "iopub.status.idle": "2025-01-17T21:31:33.781248Z",
     "shell.execute_reply": "2025-01-17T21:31:33.780415Z"
    },
    "papermill": {
     "duration": 0.02694,
     "end_time": "2025-01-17T21:31:33.782542",
     "exception": false,
     "start_time": "2025-01-17T21:31:33.755602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n#subset_size = 1000  # Use 1000 samples instead of 1M\\ndataset_dynamics = torch.utils.data.TensorDataset(states, actions, next_states, rewards)\\ndataset_behavior = torch.utils.data.TensorDataset(states, actions)\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "#subset_size = 1000  # Use 1000 samples instead of 1M\n",
    "dataset_dynamics = torch.utils.data.TensorDataset(states, actions, next_states, rewards)\n",
    "dataset_behavior = torch.utils.data.TensorDataset(states, actions)\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55841cf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:33.822888Z",
     "iopub.status.busy": "2025-01-17T21:31:33.822656Z",
     "iopub.status.idle": "2025-01-17T21:31:33.826351Z",
     "shell.execute_reply": "2025-01-17T21:31:33.825547Z"
    },
    "papermill": {
     "duration": 0.025293,
     "end_time": "2025-01-17T21:31:33.827584",
     "exception": false,
     "start_time": "2025-01-17T21:31:33.802291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader_dynamics = torch.utils.data.DataLoader(dataset_dynamics, batch_size=batch_size, shuffle=True)\n",
    "dataloader_behavior = torch.utils.data.DataLoader(dataset_behavior, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7238e84b",
   "metadata": {
    "papermill": {
     "duration": 0.019397,
     "end_time": "2025-01-17T21:31:33.866559",
     "exception": false,
     "start_time": "2025-01-17T21:31:33.847162",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf5f6cbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:33.906472Z",
     "iopub.status.busy": "2025-01-17T21:31:33.906217Z",
     "iopub.status.idle": "2025-01-17T21:31:33.918893Z",
     "shell.execute_reply": "2025-01-17T21:31:33.918068Z"
    },
    "papermill": {
     "duration": 0.034024,
     "end_time": "2025-01-17T21:31:33.920061",
     "exception": false,
     "start_time": "2025-01-17T21:31:33.886037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure all models are on the correct device before training\n",
    "for model in dynamics_ensemble.models:\n",
    "    model.to(device)\n",
    "for model in behavior_ensemble.models:\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c4ed557",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T21:31:33.960705Z",
     "iopub.status.busy": "2025-01-17T21:31:33.960432Z",
     "iopub.status.idle": "2025-01-18T03:47:11.392095Z",
     "shell.execute_reply": "2025-01-18T03:47:11.391268Z"
    },
    "papermill": {
     "duration": 22537.49689,
     "end_time": "2025-01-18T03:47:11.436889",
     "exception": false,
     "start_time": "2025-01-17T21:31:33.939999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "  Dynamics Model 1 Loss: 1236.0588\n",
      "  Dynamics Model 2 Loss: 1793.1914\n",
      "  Dynamics Model 3 Loss: 1725.1455\n",
      "  Behavior Model 1 Loss: 486.5755\n",
      "  Behavior Model 2 Loss: 503.2867\n",
      "  Behavior Model 3 Loss: 534.9189\n",
      "Epoch 2/500\n",
      "  Dynamics Model 1 Loss: 1167.2598\n",
      "  Dynamics Model 2 Loss: 1737.3824\n",
      "  Dynamics Model 3 Loss: 1688.2531\n",
      "  Behavior Model 1 Loss: 488.4313\n",
      "  Behavior Model 2 Loss: 496.5690\n",
      "  Behavior Model 3 Loss: 524.3356\n",
      "Epoch 3/500\n",
      "  Dynamics Model 1 Loss: 1163.8378\n",
      "  Dynamics Model 2 Loss: 1741.9491\n",
      "  Dynamics Model 3 Loss: 1685.1796\n",
      "  Behavior Model 1 Loss: 488.6097\n",
      "  Behavior Model 2 Loss: 491.1754\n",
      "  Behavior Model 3 Loss: 523.1706\n",
      "Epoch 4/500\n",
      "  Dynamics Model 1 Loss: 1165.4749\n",
      "  Dynamics Model 2 Loss: 1740.1859\n",
      "  Dynamics Model 3 Loss: 1686.5634\n",
      "  Behavior Model 1 Loss: 487.8992\n",
      "  Behavior Model 2 Loss: 490.5031\n",
      "  Behavior Model 3 Loss: 519.8545\n",
      "Epoch 5/500\n",
      "  Dynamics Model 1 Loss: 1162.8943\n",
      "  Dynamics Model 2 Loss: 1736.2793\n",
      "  Dynamics Model 3 Loss: 1685.3529\n",
      "  Behavior Model 1 Loss: 490.1489\n",
      "  Behavior Model 2 Loss: 489.6622\n",
      "  Behavior Model 3 Loss: 518.2387\n",
      "Epoch 6/500\n",
      "  Dynamics Model 1 Loss: 1167.9578\n",
      "  Dynamics Model 2 Loss: 1740.3178\n",
      "  Dynamics Model 3 Loss: 1687.1606\n",
      "  Behavior Model 1 Loss: 486.2301\n",
      "  Behavior Model 2 Loss: 487.6774\n",
      "  Behavior Model 3 Loss: 518.9533\n",
      "Epoch 7/500\n",
      "  Dynamics Model 1 Loss: 1165.2669\n",
      "  Dynamics Model 2 Loss: 1739.4972\n",
      "  Dynamics Model 3 Loss: 1683.4664\n",
      "  Behavior Model 1 Loss: 488.2300\n",
      "  Behavior Model 2 Loss: 485.6422\n",
      "  Behavior Model 3 Loss: 517.7660\n",
      "Epoch 8/500\n",
      "  Dynamics Model 1 Loss: 1161.9141\n",
      "  Dynamics Model 2 Loss: 1738.6752\n",
      "  Dynamics Model 3 Loss: 1684.2647\n",
      "  Behavior Model 1 Loss: 489.4799\n",
      "  Behavior Model 2 Loss: 486.5314\n",
      "  Behavior Model 3 Loss: 516.7681\n",
      "Epoch 9/500\n",
      "  Dynamics Model 1 Loss: 1163.1019\n",
      "  Dynamics Model 2 Loss: 1740.5899\n",
      "  Dynamics Model 3 Loss: 1686.4987\n",
      "  Behavior Model 1 Loss: 486.4459\n",
      "  Behavior Model 2 Loss: 483.4157\n",
      "  Behavior Model 3 Loss: 517.0997\n",
      "Epoch 10/500\n",
      "  Dynamics Model 1 Loss: 1163.4629\n",
      "  Dynamics Model 2 Loss: 1737.1857\n",
      "  Dynamics Model 3 Loss: 1687.6080\n",
      "  Behavior Model 1 Loss: 489.7890\n",
      "  Behavior Model 2 Loss: 484.6349\n",
      "  Behavior Model 3 Loss: 517.8933\n",
      "Epoch 11/500\n",
      "  Dynamics Model 1 Loss: 1163.1361\n",
      "  Dynamics Model 2 Loss: 1739.0022\n",
      "  Dynamics Model 3 Loss: 1684.5774\n",
      "  Behavior Model 1 Loss: 490.1815\n",
      "  Behavior Model 2 Loss: 482.7705\n",
      "  Behavior Model 3 Loss: 515.4118\n",
      "Epoch 12/500\n",
      "  Dynamics Model 1 Loss: 1164.3764\n",
      "  Dynamics Model 2 Loss: 1737.6132\n",
      "  Dynamics Model 3 Loss: 1686.0572\n",
      "  Behavior Model 1 Loss: 486.3861\n",
      "  Behavior Model 2 Loss: 484.4228\n",
      "  Behavior Model 3 Loss: 517.1557\n",
      "Epoch 13/500\n",
      "  Dynamics Model 1 Loss: 1164.3753\n",
      "  Dynamics Model 2 Loss: 1740.6916\n",
      "  Dynamics Model 3 Loss: 1683.4798\n",
      "  Behavior Model 1 Loss: 487.5197\n",
      "  Behavior Model 2 Loss: 481.9121\n",
      "  Behavior Model 3 Loss: 515.4794\n",
      "Epoch 14/500\n",
      "  Dynamics Model 1 Loss: 1164.1989\n",
      "  Dynamics Model 2 Loss: 1738.6670\n",
      "  Dynamics Model 3 Loss: 1685.2714\n",
      "  Behavior Model 1 Loss: 489.2857\n",
      "  Behavior Model 2 Loss: 481.8162\n",
      "  Behavior Model 3 Loss: 517.3728\n",
      "Epoch 15/500\n",
      "  Dynamics Model 1 Loss: 1162.7668\n",
      "  Dynamics Model 2 Loss: 1732.6230\n",
      "  Dynamics Model 3 Loss: 1683.3005\n",
      "  Behavior Model 1 Loss: 488.8745\n",
      "  Behavior Model 2 Loss: 483.0494\n",
      "  Behavior Model 3 Loss: 517.2287\n",
      "Epoch 16/500\n",
      "  Dynamics Model 1 Loss: 1162.1806\n",
      "  Dynamics Model 2 Loss: 1717.4139\n",
      "  Dynamics Model 3 Loss: 1685.6309\n",
      "  Behavior Model 1 Loss: 487.7414\n",
      "  Behavior Model 2 Loss: 482.7393\n",
      "  Behavior Model 3 Loss: 516.2989\n",
      "Epoch 17/500\n",
      "  Dynamics Model 1 Loss: 1162.9639\n",
      "  Dynamics Model 2 Loss: 1717.6552\n",
      "  Dynamics Model 3 Loss: 1683.7680\n",
      "  Behavior Model 1 Loss: 486.9571\n",
      "  Behavior Model 2 Loss: 482.9609\n",
      "  Behavior Model 3 Loss: 517.4662\n",
      "Epoch 18/500\n",
      "  Dynamics Model 1 Loss: 1163.8851\n",
      "  Dynamics Model 2 Loss: 1715.1648\n",
      "  Dynamics Model 3 Loss: 1684.0946\n",
      "  Behavior Model 1 Loss: 487.5301\n",
      "  Behavior Model 2 Loss: 479.5160\n",
      "  Behavior Model 3 Loss: 518.0624\n",
      "Epoch 19/500\n",
      "  Dynamics Model 1 Loss: 1162.3922\n",
      "  Dynamics Model 2 Loss: 1716.3578\n",
      "  Dynamics Model 3 Loss: 1684.8811\n",
      "  Behavior Model 1 Loss: 486.5525\n",
      "  Behavior Model 2 Loss: 482.8087\n",
      "  Behavior Model 3 Loss: 515.1103\n",
      "Epoch 20/500\n",
      "  Dynamics Model 1 Loss: 1162.9042\n",
      "  Dynamics Model 2 Loss: 1715.9157\n",
      "  Dynamics Model 3 Loss: 1683.7722\n",
      "  Behavior Model 1 Loss: 489.1784\n",
      "  Behavior Model 2 Loss: 482.9520\n",
      "  Behavior Model 3 Loss: 515.8091\n",
      "Epoch 21/500\n",
      "  Dynamics Model 1 Loss: 1163.6454\n",
      "  Dynamics Model 2 Loss: 1718.1434\n",
      "  Dynamics Model 3 Loss: 1687.2666\n",
      "  Behavior Model 1 Loss: 489.0282\n",
      "  Behavior Model 2 Loss: 482.3825\n",
      "  Behavior Model 3 Loss: 516.0617\n",
      "Epoch 22/500\n",
      "  Dynamics Model 1 Loss: 1160.7045\n",
      "  Dynamics Model 2 Loss: 1718.9018\n",
      "  Dynamics Model 3 Loss: 1686.7516\n",
      "  Behavior Model 1 Loss: 488.4341\n",
      "  Behavior Model 2 Loss: 480.8860\n",
      "  Behavior Model 3 Loss: 513.8130\n",
      "Epoch 23/500\n",
      "  Dynamics Model 1 Loss: 1162.4818\n",
      "  Dynamics Model 2 Loss: 1717.2253\n",
      "  Dynamics Model 3 Loss: 1686.5226\n",
      "  Behavior Model 1 Loss: 486.9391\n",
      "  Behavior Model 2 Loss: 482.7380\n",
      "  Behavior Model 3 Loss: 516.0729\n",
      "Epoch 24/500\n",
      "  Dynamics Model 1 Loss: 1162.2844\n",
      "  Dynamics Model 2 Loss: 1714.1820\n",
      "  Dynamics Model 3 Loss: 1684.4182\n",
      "  Behavior Model 1 Loss: 489.3245\n",
      "  Behavior Model 2 Loss: 482.1184\n",
      "  Behavior Model 3 Loss: 515.0026\n",
      "Epoch 25/500\n",
      "  Dynamics Model 1 Loss: 1160.1054\n",
      "  Dynamics Model 2 Loss: 1716.7294\n",
      "  Dynamics Model 3 Loss: 1685.4830\n",
      "  Behavior Model 1 Loss: 489.4809\n",
      "  Behavior Model 2 Loss: 479.5769\n",
      "  Behavior Model 3 Loss: 514.9787\n",
      "Epoch 26/500\n",
      "  Dynamics Model 1 Loss: 1164.0073\n",
      "  Dynamics Model 2 Loss: 1716.9471\n",
      "  Dynamics Model 3 Loss: 1685.3404\n",
      "  Behavior Model 1 Loss: 487.7378\n",
      "  Behavior Model 2 Loss: 482.6947\n",
      "  Behavior Model 3 Loss: 512.8957\n",
      "Epoch 27/500\n",
      "  Dynamics Model 1 Loss: 1162.9371\n",
      "  Dynamics Model 2 Loss: 1714.6794\n",
      "  Dynamics Model 3 Loss: 1685.0499\n",
      "  Behavior Model 1 Loss: 489.0999\n",
      "  Behavior Model 2 Loss: 481.1754\n",
      "  Behavior Model 3 Loss: 513.2271\n",
      "Epoch 28/500\n",
      "  Dynamics Model 1 Loss: 1162.4929\n",
      "  Dynamics Model 2 Loss: 1713.8871\n",
      "  Dynamics Model 3 Loss: 1682.7404\n",
      "  Behavior Model 1 Loss: 490.0683\n",
      "  Behavior Model 2 Loss: 481.6350\n",
      "  Behavior Model 3 Loss: 513.2773\n",
      "Epoch 29/500\n",
      "  Dynamics Model 1 Loss: 1162.0356\n",
      "  Dynamics Model 2 Loss: 1716.3024\n",
      "  Dynamics Model 3 Loss: 1683.3940\n",
      "  Behavior Model 1 Loss: 488.7976\n",
      "  Behavior Model 2 Loss: 479.1262\n",
      "  Behavior Model 3 Loss: 516.3123\n",
      "Epoch 30/500\n",
      "  Dynamics Model 1 Loss: 1164.1359\n",
      "  Dynamics Model 2 Loss: 1713.7685\n",
      "  Dynamics Model 3 Loss: 1684.1216\n",
      "  Behavior Model 1 Loss: 488.1280\n",
      "  Behavior Model 2 Loss: 480.4772\n",
      "  Behavior Model 3 Loss: 514.5117\n",
      "Epoch 31/500\n",
      "  Dynamics Model 1 Loss: 1163.8730\n",
      "  Dynamics Model 2 Loss: 1715.6023\n",
      "  Dynamics Model 3 Loss: 1684.7310\n",
      "  Behavior Model 1 Loss: 485.7703\n",
      "  Behavior Model 2 Loss: 481.6787\n",
      "  Behavior Model 3 Loss: 515.0744\n",
      "Epoch 32/500\n",
      "  Dynamics Model 1 Loss: 1162.9649\n",
      "  Dynamics Model 2 Loss: 1715.8275\n",
      "  Dynamics Model 3 Loss: 1685.7129\n",
      "  Behavior Model 1 Loss: 489.9848\n",
      "  Behavior Model 2 Loss: 482.8446\n",
      "  Behavior Model 3 Loss: 514.2943\n",
      "Epoch 33/500\n",
      "  Dynamics Model 1 Loss: 1164.0252\n",
      "  Dynamics Model 2 Loss: 1714.7404\n",
      "  Dynamics Model 3 Loss: 1659.5208\n",
      "  Behavior Model 1 Loss: 489.2916\n",
      "  Behavior Model 2 Loss: 481.7287\n",
      "  Behavior Model 3 Loss: 516.6047\n",
      "Epoch 34/500\n",
      "  Dynamics Model 1 Loss: 1163.5445\n",
      "  Dynamics Model 2 Loss: 1714.7767\n",
      "  Dynamics Model 3 Loss: 1229.0501\n",
      "  Behavior Model 1 Loss: 488.1192\n",
      "  Behavior Model 2 Loss: 480.6410\n",
      "  Behavior Model 3 Loss: 511.7590\n",
      "Epoch 35/500\n",
      "  Dynamics Model 1 Loss: 1162.2905\n",
      "  Dynamics Model 2 Loss: 1715.1499\n",
      "  Dynamics Model 3 Loss: 1229.0472\n",
      "  Behavior Model 1 Loss: 488.3491\n",
      "  Behavior Model 2 Loss: 481.7550\n",
      "  Behavior Model 3 Loss: 513.1083\n",
      "Epoch 36/500\n",
      "  Dynamics Model 1 Loss: 1162.9279\n",
      "  Dynamics Model 2 Loss: 1716.4382\n",
      "  Dynamics Model 3 Loss: 1230.6546\n",
      "  Behavior Model 1 Loss: 487.6160\n",
      "  Behavior Model 2 Loss: 481.1258\n",
      "  Behavior Model 3 Loss: 511.9519\n",
      "Epoch 37/500\n",
      "  Dynamics Model 1 Loss: 1163.2626\n",
      "  Dynamics Model 2 Loss: 1713.7032\n",
      "  Dynamics Model 3 Loss: 1228.0693\n",
      "  Behavior Model 1 Loss: 489.7612\n",
      "  Behavior Model 2 Loss: 480.8889\n",
      "  Behavior Model 3 Loss: 514.2116\n",
      "Epoch 38/500\n",
      "  Dynamics Model 1 Loss: 1161.0169\n",
      "  Dynamics Model 2 Loss: 1716.1289\n",
      "  Dynamics Model 3 Loss: 1226.9741\n",
      "  Behavior Model 1 Loss: 489.2851\n",
      "  Behavior Model 2 Loss: 479.9042\n",
      "  Behavior Model 3 Loss: 513.4349\n",
      "Epoch 39/500\n",
      "  Dynamics Model 1 Loss: 1163.4216\n",
      "  Dynamics Model 2 Loss: 1717.6551\n",
      "  Dynamics Model 3 Loss: 1226.7272\n",
      "  Behavior Model 1 Loss: 486.8245\n",
      "  Behavior Model 2 Loss: 479.3315\n",
      "  Behavior Model 3 Loss: 513.3617\n",
      "Epoch 40/500\n",
      "  Dynamics Model 1 Loss: 1162.4752\n",
      "  Dynamics Model 2 Loss: 1714.1812\n",
      "  Dynamics Model 3 Loss: 1227.1240\n",
      "  Behavior Model 1 Loss: 488.0102\n",
      "  Behavior Model 2 Loss: 482.2657\n",
      "  Behavior Model 3 Loss: 514.8021\n",
      "Epoch 41/500\n",
      "  Dynamics Model 1 Loss: 1162.9778\n",
      "  Dynamics Model 2 Loss: 1716.9380\n",
      "  Dynamics Model 3 Loss: 1227.3689\n",
      "  Behavior Model 1 Loss: 489.5718\n",
      "  Behavior Model 2 Loss: 480.7034\n",
      "  Behavior Model 3 Loss: 513.9276\n",
      "Epoch 42/500\n",
      "  Dynamics Model 1 Loss: 1163.8962\n",
      "  Dynamics Model 2 Loss: 1713.8117\n",
      "  Dynamics Model 3 Loss: 1224.2797\n",
      "  Behavior Model 1 Loss: 489.3292\n",
      "  Behavior Model 2 Loss: 482.6653\n",
      "  Behavior Model 3 Loss: 513.9598\n",
      "Epoch 43/500\n",
      "  Dynamics Model 1 Loss: 1161.7869\n",
      "  Dynamics Model 2 Loss: 1715.1616\n",
      "  Dynamics Model 3 Loss: 1225.6699\n",
      "  Behavior Model 1 Loss: 486.8797\n",
      "  Behavior Model 2 Loss: 483.1900\n",
      "  Behavior Model 3 Loss: 514.8270\n",
      "Epoch 44/500\n",
      "  Dynamics Model 1 Loss: 1162.6989\n",
      "  Dynamics Model 2 Loss: 1715.0000\n",
      "  Dynamics Model 3 Loss: 1227.9414\n",
      "  Behavior Model 1 Loss: 489.6005\n",
      "  Behavior Model 2 Loss: 481.2489\n",
      "  Behavior Model 3 Loss: 515.4097\n",
      "Epoch 45/500\n",
      "  Dynamics Model 1 Loss: 1163.5380\n",
      "  Dynamics Model 2 Loss: 1717.2030\n",
      "  Dynamics Model 3 Loss: 1227.0396\n",
      "  Behavior Model 1 Loss: 488.5358\n",
      "  Behavior Model 2 Loss: 481.1745\n",
      "  Behavior Model 3 Loss: 514.8635\n",
      "Epoch 46/500\n",
      "  Dynamics Model 1 Loss: 1163.3252\n",
      "  Dynamics Model 2 Loss: 1714.2678\n",
      "  Dynamics Model 3 Loss: 1227.8604\n",
      "  Behavior Model 1 Loss: 487.5003\n",
      "  Behavior Model 2 Loss: 484.5412\n",
      "  Behavior Model 3 Loss: 514.0545\n",
      "Epoch 47/500\n",
      "  Dynamics Model 1 Loss: 1162.3322\n",
      "  Dynamics Model 2 Loss: 1716.3118\n",
      "  Dynamics Model 3 Loss: 1227.2071\n",
      "  Behavior Model 1 Loss: 486.2537\n",
      "  Behavior Model 2 Loss: 480.5509\n",
      "  Behavior Model 3 Loss: 514.3685\n",
      "Epoch 48/500\n",
      "  Dynamics Model 1 Loss: 1163.6720\n",
      "  Dynamics Model 2 Loss: 1718.6633\n",
      "  Dynamics Model 3 Loss: 1226.7849\n",
      "  Behavior Model 1 Loss: 486.8394\n",
      "  Behavior Model 2 Loss: 478.8967\n",
      "  Behavior Model 3 Loss: 515.5273\n",
      "Epoch 49/500\n",
      "  Dynamics Model 1 Loss: 1161.7609\n",
      "  Dynamics Model 2 Loss: 1717.0222\n",
      "  Dynamics Model 3 Loss: 1226.9528\n",
      "  Behavior Model 1 Loss: 488.0109\n",
      "  Behavior Model 2 Loss: 480.4706\n",
      "  Behavior Model 3 Loss: 513.0473\n",
      "Epoch 50/500\n",
      "  Dynamics Model 1 Loss: 1160.2662\n",
      "  Dynamics Model 2 Loss: 1716.6360\n",
      "  Dynamics Model 3 Loss: 1224.4919\n",
      "  Behavior Model 1 Loss: 487.0603\n",
      "  Behavior Model 2 Loss: 480.4285\n",
      "  Behavior Model 3 Loss: 513.3820\n",
      "Epoch 51/500\n",
      "  Dynamics Model 1 Loss: 1163.8310\n",
      "  Dynamics Model 2 Loss: 1715.8669\n",
      "  Dynamics Model 3 Loss: 1226.9244\n",
      "  Behavior Model 1 Loss: 488.2730\n",
      "  Behavior Model 2 Loss: 479.8034\n",
      "  Behavior Model 3 Loss: 512.0101\n",
      "Epoch 52/500\n",
      "  Dynamics Model 1 Loss: 1162.6997\n",
      "  Dynamics Model 2 Loss: 1715.2925\n",
      "  Dynamics Model 3 Loss: 1226.1712\n",
      "  Behavior Model 1 Loss: 489.2605\n",
      "  Behavior Model 2 Loss: 481.2289\n",
      "  Behavior Model 3 Loss: 515.8040\n",
      "Epoch 53/500\n",
      "  Dynamics Model 1 Loss: 1164.2980\n",
      "  Dynamics Model 2 Loss: 1715.3915\n",
      "  Dynamics Model 3 Loss: 1226.6879\n",
      "  Behavior Model 1 Loss: 488.2738\n",
      "  Behavior Model 2 Loss: 481.6363\n",
      "  Behavior Model 3 Loss: 513.4740\n",
      "Epoch 54/500\n",
      "  Dynamics Model 1 Loss: 1162.5960\n",
      "  Dynamics Model 2 Loss: 1719.1572\n",
      "  Dynamics Model 3 Loss: 1227.0940\n",
      "  Behavior Model 1 Loss: 488.1324\n",
      "  Behavior Model 2 Loss: 482.7140\n",
      "  Behavior Model 3 Loss: 514.7382\n",
      "Epoch 55/500\n",
      "  Dynamics Model 1 Loss: 1162.6797\n",
      "  Dynamics Model 2 Loss: 1713.9647\n",
      "  Dynamics Model 3 Loss: 1225.5723\n",
      "  Behavior Model 1 Loss: 487.5451\n",
      "  Behavior Model 2 Loss: 480.4358\n",
      "  Behavior Model 3 Loss: 514.7144\n",
      "Epoch 56/500\n",
      "  Dynamics Model 1 Loss: 1163.5871\n",
      "  Dynamics Model 2 Loss: 1714.8570\n",
      "  Dynamics Model 3 Loss: 1226.2272\n",
      "  Behavior Model 1 Loss: 488.3479\n",
      "  Behavior Model 2 Loss: 480.0591\n",
      "  Behavior Model 3 Loss: 514.7209\n",
      "Epoch 57/500\n",
      "  Dynamics Model 1 Loss: 1163.8041\n",
      "  Dynamics Model 2 Loss: 1714.6849\n",
      "  Dynamics Model 3 Loss: 1226.6118\n",
      "  Behavior Model 1 Loss: 489.4311\n",
      "  Behavior Model 2 Loss: 479.6852\n",
      "  Behavior Model 3 Loss: 515.4402\n",
      "Epoch 58/500\n",
      "  Dynamics Model 1 Loss: 1164.0932\n",
      "  Dynamics Model 2 Loss: 1713.2507\n",
      "  Dynamics Model 3 Loss: 1226.7529\n",
      "  Behavior Model 1 Loss: 488.2361\n",
      "  Behavior Model 2 Loss: 480.8535\n",
      "  Behavior Model 3 Loss: 512.7999\n",
      "Epoch 59/500\n",
      "  Dynamics Model 1 Loss: 1161.9744\n",
      "  Dynamics Model 2 Loss: 1714.5376\n",
      "  Dynamics Model 3 Loss: 1227.8987\n",
      "  Behavior Model 1 Loss: 490.2974\n",
      "  Behavior Model 2 Loss: 481.2085\n",
      "  Behavior Model 3 Loss: 512.6155\n",
      "Epoch 60/500\n",
      "  Dynamics Model 1 Loss: 1164.6887\n",
      "  Dynamics Model 2 Loss: 1715.6285\n",
      "  Dynamics Model 3 Loss: 1225.6694\n",
      "  Behavior Model 1 Loss: 487.0087\n",
      "  Behavior Model 2 Loss: 481.6189\n",
      "  Behavior Model 3 Loss: 514.0080\n",
      "Epoch 61/500\n",
      "  Dynamics Model 1 Loss: 1163.1717\n",
      "  Dynamics Model 2 Loss: 1714.0207\n",
      "  Dynamics Model 3 Loss: 1226.9022\n",
      "  Behavior Model 1 Loss: 488.3482\n",
      "  Behavior Model 2 Loss: 481.5818\n",
      "  Behavior Model 3 Loss: 515.0360\n",
      "Epoch 62/500\n",
      "  Dynamics Model 1 Loss: 1161.8020\n",
      "  Dynamics Model 2 Loss: 1716.1851\n",
      "  Dynamics Model 3 Loss: 1224.7495\n",
      "  Behavior Model 1 Loss: 487.4392\n",
      "  Behavior Model 2 Loss: 481.7452\n",
      "  Behavior Model 3 Loss: 513.8866\n",
      "Epoch 63/500\n",
      "  Dynamics Model 1 Loss: 1162.1384\n",
      "  Dynamics Model 2 Loss: 1714.8211\n",
      "  Dynamics Model 3 Loss: 1227.5417\n",
      "  Behavior Model 1 Loss: 488.3622\n",
      "  Behavior Model 2 Loss: 480.2176\n",
      "  Behavior Model 3 Loss: 512.6150\n",
      "Epoch 64/500\n",
      "  Dynamics Model 1 Loss: 1164.3789\n",
      "  Dynamics Model 2 Loss: 1719.0487\n",
      "  Dynamics Model 3 Loss: 1228.4947\n",
      "  Behavior Model 1 Loss: 488.4368\n",
      "  Behavior Model 2 Loss: 482.6204\n",
      "  Behavior Model 3 Loss: 510.9698\n",
      "Epoch 65/500\n",
      "  Dynamics Model 1 Loss: 1160.9151\n",
      "  Dynamics Model 2 Loss: 1715.6274\n",
      "  Dynamics Model 3 Loss: 1226.3184\n",
      "  Behavior Model 1 Loss: 487.4354\n",
      "  Behavior Model 2 Loss: 480.5692\n",
      "  Behavior Model 3 Loss: 512.8044\n",
      "Epoch 66/500\n",
      "  Dynamics Model 1 Loss: 1160.4914\n",
      "  Dynamics Model 2 Loss: 1714.4780\n",
      "  Dynamics Model 3 Loss: 1224.3822\n",
      "  Behavior Model 1 Loss: 487.2371\n",
      "  Behavior Model 2 Loss: 481.0735\n",
      "  Behavior Model 3 Loss: 515.6763\n",
      "Epoch 67/500\n",
      "  Dynamics Model 1 Loss: 1161.6562\n",
      "  Dynamics Model 2 Loss: 1714.4663\n",
      "  Dynamics Model 3 Loss: 1227.6676\n",
      "  Behavior Model 1 Loss: 487.7146\n",
      "  Behavior Model 2 Loss: 479.5957\n",
      "  Behavior Model 3 Loss: 516.8422\n",
      "Epoch 68/500\n",
      "  Dynamics Model 1 Loss: 1161.6788\n",
      "  Dynamics Model 2 Loss: 1714.8512\n",
      "  Dynamics Model 3 Loss: 1227.1630\n",
      "  Behavior Model 1 Loss: 487.7084\n",
      "  Behavior Model 2 Loss: 480.2194\n",
      "  Behavior Model 3 Loss: 514.0104\n",
      "Epoch 69/500\n",
      "  Dynamics Model 1 Loss: 1162.9737\n",
      "  Dynamics Model 2 Loss: 1715.9721\n",
      "  Dynamics Model 3 Loss: 1225.5836\n",
      "  Behavior Model 1 Loss: 490.4178\n",
      "  Behavior Model 2 Loss: 481.2586\n",
      "  Behavior Model 3 Loss: 514.6349\n",
      "Epoch 70/500\n",
      "  Dynamics Model 1 Loss: 1163.2416\n",
      "  Dynamics Model 2 Loss: 1716.5471\n",
      "  Dynamics Model 3 Loss: 1226.3875\n",
      "  Behavior Model 1 Loss: 490.1596\n",
      "  Behavior Model 2 Loss: 479.9252\n",
      "  Behavior Model 3 Loss: 512.7479\n",
      "Epoch 71/500\n",
      "  Dynamics Model 1 Loss: 1164.3719\n",
      "  Dynamics Model 2 Loss: 1714.0573\n",
      "  Dynamics Model 3 Loss: 1224.4980\n",
      "  Behavior Model 1 Loss: 488.1058\n",
      "  Behavior Model 2 Loss: 480.2048\n",
      "  Behavior Model 3 Loss: 514.0262\n",
      "Epoch 72/500\n",
      "  Dynamics Model 1 Loss: 1162.9916\n",
      "  Dynamics Model 2 Loss: 1715.2509\n",
      "  Dynamics Model 3 Loss: 1226.2692\n",
      "  Behavior Model 1 Loss: 487.8071\n",
      "  Behavior Model 2 Loss: 480.6792\n",
      "  Behavior Model 3 Loss: 515.4963\n",
      "Epoch 73/500\n",
      "  Dynamics Model 1 Loss: 1162.2338\n",
      "  Dynamics Model 2 Loss: 1710.1874\n",
      "  Dynamics Model 3 Loss: 1226.3691\n",
      "  Behavior Model 1 Loss: 488.0097\n",
      "  Behavior Model 2 Loss: 480.2189\n",
      "  Behavior Model 3 Loss: 513.3395\n",
      "Epoch 74/500\n",
      "  Dynamics Model 1 Loss: 1163.1444\n",
      "  Dynamics Model 2 Loss: 1714.3901\n",
      "  Dynamics Model 3 Loss: 1227.1138\n",
      "  Behavior Model 1 Loss: 486.1974\n",
      "  Behavior Model 2 Loss: 480.2359\n",
      "  Behavior Model 3 Loss: 513.9481\n",
      "Epoch 75/500\n",
      "  Dynamics Model 1 Loss: 1162.7014\n",
      "  Dynamics Model 2 Loss: 1714.7868\n",
      "  Dynamics Model 3 Loss: 1225.8882\n",
      "  Behavior Model 1 Loss: 486.1206\n",
      "  Behavior Model 2 Loss: 478.1962\n",
      "  Behavior Model 3 Loss: 513.8518\n",
      "Epoch 76/500\n",
      "  Dynamics Model 1 Loss: 1164.0212\n",
      "  Dynamics Model 2 Loss: 1716.6832\n",
      "  Dynamics Model 3 Loss: 1227.0412\n",
      "  Behavior Model 1 Loss: 488.4273\n",
      "  Behavior Model 2 Loss: 479.9308\n",
      "  Behavior Model 3 Loss: 512.2978\n",
      "Epoch 77/500\n",
      "  Dynamics Model 1 Loss: 1159.8602\n",
      "  Dynamics Model 2 Loss: 1711.6598\n",
      "  Dynamics Model 3 Loss: 1227.5157\n",
      "  Behavior Model 1 Loss: 488.5301\n",
      "  Behavior Model 2 Loss: 482.5461\n",
      "  Behavior Model 3 Loss: 513.6498\n",
      "Epoch 78/500\n",
      "  Dynamics Model 1 Loss: 1162.9584\n",
      "  Dynamics Model 2 Loss: 1714.2482\n",
      "  Dynamics Model 3 Loss: 1226.3055\n",
      "  Behavior Model 1 Loss: 486.0004\n",
      "  Behavior Model 2 Loss: 479.5393\n",
      "  Behavior Model 3 Loss: 512.6085\n",
      "Epoch 79/500\n",
      "  Dynamics Model 1 Loss: 1163.5806\n",
      "  Dynamics Model 2 Loss: 1715.2476\n",
      "  Dynamics Model 3 Loss: 1228.6312\n",
      "  Behavior Model 1 Loss: 489.3248\n",
      "  Behavior Model 2 Loss: 480.2286\n",
      "  Behavior Model 3 Loss: 513.7206\n",
      "Epoch 80/500\n",
      "  Dynamics Model 1 Loss: 1162.1558\n",
      "  Dynamics Model 2 Loss: 1714.5162\n",
      "  Dynamics Model 3 Loss: 1227.7942\n",
      "  Behavior Model 1 Loss: 488.6791\n",
      "  Behavior Model 2 Loss: 480.9056\n",
      "  Behavior Model 3 Loss: 514.3559\n",
      "Epoch 81/500\n",
      "  Dynamics Model 1 Loss: 1163.2947\n",
      "  Dynamics Model 2 Loss: 1714.2881\n",
      "  Dynamics Model 3 Loss: 1227.2790\n",
      "  Behavior Model 1 Loss: 489.6062\n",
      "  Behavior Model 2 Loss: 479.8903\n",
      "  Behavior Model 3 Loss: 511.6335\n",
      "Epoch 82/500\n",
      "  Dynamics Model 1 Loss: 1163.5429\n",
      "  Dynamics Model 2 Loss: 1714.8310\n",
      "  Dynamics Model 3 Loss: 1227.8469\n",
      "  Behavior Model 1 Loss: 486.6484\n",
      "  Behavior Model 2 Loss: 480.4190\n",
      "  Behavior Model 3 Loss: 512.3497\n",
      "Epoch 83/500\n",
      "  Dynamics Model 1 Loss: 1163.4564\n",
      "  Dynamics Model 2 Loss: 1716.5285\n",
      "  Dynamics Model 3 Loss: 1226.3116\n",
      "  Behavior Model 1 Loss: 489.0858\n",
      "  Behavior Model 2 Loss: 479.1251\n",
      "  Behavior Model 3 Loss: 512.0019\n",
      "Epoch 84/500\n",
      "  Dynamics Model 1 Loss: 1164.3862\n",
      "  Dynamics Model 2 Loss: 1714.5390\n",
      "  Dynamics Model 3 Loss: 1228.1697\n",
      "  Behavior Model 1 Loss: 488.5616\n",
      "  Behavior Model 2 Loss: 478.9378\n",
      "  Behavior Model 3 Loss: 514.5303\n",
      "Epoch 85/500\n",
      "  Dynamics Model 1 Loss: 1161.2850\n",
      "  Dynamics Model 2 Loss: 1714.8957\n",
      "  Dynamics Model 3 Loss: 1225.2790\n",
      "  Behavior Model 1 Loss: 486.8480\n",
      "  Behavior Model 2 Loss: 480.9544\n",
      "  Behavior Model 3 Loss: 514.2682\n",
      "Epoch 86/500\n",
      "  Dynamics Model 1 Loss: 1167.2617\n",
      "  Dynamics Model 2 Loss: 1714.8378\n",
      "  Dynamics Model 3 Loss: 1224.7113\n",
      "  Behavior Model 1 Loss: 487.8256\n",
      "  Behavior Model 2 Loss: 479.9768\n",
      "  Behavior Model 3 Loss: 513.7240\n",
      "Epoch 87/500\n",
      "  Dynamics Model 1 Loss: 1162.1572\n",
      "  Dynamics Model 2 Loss: 1715.4901\n",
      "  Dynamics Model 3 Loss: 1225.3463\n",
      "  Behavior Model 1 Loss: 488.5255\n",
      "  Behavior Model 2 Loss: 479.1894\n",
      "  Behavior Model 3 Loss: 513.9517\n",
      "Epoch 88/500\n",
      "  Dynamics Model 1 Loss: 1162.1945\n",
      "  Dynamics Model 2 Loss: 1717.0451\n",
      "  Dynamics Model 3 Loss: 1227.5046\n",
      "  Behavior Model 1 Loss: 488.1619\n",
      "  Behavior Model 2 Loss: 479.6289\n",
      "  Behavior Model 3 Loss: 513.5686\n",
      "Epoch 89/500\n",
      "  Dynamics Model 1 Loss: 1162.0097\n",
      "  Dynamics Model 2 Loss: 1713.6364\n",
      "  Dynamics Model 3 Loss: 1227.1789\n",
      "  Behavior Model 1 Loss: 489.2026\n",
      "  Behavior Model 2 Loss: 480.3809\n",
      "  Behavior Model 3 Loss: 514.2941\n",
      "Epoch 90/500\n",
      "  Dynamics Model 1 Loss: 1161.0791\n",
      "  Dynamics Model 2 Loss: 1715.0861\n",
      "  Dynamics Model 3 Loss: 1226.6243\n",
      "  Behavior Model 1 Loss: 488.9848\n",
      "  Behavior Model 2 Loss: 480.3330\n",
      "  Behavior Model 3 Loss: 512.9855\n",
      "Epoch 91/500\n",
      "  Dynamics Model 1 Loss: 1161.3748\n",
      "  Dynamics Model 2 Loss: 1713.8533\n",
      "  Dynamics Model 3 Loss: 1227.0841\n",
      "  Behavior Model 1 Loss: 486.5988\n",
      "  Behavior Model 2 Loss: 481.4472\n",
      "  Behavior Model 3 Loss: 513.7266\n",
      "Epoch 92/500\n",
      "  Dynamics Model 1 Loss: 1163.0221\n",
      "  Dynamics Model 2 Loss: 1714.2061\n",
      "  Dynamics Model 3 Loss: 1226.1807\n",
      "  Behavior Model 1 Loss: 486.3475\n",
      "  Behavior Model 2 Loss: 479.8342\n",
      "  Behavior Model 3 Loss: 513.5158\n",
      "Epoch 93/500\n",
      "  Dynamics Model 1 Loss: 1161.4680\n",
      "  Dynamics Model 2 Loss: 1713.5163\n",
      "  Dynamics Model 3 Loss: 1225.3737\n",
      "  Behavior Model 1 Loss: 489.0210\n",
      "  Behavior Model 2 Loss: 477.7988\n",
      "  Behavior Model 3 Loss: 513.3495\n",
      "Epoch 94/500\n",
      "  Dynamics Model 1 Loss: 1164.8974\n",
      "  Dynamics Model 2 Loss: 1715.0614\n",
      "  Dynamics Model 3 Loss: 1225.3544\n",
      "  Behavior Model 1 Loss: 489.1179\n",
      "  Behavior Model 2 Loss: 479.7057\n",
      "  Behavior Model 3 Loss: 511.7550\n",
      "Epoch 95/500\n",
      "  Dynamics Model 1 Loss: 1163.8755\n",
      "  Dynamics Model 2 Loss: 1714.8767\n",
      "  Dynamics Model 3 Loss: 1226.9812\n",
      "  Behavior Model 1 Loss: 487.9285\n",
      "  Behavior Model 2 Loss: 480.4635\n",
      "  Behavior Model 3 Loss: 513.9418\n",
      "Epoch 96/500\n",
      "  Dynamics Model 1 Loss: 1164.7236\n",
      "  Dynamics Model 2 Loss: 1711.8138\n",
      "  Dynamics Model 3 Loss: 1228.4789\n",
      "  Behavior Model 1 Loss: 489.6858\n",
      "  Behavior Model 2 Loss: 480.7746\n",
      "  Behavior Model 3 Loss: 516.5148\n",
      "Epoch 97/500\n",
      "  Dynamics Model 1 Loss: 1161.1841\n",
      "  Dynamics Model 2 Loss: 1716.0258\n",
      "  Dynamics Model 3 Loss: 1224.6697\n",
      "  Behavior Model 1 Loss: 486.7636\n",
      "  Behavior Model 2 Loss: 479.6003\n",
      "  Behavior Model 3 Loss: 513.6696\n",
      "Epoch 98/500\n",
      "  Dynamics Model 1 Loss: 1164.4241\n",
      "  Dynamics Model 2 Loss: 1716.4693\n",
      "  Dynamics Model 3 Loss: 1223.6795\n",
      "  Behavior Model 1 Loss: 486.4009\n",
      "  Behavior Model 2 Loss: 480.0069\n",
      "  Behavior Model 3 Loss: 512.5683\n",
      "Epoch 99/500\n",
      "  Dynamics Model 1 Loss: 1160.9563\n",
      "  Dynamics Model 2 Loss: 1715.2414\n",
      "  Dynamics Model 3 Loss: 1226.7180\n",
      "  Behavior Model 1 Loss: 487.8890\n",
      "  Behavior Model 2 Loss: 480.5706\n",
      "  Behavior Model 3 Loss: 514.5330\n",
      "Epoch 100/500\n",
      "  Dynamics Model 1 Loss: 1161.6547\n",
      "  Dynamics Model 2 Loss: 1714.8428\n",
      "  Dynamics Model 3 Loss: 1227.2244\n",
      "  Behavior Model 1 Loss: 489.2773\n",
      "  Behavior Model 2 Loss: 480.0965\n",
      "  Behavior Model 3 Loss: 514.0352\n",
      "Epoch 101/500\n",
      "  Dynamics Model 1 Loss: 1165.2543\n",
      "  Dynamics Model 2 Loss: 1715.5068\n",
      "  Dynamics Model 3 Loss: 1226.1441\n",
      "  Behavior Model 1 Loss: 489.5549\n",
      "  Behavior Model 2 Loss: 480.3843\n",
      "  Behavior Model 3 Loss: 512.3029\n",
      "Epoch 102/500\n",
      "  Dynamics Model 1 Loss: 1162.7073\n",
      "  Dynamics Model 2 Loss: 1715.7426\n",
      "  Dynamics Model 3 Loss: 1223.3435\n",
      "  Behavior Model 1 Loss: 489.0151\n",
      "  Behavior Model 2 Loss: 479.4266\n",
      "  Behavior Model 3 Loss: 512.4489\n",
      "Epoch 103/500\n",
      "  Dynamics Model 1 Loss: 1165.1963\n",
      "  Dynamics Model 2 Loss: 1716.2389\n",
      "  Dynamics Model 3 Loss: 1226.4023\n",
      "  Behavior Model 1 Loss: 487.2856\n",
      "  Behavior Model 2 Loss: 478.3391\n",
      "  Behavior Model 3 Loss: 511.1230\n",
      "Epoch 104/500\n",
      "  Dynamics Model 1 Loss: 1161.4335\n",
      "  Dynamics Model 2 Loss: 1710.5990\n",
      "  Dynamics Model 3 Loss: 1227.3651\n",
      "  Behavior Model 1 Loss: 489.4327\n",
      "  Behavior Model 2 Loss: 479.0960\n",
      "  Behavior Model 3 Loss: 513.5012\n",
      "Epoch 105/500\n",
      "  Dynamics Model 1 Loss: 1162.5573\n",
      "  Dynamics Model 2 Loss: 1714.6960\n",
      "  Dynamics Model 3 Loss: 1224.1432\n",
      "  Behavior Model 1 Loss: 490.3958\n",
      "  Behavior Model 2 Loss: 479.4634\n",
      "  Behavior Model 3 Loss: 512.0209\n",
      "Epoch 106/500\n",
      "  Dynamics Model 1 Loss: 1163.3228\n",
      "  Dynamics Model 2 Loss: 1715.4905\n",
      "  Dynamics Model 3 Loss: 1225.0466\n",
      "  Behavior Model 1 Loss: 486.5542\n",
      "  Behavior Model 2 Loss: 481.4647\n",
      "  Behavior Model 3 Loss: 515.3476\n",
      "Epoch 107/500\n",
      "  Dynamics Model 1 Loss: 1161.1237\n",
      "  Dynamics Model 2 Loss: 1716.8605\n",
      "  Dynamics Model 3 Loss: 1225.7605\n",
      "  Behavior Model 1 Loss: 488.3319\n",
      "  Behavior Model 2 Loss: 478.5429\n",
      "  Behavior Model 3 Loss: 515.2510\n",
      "Epoch 108/500\n",
      "  Dynamics Model 1 Loss: 1161.0274\n",
      "  Dynamics Model 2 Loss: 1717.0913\n",
      "  Dynamics Model 3 Loss: 1227.2157\n",
      "  Behavior Model 1 Loss: 488.6044\n",
      "  Behavior Model 2 Loss: 477.9095\n",
      "  Behavior Model 3 Loss: 513.7194\n",
      "Epoch 109/500\n",
      "  Dynamics Model 1 Loss: 1163.1971\n",
      "  Dynamics Model 2 Loss: 1713.8757\n",
      "  Dynamics Model 3 Loss: 1226.4765\n",
      "  Behavior Model 1 Loss: 486.7264\n",
      "  Behavior Model 2 Loss: 479.7678\n",
      "  Behavior Model 3 Loss: 512.8483\n",
      "Epoch 110/500\n",
      "  Dynamics Model 1 Loss: 1162.7726\n",
      "  Dynamics Model 2 Loss: 1714.6215\n",
      "  Dynamics Model 3 Loss: 1227.4132\n",
      "  Behavior Model 1 Loss: 487.0092\n",
      "  Behavior Model 2 Loss: 479.2059\n",
      "  Behavior Model 3 Loss: 511.3931\n",
      "Epoch 111/500\n",
      "  Dynamics Model 1 Loss: 1160.9684\n",
      "  Dynamics Model 2 Loss: 1716.1333\n",
      "  Dynamics Model 3 Loss: 1227.2703\n",
      "  Behavior Model 1 Loss: 487.4593\n",
      "  Behavior Model 2 Loss: 479.0947\n",
      "  Behavior Model 3 Loss: 514.0983\n",
      "Epoch 112/500\n",
      "  Dynamics Model 1 Loss: 1162.5034\n",
      "  Dynamics Model 2 Loss: 1716.1995\n",
      "  Dynamics Model 3 Loss: 1227.5664\n",
      "  Behavior Model 1 Loss: 490.0791\n",
      "  Behavior Model 2 Loss: 479.3761\n",
      "  Behavior Model 3 Loss: 513.6875\n",
      "Epoch 113/500\n",
      "  Dynamics Model 1 Loss: 1163.9419\n",
      "  Dynamics Model 2 Loss: 1715.0147\n",
      "  Dynamics Model 3 Loss: 1226.1976\n",
      "  Behavior Model 1 Loss: 486.9829\n",
      "  Behavior Model 2 Loss: 480.4529\n",
      "  Behavior Model 3 Loss: 514.1683\n",
      "Epoch 114/500\n",
      "  Dynamics Model 1 Loss: 1163.1889\n",
      "  Dynamics Model 2 Loss: 1712.9762\n",
      "  Dynamics Model 3 Loss: 1226.7059\n",
      "  Behavior Model 1 Loss: 488.6675\n",
      "  Behavior Model 2 Loss: 482.1440\n",
      "  Behavior Model 3 Loss: 512.8527\n",
      "Epoch 115/500\n",
      "  Dynamics Model 1 Loss: 1162.7594\n",
      "  Dynamics Model 2 Loss: 1715.2572\n",
      "  Dynamics Model 3 Loss: 1228.1628\n",
      "  Behavior Model 1 Loss: 487.9825\n",
      "  Behavior Model 2 Loss: 480.1948\n",
      "  Behavior Model 3 Loss: 514.1243\n",
      "Epoch 116/500\n",
      "  Dynamics Model 1 Loss: 1161.4672\n",
      "  Dynamics Model 2 Loss: 1716.7684\n",
      "  Dynamics Model 3 Loss: 1226.8709\n",
      "  Behavior Model 1 Loss: 487.9482\n",
      "  Behavior Model 2 Loss: 480.5872\n",
      "  Behavior Model 3 Loss: 513.7947\n",
      "Epoch 117/500\n",
      "  Dynamics Model 1 Loss: 1161.9556\n",
      "  Dynamics Model 2 Loss: 1714.4071\n",
      "  Dynamics Model 3 Loss: 1226.4063\n",
      "  Behavior Model 1 Loss: 487.5612\n",
      "  Behavior Model 2 Loss: 481.5614\n",
      "  Behavior Model 3 Loss: 513.9710\n",
      "Epoch 118/500\n",
      "  Dynamics Model 1 Loss: 1165.6287\n",
      "  Dynamics Model 2 Loss: 1714.7800\n",
      "  Dynamics Model 3 Loss: 1226.4242\n",
      "  Behavior Model 1 Loss: 487.7421\n",
      "  Behavior Model 2 Loss: 478.4964\n",
      "  Behavior Model 3 Loss: 512.8290\n",
      "Epoch 119/500\n",
      "  Dynamics Model 1 Loss: 1163.9864\n",
      "  Dynamics Model 2 Loss: 1714.2406\n",
      "  Dynamics Model 3 Loss: 1228.9477\n",
      "  Behavior Model 1 Loss: 489.1704\n",
      "  Behavior Model 2 Loss: 477.8931\n",
      "  Behavior Model 3 Loss: 514.4019\n",
      "Epoch 120/500\n",
      "  Dynamics Model 1 Loss: 1161.1060\n",
      "  Dynamics Model 2 Loss: 1714.9650\n",
      "  Dynamics Model 3 Loss: 1224.9112\n",
      "  Behavior Model 1 Loss: 489.1067\n",
      "  Behavior Model 2 Loss: 479.0350\n",
      "  Behavior Model 3 Loss: 515.2852\n",
      "Epoch 121/500\n",
      "  Dynamics Model 1 Loss: 1161.1724\n",
      "  Dynamics Model 2 Loss: 1714.5490\n",
      "  Dynamics Model 3 Loss: 1225.0342\n",
      "  Behavior Model 1 Loss: 488.0752\n",
      "  Behavior Model 2 Loss: 480.5805\n",
      "  Behavior Model 3 Loss: 513.1647\n",
      "Epoch 122/500\n",
      "  Dynamics Model 1 Loss: 1162.7999\n",
      "  Dynamics Model 2 Loss: 1714.0022\n",
      "  Dynamics Model 3 Loss: 1225.8028\n",
      "  Behavior Model 1 Loss: 489.5613\n",
      "  Behavior Model 2 Loss: 480.5088\n",
      "  Behavior Model 3 Loss: 513.6568\n",
      "Epoch 123/500\n",
      "  Dynamics Model 1 Loss: 1162.5011\n",
      "  Dynamics Model 2 Loss: 1715.2515\n",
      "  Dynamics Model 3 Loss: 1228.2343\n",
      "  Behavior Model 1 Loss: 487.7411\n",
      "  Behavior Model 2 Loss: 478.5185\n",
      "  Behavior Model 3 Loss: 514.8635\n",
      "Epoch 124/500\n",
      "  Dynamics Model 1 Loss: 1162.5042\n",
      "  Dynamics Model 2 Loss: 1713.9753\n",
      "  Dynamics Model 3 Loss: 1227.0961\n",
      "  Behavior Model 1 Loss: 489.8580\n",
      "  Behavior Model 2 Loss: 480.5500\n",
      "  Behavior Model 3 Loss: 511.2481\n",
      "Epoch 125/500\n",
      "  Dynamics Model 1 Loss: 1162.5822\n",
      "  Dynamics Model 2 Loss: 1714.8357\n",
      "  Dynamics Model 3 Loss: 1225.0348\n",
      "  Behavior Model 1 Loss: 488.1137\n",
      "  Behavior Model 2 Loss: 478.7597\n",
      "  Behavior Model 3 Loss: 510.8197\n",
      "Epoch 126/500\n",
      "  Dynamics Model 1 Loss: 1161.6577\n",
      "  Dynamics Model 2 Loss: 1716.9097\n",
      "  Dynamics Model 3 Loss: 1226.6791\n",
      "  Behavior Model 1 Loss: 489.0810\n",
      "  Behavior Model 2 Loss: 478.7388\n",
      "  Behavior Model 3 Loss: 513.7006\n",
      "Epoch 127/500\n",
      "  Dynamics Model 1 Loss: 1163.7555\n",
      "  Dynamics Model 2 Loss: 1713.0667\n",
      "  Dynamics Model 3 Loss: 1225.0721\n",
      "  Behavior Model 1 Loss: 487.2431\n",
      "  Behavior Model 2 Loss: 481.5044\n",
      "  Behavior Model 3 Loss: 513.7430\n",
      "Epoch 128/500\n",
      "  Dynamics Model 1 Loss: 1161.1446\n",
      "  Dynamics Model 2 Loss: 1715.8098\n",
      "  Dynamics Model 3 Loss: 1225.7001\n",
      "  Behavior Model 1 Loss: 486.2873\n",
      "  Behavior Model 2 Loss: 479.4834\n",
      "  Behavior Model 3 Loss: 512.4911\n",
      "Epoch 129/500\n",
      "  Dynamics Model 1 Loss: 1162.0304\n",
      "  Dynamics Model 2 Loss: 1714.6405\n",
      "  Dynamics Model 3 Loss: 1225.5083\n",
      "  Behavior Model 1 Loss: 487.6169\n",
      "  Behavior Model 2 Loss: 478.6174\n",
      "  Behavior Model 3 Loss: 514.2148\n",
      "Epoch 130/500\n",
      "  Dynamics Model 1 Loss: 1161.7329\n",
      "  Dynamics Model 2 Loss: 1717.1132\n",
      "  Dynamics Model 3 Loss: 1225.8301\n",
      "  Behavior Model 1 Loss: 488.9971\n",
      "  Behavior Model 2 Loss: 480.4715\n",
      "  Behavior Model 3 Loss: 512.0240\n",
      "Epoch 131/500\n",
      "  Dynamics Model 1 Loss: 1160.0744\n",
      "  Dynamics Model 2 Loss: 1716.7009\n",
      "  Dynamics Model 3 Loss: 1227.0763\n",
      "  Behavior Model 1 Loss: 485.9334\n",
      "  Behavior Model 2 Loss: 478.3691\n",
      "  Behavior Model 3 Loss: 511.1155\n",
      "Epoch 132/500\n",
      "  Dynamics Model 1 Loss: 1162.0830\n",
      "  Dynamics Model 2 Loss: 1714.5007\n",
      "  Dynamics Model 3 Loss: 1225.4390\n",
      "  Behavior Model 1 Loss: 486.4590\n",
      "  Behavior Model 2 Loss: 478.4920\n",
      "  Behavior Model 3 Loss: 511.6783\n",
      "Epoch 133/500\n",
      "  Dynamics Model 1 Loss: 1163.4605\n",
      "  Dynamics Model 2 Loss: 1716.1825\n",
      "  Dynamics Model 3 Loss: 1227.1030\n",
      "  Behavior Model 1 Loss: 488.0039\n",
      "  Behavior Model 2 Loss: 480.5023\n",
      "  Behavior Model 3 Loss: 512.1372\n",
      "Epoch 134/500\n",
      "  Dynamics Model 1 Loss: 1163.3455\n",
      "  Dynamics Model 2 Loss: 1717.0170\n",
      "  Dynamics Model 3 Loss: 1226.2184\n",
      "  Behavior Model 1 Loss: 488.0246\n",
      "  Behavior Model 2 Loss: 480.0108\n",
      "  Behavior Model 3 Loss: 513.6495\n",
      "Epoch 135/500\n",
      "  Dynamics Model 1 Loss: 1162.8034\n",
      "  Dynamics Model 2 Loss: 1715.9517\n",
      "  Dynamics Model 3 Loss: 1226.9279\n",
      "  Behavior Model 1 Loss: 490.0702\n",
      "  Behavior Model 2 Loss: 479.1679\n",
      "  Behavior Model 3 Loss: 512.7608\n",
      "Epoch 136/500\n",
      "  Dynamics Model 1 Loss: 1162.8680\n",
      "  Dynamics Model 2 Loss: 1714.1058\n",
      "  Dynamics Model 3 Loss: 1227.7864\n",
      "  Behavior Model 1 Loss: 488.4975\n",
      "  Behavior Model 2 Loss: 478.2502\n",
      "  Behavior Model 3 Loss: 511.9980\n",
      "Epoch 137/500\n",
      "  Dynamics Model 1 Loss: 1163.4773\n",
      "  Dynamics Model 2 Loss: 1714.7955\n",
      "  Dynamics Model 3 Loss: 1226.5895\n",
      "  Behavior Model 1 Loss: 488.3568\n",
      "  Behavior Model 2 Loss: 480.5818\n",
      "  Behavior Model 3 Loss: 513.4808\n",
      "Epoch 138/500\n",
      "  Dynamics Model 1 Loss: 1162.6703\n",
      "  Dynamics Model 2 Loss: 1713.8130\n",
      "  Dynamics Model 3 Loss: 1225.4461\n",
      "  Behavior Model 1 Loss: 490.4677\n",
      "  Behavior Model 2 Loss: 481.4086\n",
      "  Behavior Model 3 Loss: 512.9813\n",
      "Epoch 139/500\n",
      "  Dynamics Model 1 Loss: 1163.4770\n",
      "  Dynamics Model 2 Loss: 1716.3308\n",
      "  Dynamics Model 3 Loss: 1226.1609\n",
      "  Behavior Model 1 Loss: 489.4124\n",
      "  Behavior Model 2 Loss: 481.7648\n",
      "  Behavior Model 3 Loss: 515.3657\n",
      "Epoch 140/500\n",
      "  Dynamics Model 1 Loss: 1161.9282\n",
      "  Dynamics Model 2 Loss: 1713.5447\n",
      "  Dynamics Model 3 Loss: 1227.2100\n",
      "  Behavior Model 1 Loss: 488.1321\n",
      "  Behavior Model 2 Loss: 480.3519\n",
      "  Behavior Model 3 Loss: 512.1310\n",
      "Epoch 141/500\n",
      "  Dynamics Model 1 Loss: 1164.0722\n",
      "  Dynamics Model 2 Loss: 1715.7124\n",
      "  Dynamics Model 3 Loss: 1226.9667\n",
      "  Behavior Model 1 Loss: 487.9062\n",
      "  Behavior Model 2 Loss: 479.3790\n",
      "  Behavior Model 3 Loss: 511.2062\n",
      "Epoch 142/500\n",
      "  Dynamics Model 1 Loss: 1161.0827\n",
      "  Dynamics Model 2 Loss: 1715.5059\n",
      "  Dynamics Model 3 Loss: 1227.0917\n",
      "  Behavior Model 1 Loss: 486.7663\n",
      "  Behavior Model 2 Loss: 478.3518\n",
      "  Behavior Model 3 Loss: 514.1982\n",
      "Epoch 143/500\n",
      "  Dynamics Model 1 Loss: 1161.4216\n",
      "  Dynamics Model 2 Loss: 1714.8074\n",
      "  Dynamics Model 3 Loss: 1224.4839\n",
      "  Behavior Model 1 Loss: 488.9682\n",
      "  Behavior Model 2 Loss: 479.0482\n",
      "  Behavior Model 3 Loss: 514.3700\n",
      "Epoch 144/500\n",
      "  Dynamics Model 1 Loss: 1162.6797\n",
      "  Dynamics Model 2 Loss: 1715.0564\n",
      "  Dynamics Model 3 Loss: 1228.1790\n",
      "  Behavior Model 1 Loss: 488.7659\n",
      "  Behavior Model 2 Loss: 479.5956\n",
      "  Behavior Model 3 Loss: 512.3449\n",
      "Epoch 145/500\n",
      "  Dynamics Model 1 Loss: 1164.2013\n",
      "  Dynamics Model 2 Loss: 1716.0042\n",
      "  Dynamics Model 3 Loss: 1226.3400\n",
      "  Behavior Model 1 Loss: 489.0921\n",
      "  Behavior Model 2 Loss: 476.4737\n",
      "  Behavior Model 3 Loss: 513.2454\n",
      "Epoch 146/500\n",
      "  Dynamics Model 1 Loss: 1163.7246\n",
      "  Dynamics Model 2 Loss: 1714.0342\n",
      "  Dynamics Model 3 Loss: 1225.8953\n",
      "  Behavior Model 1 Loss: 487.4219\n",
      "  Behavior Model 2 Loss: 480.3970\n",
      "  Behavior Model 3 Loss: 511.2575\n",
      "Epoch 147/500\n",
      "  Dynamics Model 1 Loss: 1162.3136\n",
      "  Dynamics Model 2 Loss: 1714.7037\n",
      "  Dynamics Model 3 Loss: 1226.8105\n",
      "  Behavior Model 1 Loss: 486.3724\n",
      "  Behavior Model 2 Loss: 479.8954\n",
      "  Behavior Model 3 Loss: 513.4987\n",
      "Epoch 148/500\n",
      "  Dynamics Model 1 Loss: 1161.7364\n",
      "  Dynamics Model 2 Loss: 1717.7735\n",
      "  Dynamics Model 3 Loss: 1226.2853\n",
      "  Behavior Model 1 Loss: 487.3125\n",
      "  Behavior Model 2 Loss: 480.3950\n",
      "  Behavior Model 3 Loss: 512.5703\n",
      "Epoch 149/500\n",
      "  Dynamics Model 1 Loss: 1163.5899\n",
      "  Dynamics Model 2 Loss: 1716.1718\n",
      "  Dynamics Model 3 Loss: 1226.2191\n",
      "  Behavior Model 1 Loss: 486.8581\n",
      "  Behavior Model 2 Loss: 480.4004\n",
      "  Behavior Model 3 Loss: 514.6498\n",
      "Epoch 150/500\n",
      "  Dynamics Model 1 Loss: 1163.7775\n",
      "  Dynamics Model 2 Loss: 1714.3212\n",
      "  Dynamics Model 3 Loss: 1225.8350\n",
      "  Behavior Model 1 Loss: 487.6658\n",
      "  Behavior Model 2 Loss: 478.2299\n",
      "  Behavior Model 3 Loss: 513.4761\n",
      "Epoch 151/500\n",
      "  Dynamics Model 1 Loss: 1162.0342\n",
      "  Dynamics Model 2 Loss: 1713.0610\n",
      "  Dynamics Model 3 Loss: 1225.9851\n",
      "  Behavior Model 1 Loss: 486.9707\n",
      "  Behavior Model 2 Loss: 480.5209\n",
      "  Behavior Model 3 Loss: 512.6187\n",
      "Epoch 152/500\n",
      "  Dynamics Model 1 Loss: 1161.6540\n",
      "  Dynamics Model 2 Loss: 1716.7622\n",
      "  Dynamics Model 3 Loss: 1226.7484\n",
      "  Behavior Model 1 Loss: 487.6313\n",
      "  Behavior Model 2 Loss: 479.5660\n",
      "  Behavior Model 3 Loss: 511.7989\n",
      "Epoch 153/500\n",
      "  Dynamics Model 1 Loss: 1163.2330\n",
      "  Dynamics Model 2 Loss: 1716.1487\n",
      "  Dynamics Model 3 Loss: 1228.8760\n",
      "  Behavior Model 1 Loss: 488.2533\n",
      "  Behavior Model 2 Loss: 478.3608\n",
      "  Behavior Model 3 Loss: 512.1779\n",
      "Epoch 154/500\n",
      "  Dynamics Model 1 Loss: 1162.4853\n",
      "  Dynamics Model 2 Loss: 1715.0219\n",
      "  Dynamics Model 3 Loss: 1226.0546\n",
      "  Behavior Model 1 Loss: 485.5185\n",
      "  Behavior Model 2 Loss: 479.6663\n",
      "  Behavior Model 3 Loss: 513.7987\n",
      "Epoch 155/500\n",
      "  Dynamics Model 1 Loss: 1161.7928\n",
      "  Dynamics Model 2 Loss: 1716.6925\n",
      "  Dynamics Model 3 Loss: 1226.3015\n",
      "  Behavior Model 1 Loss: 491.1654\n",
      "  Behavior Model 2 Loss: 481.5154\n",
      "  Behavior Model 3 Loss: 512.2222\n",
      "Epoch 156/500\n",
      "  Dynamics Model 1 Loss: 1161.5787\n",
      "  Dynamics Model 2 Loss: 1716.2264\n",
      "  Dynamics Model 3 Loss: 1227.0262\n",
      "  Behavior Model 1 Loss: 490.2365\n",
      "  Behavior Model 2 Loss: 480.6942\n",
      "  Behavior Model 3 Loss: 512.2717\n",
      "Epoch 157/500\n",
      "  Dynamics Model 1 Loss: 1163.7725\n",
      "  Dynamics Model 2 Loss: 1712.9967\n",
      "  Dynamics Model 3 Loss: 1225.4576\n",
      "  Behavior Model 1 Loss: 488.6780\n",
      "  Behavior Model 2 Loss: 478.0478\n",
      "  Behavior Model 3 Loss: 513.0565\n",
      "Epoch 158/500\n",
      "  Dynamics Model 1 Loss: 1161.9816\n",
      "  Dynamics Model 2 Loss: 1717.2783\n",
      "  Dynamics Model 3 Loss: 1226.4305\n",
      "  Behavior Model 1 Loss: 488.9108\n",
      "  Behavior Model 2 Loss: 482.0273\n",
      "  Behavior Model 3 Loss: 512.6119\n",
      "Epoch 159/500\n",
      "  Dynamics Model 1 Loss: 1160.9154\n",
      "  Dynamics Model 2 Loss: 1712.2428\n",
      "  Dynamics Model 3 Loss: 1226.0318\n",
      "  Behavior Model 1 Loss: 485.6656\n",
      "  Behavior Model 2 Loss: 479.4789\n",
      "  Behavior Model 3 Loss: 510.6124\n",
      "Epoch 160/500\n",
      "  Dynamics Model 1 Loss: 1164.2474\n",
      "  Dynamics Model 2 Loss: 1714.6377\n",
      "  Dynamics Model 3 Loss: 1227.7558\n",
      "  Behavior Model 1 Loss: 487.5801\n",
      "  Behavior Model 2 Loss: 478.5524\n",
      "  Behavior Model 3 Loss: 514.0010\n",
      "Epoch 161/500\n",
      "  Dynamics Model 1 Loss: 1161.8588\n",
      "  Dynamics Model 2 Loss: 1715.9909\n",
      "  Dynamics Model 3 Loss: 1227.7833\n",
      "  Behavior Model 1 Loss: 489.6594\n",
      "  Behavior Model 2 Loss: 479.9060\n",
      "  Behavior Model 3 Loss: 511.7637\n",
      "Epoch 162/500\n",
      "  Dynamics Model 1 Loss: 1162.5949\n",
      "  Dynamics Model 2 Loss: 1713.7363\n",
      "  Dynamics Model 3 Loss: 1226.0772\n",
      "  Behavior Model 1 Loss: 489.6637\n",
      "  Behavior Model 2 Loss: 480.3086\n",
      "  Behavior Model 3 Loss: 512.2984\n",
      "Epoch 163/500\n",
      "  Dynamics Model 1 Loss: 1162.2078\n",
      "  Dynamics Model 2 Loss: 1713.2788\n",
      "  Dynamics Model 3 Loss: 1225.3868\n",
      "  Behavior Model 1 Loss: 484.1529\n",
      "  Behavior Model 2 Loss: 478.2040\n",
      "  Behavior Model 3 Loss: 513.7908\n",
      "Epoch 164/500\n",
      "  Dynamics Model 1 Loss: 1161.9363\n",
      "  Dynamics Model 2 Loss: 1713.6882\n",
      "  Dynamics Model 3 Loss: 1226.0476\n",
      "  Behavior Model 1 Loss: 486.7941\n",
      "  Behavior Model 2 Loss: 482.1322\n",
      "  Behavior Model 3 Loss: 512.5973\n",
      "Epoch 165/500\n",
      "  Dynamics Model 1 Loss: 1162.1763\n",
      "  Dynamics Model 2 Loss: 1715.7152\n",
      "  Dynamics Model 3 Loss: 1226.8936\n",
      "  Behavior Model 1 Loss: 486.9551\n",
      "  Behavior Model 2 Loss: 481.0401\n",
      "  Behavior Model 3 Loss: 512.7293\n",
      "Epoch 166/500\n",
      "  Dynamics Model 1 Loss: 1161.4841\n",
      "  Dynamics Model 2 Loss: 1715.2583\n",
      "  Dynamics Model 3 Loss: 1226.0758\n",
      "  Behavior Model 1 Loss: 488.2586\n",
      "  Behavior Model 2 Loss: 477.5474\n",
      "  Behavior Model 3 Loss: 513.9278\n",
      "Epoch 167/500\n",
      "  Dynamics Model 1 Loss: 1162.9365\n",
      "  Dynamics Model 2 Loss: 1715.4311\n",
      "  Dynamics Model 3 Loss: 1226.8382\n",
      "  Behavior Model 1 Loss: 488.9886\n",
      "  Behavior Model 2 Loss: 479.5511\n",
      "  Behavior Model 3 Loss: 512.6190\n",
      "Epoch 168/500\n",
      "  Dynamics Model 1 Loss: 1163.0925\n",
      "  Dynamics Model 2 Loss: 1714.8332\n",
      "  Dynamics Model 3 Loss: 1225.7142\n",
      "  Behavior Model 1 Loss: 490.4513\n",
      "  Behavior Model 2 Loss: 479.2553\n",
      "  Behavior Model 3 Loss: 514.9785\n",
      "Epoch 169/500\n",
      "  Dynamics Model 1 Loss: 1162.7856\n",
      "  Dynamics Model 2 Loss: 1716.1648\n",
      "  Dynamics Model 3 Loss: 1226.4793\n",
      "  Behavior Model 1 Loss: 488.0119\n",
      "  Behavior Model 2 Loss: 479.0457\n",
      "  Behavior Model 3 Loss: 514.0864\n",
      "Epoch 170/500\n",
      "  Dynamics Model 1 Loss: 1162.1421\n",
      "  Dynamics Model 2 Loss: 1715.2895\n",
      "  Dynamics Model 3 Loss: 1224.9778\n",
      "  Behavior Model 1 Loss: 490.0592\n",
      "  Behavior Model 2 Loss: 479.2629\n",
      "  Behavior Model 3 Loss: 512.6487\n",
      "Epoch 171/500\n",
      "  Dynamics Model 1 Loss: 1162.2976\n",
      "  Dynamics Model 2 Loss: 1714.0742\n",
      "  Dynamics Model 3 Loss: 1227.4257\n",
      "  Behavior Model 1 Loss: 489.0148\n",
      "  Behavior Model 2 Loss: 480.0528\n",
      "  Behavior Model 3 Loss: 511.8915\n",
      "Epoch 172/500\n",
      "  Dynamics Model 1 Loss: 1164.2021\n",
      "  Dynamics Model 2 Loss: 1715.2975\n",
      "  Dynamics Model 3 Loss: 1227.9316\n",
      "  Behavior Model 1 Loss: 488.1339\n",
      "  Behavior Model 2 Loss: 479.4689\n",
      "  Behavior Model 3 Loss: 512.9821\n",
      "Epoch 173/500\n",
      "  Dynamics Model 1 Loss: 1160.7245\n",
      "  Dynamics Model 2 Loss: 1714.4524\n",
      "  Dynamics Model 3 Loss: 1227.2234\n",
      "  Behavior Model 1 Loss: 488.2815\n",
      "  Behavior Model 2 Loss: 480.2676\n",
      "  Behavior Model 3 Loss: 510.4193\n",
      "Epoch 174/500\n",
      "  Dynamics Model 1 Loss: 1162.1537\n",
      "  Dynamics Model 2 Loss: 1713.4610\n",
      "  Dynamics Model 3 Loss: 1226.9868\n",
      "  Behavior Model 1 Loss: 487.1350\n",
      "  Behavior Model 2 Loss: 477.2734\n",
      "  Behavior Model 3 Loss: 512.6652\n",
      "Epoch 175/500\n",
      "  Dynamics Model 1 Loss: 1162.3440\n",
      "  Dynamics Model 2 Loss: 1713.3695\n",
      "  Dynamics Model 3 Loss: 1226.7546\n",
      "  Behavior Model 1 Loss: 488.3320\n",
      "  Behavior Model 2 Loss: 480.2244\n",
      "  Behavior Model 3 Loss: 511.5442\n",
      "Epoch 176/500\n",
      "  Dynamics Model 1 Loss: 1163.2027\n",
      "  Dynamics Model 2 Loss: 1716.8889\n",
      "  Dynamics Model 3 Loss: 1225.2680\n",
      "  Behavior Model 1 Loss: 488.2249\n",
      "  Behavior Model 2 Loss: 479.9599\n",
      "  Behavior Model 3 Loss: 513.0478\n",
      "Epoch 177/500\n",
      "  Dynamics Model 1 Loss: 1161.4822\n",
      "  Dynamics Model 2 Loss: 1713.9701\n",
      "  Dynamics Model 3 Loss: 1226.7156\n",
      "  Behavior Model 1 Loss: 487.8304\n",
      "  Behavior Model 2 Loss: 479.4276\n",
      "  Behavior Model 3 Loss: 512.6496\n",
      "Epoch 178/500\n",
      "  Dynamics Model 1 Loss: 1162.8282\n",
      "  Dynamics Model 2 Loss: 1717.2127\n",
      "  Dynamics Model 3 Loss: 1225.4976\n",
      "  Behavior Model 1 Loss: 488.6739\n",
      "  Behavior Model 2 Loss: 477.4911\n",
      "  Behavior Model 3 Loss: 513.0155\n",
      "Epoch 179/500\n",
      "  Dynamics Model 1 Loss: 1161.8328\n",
      "  Dynamics Model 2 Loss: 1715.3454\n",
      "  Dynamics Model 3 Loss: 1225.9791\n",
      "  Behavior Model 1 Loss: 491.3338\n",
      "  Behavior Model 2 Loss: 476.6124\n",
      "  Behavior Model 3 Loss: 513.3502\n",
      "Epoch 180/500\n",
      "  Dynamics Model 1 Loss: 1161.5288\n",
      "  Dynamics Model 2 Loss: 1714.7975\n",
      "  Dynamics Model 3 Loss: 1225.4295\n",
      "  Behavior Model 1 Loss: 488.2167\n",
      "  Behavior Model 2 Loss: 480.1036\n",
      "  Behavior Model 3 Loss: 512.3115\n",
      "Epoch 181/500\n",
      "  Dynamics Model 1 Loss: 1161.9287\n",
      "  Dynamics Model 2 Loss: 1715.9543\n",
      "  Dynamics Model 3 Loss: 1225.8780\n",
      "  Behavior Model 1 Loss: 490.0183\n",
      "  Behavior Model 2 Loss: 480.1613\n",
      "  Behavior Model 3 Loss: 511.6750\n",
      "Epoch 182/500\n",
      "  Dynamics Model 1 Loss: 1164.2619\n",
      "  Dynamics Model 2 Loss: 1715.3703\n",
      "  Dynamics Model 3 Loss: 1224.7383\n",
      "  Behavior Model 1 Loss: 487.2611\n",
      "  Behavior Model 2 Loss: 480.7288\n",
      "  Behavior Model 3 Loss: 512.8926\n",
      "Epoch 183/500\n",
      "  Dynamics Model 1 Loss: 1162.5812\n",
      "  Dynamics Model 2 Loss: 1715.4618\n",
      "  Dynamics Model 3 Loss: 1227.4753\n",
      "  Behavior Model 1 Loss: 489.3729\n",
      "  Behavior Model 2 Loss: 480.1454\n",
      "  Behavior Model 3 Loss: 510.4821\n",
      "Epoch 184/500\n",
      "  Dynamics Model 1 Loss: 1163.0164\n",
      "  Dynamics Model 2 Loss: 1716.7024\n",
      "  Dynamics Model 3 Loss: 1226.2529\n",
      "  Behavior Model 1 Loss: 489.5086\n",
      "  Behavior Model 2 Loss: 478.6376\n",
      "  Behavior Model 3 Loss: 514.1455\n",
      "Epoch 185/500\n",
      "  Dynamics Model 1 Loss: 1161.4651\n",
      "  Dynamics Model 2 Loss: 1715.2101\n",
      "  Dynamics Model 3 Loss: 1227.4252\n",
      "  Behavior Model 1 Loss: 487.0988\n",
      "  Behavior Model 2 Loss: 480.8887\n",
      "  Behavior Model 3 Loss: 514.3954\n",
      "Epoch 186/500\n",
      "  Dynamics Model 1 Loss: 1161.9124\n",
      "  Dynamics Model 2 Loss: 1716.7688\n",
      "  Dynamics Model 3 Loss: 1227.6962\n",
      "  Behavior Model 1 Loss: 490.8471\n",
      "  Behavior Model 2 Loss: 479.2665\n",
      "  Behavior Model 3 Loss: 513.4193\n",
      "Epoch 187/500\n",
      "  Dynamics Model 1 Loss: 1163.3310\n",
      "  Dynamics Model 2 Loss: 1713.6702\n",
      "  Dynamics Model 3 Loss: 1227.9546\n",
      "  Behavior Model 1 Loss: 489.4643\n",
      "  Behavior Model 2 Loss: 478.1902\n",
      "  Behavior Model 3 Loss: 512.0003\n",
      "Epoch 188/500\n",
      "  Dynamics Model 1 Loss: 1162.1539\n",
      "  Dynamics Model 2 Loss: 1715.3421\n",
      "  Dynamics Model 3 Loss: 1224.6018\n",
      "  Behavior Model 1 Loss: 485.3694\n",
      "  Behavior Model 2 Loss: 482.2919\n",
      "  Behavior Model 3 Loss: 512.8825\n",
      "Epoch 189/500\n",
      "  Dynamics Model 1 Loss: 1165.0350\n",
      "  Dynamics Model 2 Loss: 1715.7427\n",
      "  Dynamics Model 3 Loss: 1225.0470\n",
      "  Behavior Model 1 Loss: 489.6572\n",
      "  Behavior Model 2 Loss: 479.2116\n",
      "  Behavior Model 3 Loss: 512.3630\n",
      "Epoch 190/500\n",
      "  Dynamics Model 1 Loss: 1163.8600\n",
      "  Dynamics Model 2 Loss: 1713.5099\n",
      "  Dynamics Model 3 Loss: 1224.5991\n",
      "  Behavior Model 1 Loss: 487.5904\n",
      "  Behavior Model 2 Loss: 479.2004\n",
      "  Behavior Model 3 Loss: 511.7564\n",
      "Epoch 191/500\n",
      "  Dynamics Model 1 Loss: 1164.6176\n",
      "  Dynamics Model 2 Loss: 1712.5067\n",
      "  Dynamics Model 3 Loss: 1225.0989\n",
      "  Behavior Model 1 Loss: 486.5476\n",
      "  Behavior Model 2 Loss: 483.0598\n",
      "  Behavior Model 3 Loss: 513.0772\n",
      "Epoch 192/500\n",
      "  Dynamics Model 1 Loss: 1158.8340\n",
      "  Dynamics Model 2 Loss: 1714.0349\n",
      "  Dynamics Model 3 Loss: 1225.7279\n",
      "  Behavior Model 1 Loss: 486.9335\n",
      "  Behavior Model 2 Loss: 480.5500\n",
      "  Behavior Model 3 Loss: 514.1202\n",
      "Epoch 193/500\n",
      "  Dynamics Model 1 Loss: 1161.5993\n",
      "  Dynamics Model 2 Loss: 1714.9892\n",
      "  Dynamics Model 3 Loss: 1228.2170\n",
      "  Behavior Model 1 Loss: 489.3819\n",
      "  Behavior Model 2 Loss: 479.6084\n",
      "  Behavior Model 3 Loss: 513.0200\n",
      "Epoch 194/500\n",
      "  Dynamics Model 1 Loss: 1162.3615\n",
      "  Dynamics Model 2 Loss: 1716.3103\n",
      "  Dynamics Model 3 Loss: 1225.2732\n",
      "  Behavior Model 1 Loss: 489.1027\n",
      "  Behavior Model 2 Loss: 479.5318\n",
      "  Behavior Model 3 Loss: 513.4349\n",
      "Epoch 195/500\n",
      "  Dynamics Model 1 Loss: 1161.9991\n",
      "  Dynamics Model 2 Loss: 1714.2206\n",
      "  Dynamics Model 3 Loss: 1226.6187\n",
      "  Behavior Model 1 Loss: 487.5493\n",
      "  Behavior Model 2 Loss: 479.1460\n",
      "  Behavior Model 3 Loss: 512.1616\n",
      "Epoch 196/500\n",
      "  Dynamics Model 1 Loss: 1161.0146\n",
      "  Dynamics Model 2 Loss: 1715.9974\n",
      "  Dynamics Model 3 Loss: 1226.2395\n",
      "  Behavior Model 1 Loss: 488.5561\n",
      "  Behavior Model 2 Loss: 480.5486\n",
      "  Behavior Model 3 Loss: 512.0193\n",
      "Epoch 197/500\n",
      "  Dynamics Model 1 Loss: 1161.9204\n",
      "  Dynamics Model 2 Loss: 1718.5042\n",
      "  Dynamics Model 3 Loss: 1225.8618\n",
      "  Behavior Model 1 Loss: 488.4097\n",
      "  Behavior Model 2 Loss: 480.4164\n",
      "  Behavior Model 3 Loss: 512.0416\n",
      "Epoch 198/500\n",
      "  Dynamics Model 1 Loss: 1162.9762\n",
      "  Dynamics Model 2 Loss: 1714.0974\n",
      "  Dynamics Model 3 Loss: 1226.9391\n",
      "  Behavior Model 1 Loss: 488.4942\n",
      "  Behavior Model 2 Loss: 477.5820\n",
      "  Behavior Model 3 Loss: 512.3683\n",
      "Epoch 199/500\n",
      "  Dynamics Model 1 Loss: 1162.6945\n",
      "  Dynamics Model 2 Loss: 1714.3492\n",
      "  Dynamics Model 3 Loss: 1224.6892\n",
      "  Behavior Model 1 Loss: 487.2075\n",
      "  Behavior Model 2 Loss: 480.8050\n",
      "  Behavior Model 3 Loss: 512.6839\n",
      "Epoch 200/500\n",
      "  Dynamics Model 1 Loss: 1161.8058\n",
      "  Dynamics Model 2 Loss: 1715.0473\n",
      "  Dynamics Model 3 Loss: 1224.9178\n",
      "  Behavior Model 1 Loss: 488.1861\n",
      "  Behavior Model 2 Loss: 477.7900\n",
      "  Behavior Model 3 Loss: 511.6499\n",
      "Epoch 201/500\n",
      "  Dynamics Model 1 Loss: 1160.6403\n",
      "  Dynamics Model 2 Loss: 1712.1200\n",
      "  Dynamics Model 3 Loss: 1226.6008\n",
      "  Behavior Model 1 Loss: 487.8483\n",
      "  Behavior Model 2 Loss: 478.3583\n",
      "  Behavior Model 3 Loss: 511.6293\n",
      "Epoch 202/500\n",
      "  Dynamics Model 1 Loss: 1162.5949\n",
      "  Dynamics Model 2 Loss: 1717.1319\n",
      "  Dynamics Model 3 Loss: 1224.3820\n",
      "  Behavior Model 1 Loss: 488.7565\n",
      "  Behavior Model 2 Loss: 479.1387\n",
      "  Behavior Model 3 Loss: 512.8063\n",
      "Epoch 203/500\n",
      "  Dynamics Model 1 Loss: 1163.3697\n",
      "  Dynamics Model 2 Loss: 1716.2313\n",
      "  Dynamics Model 3 Loss: 1226.6778\n",
      "  Behavior Model 1 Loss: 488.0405\n",
      "  Behavior Model 2 Loss: 481.6795\n",
      "  Behavior Model 3 Loss: 511.5506\n",
      "Epoch 204/500\n",
      "  Dynamics Model 1 Loss: 1161.1754\n",
      "  Dynamics Model 2 Loss: 1714.9290\n",
      "  Dynamics Model 3 Loss: 1227.0661\n",
      "  Behavior Model 1 Loss: 487.3879\n",
      "  Behavior Model 2 Loss: 480.5443\n",
      "  Behavior Model 3 Loss: 513.5823\n",
      "Epoch 205/500\n",
      "  Dynamics Model 1 Loss: 1163.4770\n",
      "  Dynamics Model 2 Loss: 1714.6672\n",
      "  Dynamics Model 3 Loss: 1227.6708\n",
      "  Behavior Model 1 Loss: 487.9595\n",
      "  Behavior Model 2 Loss: 480.5405\n",
      "  Behavior Model 3 Loss: 510.3769\n",
      "Epoch 206/500\n",
      "  Dynamics Model 1 Loss: 1162.5873\n",
      "  Dynamics Model 2 Loss: 1715.1331\n",
      "  Dynamics Model 3 Loss: 1228.2611\n",
      "  Behavior Model 1 Loss: 488.8555\n",
      "  Behavior Model 2 Loss: 480.7743\n",
      "  Behavior Model 3 Loss: 514.5612\n",
      "Epoch 207/500\n",
      "  Dynamics Model 1 Loss: 1163.5091\n",
      "  Dynamics Model 2 Loss: 1716.3098\n",
      "  Dynamics Model 3 Loss: 1225.4665\n",
      "  Behavior Model 1 Loss: 488.8547\n",
      "  Behavior Model 2 Loss: 480.4769\n",
      "  Behavior Model 3 Loss: 512.4689\n",
      "Epoch 208/500\n",
      "  Dynamics Model 1 Loss: 1163.0630\n",
      "  Dynamics Model 2 Loss: 1716.8637\n",
      "  Dynamics Model 3 Loss: 1225.9224\n",
      "  Behavior Model 1 Loss: 486.2491\n",
      "  Behavior Model 2 Loss: 478.7545\n",
      "  Behavior Model 3 Loss: 512.3399\n",
      "Epoch 209/500\n",
      "  Dynamics Model 1 Loss: 1162.4062\n",
      "  Dynamics Model 2 Loss: 1717.2961\n",
      "  Dynamics Model 3 Loss: 1225.1610\n",
      "  Behavior Model 1 Loss: 488.1831\n",
      "  Behavior Model 2 Loss: 478.7715\n",
      "  Behavior Model 3 Loss: 514.2896\n",
      "Epoch 210/500\n",
      "  Dynamics Model 1 Loss: 1161.8609\n",
      "  Dynamics Model 2 Loss: 1716.0167\n",
      "  Dynamics Model 3 Loss: 1226.6948\n",
      "  Behavior Model 1 Loss: 487.5897\n",
      "  Behavior Model 2 Loss: 476.5534\n",
      "  Behavior Model 3 Loss: 514.7924\n",
      "Epoch 211/500\n",
      "  Dynamics Model 1 Loss: 1165.5980\n",
      "  Dynamics Model 2 Loss: 1713.7595\n",
      "  Dynamics Model 3 Loss: 1226.3639\n",
      "  Behavior Model 1 Loss: 488.3898\n",
      "  Behavior Model 2 Loss: 479.1306\n",
      "  Behavior Model 3 Loss: 513.9356\n",
      "Epoch 212/500\n",
      "  Dynamics Model 1 Loss: 1163.4586\n",
      "  Dynamics Model 2 Loss: 1717.0631\n",
      "  Dynamics Model 3 Loss: 1226.3811\n",
      "  Behavior Model 1 Loss: 489.3741\n",
      "  Behavior Model 2 Loss: 479.4628\n",
      "  Behavior Model 3 Loss: 512.5700\n",
      "Epoch 213/500\n",
      "  Dynamics Model 1 Loss: 1161.4276\n",
      "  Dynamics Model 2 Loss: 1712.7297\n",
      "  Dynamics Model 3 Loss: 1228.3197\n",
      "  Behavior Model 1 Loss: 488.3215\n",
      "  Behavior Model 2 Loss: 477.5712\n",
      "  Behavior Model 3 Loss: 510.6656\n",
      "Epoch 214/500\n",
      "  Dynamics Model 1 Loss: 1162.3906\n",
      "  Dynamics Model 2 Loss: 1714.5145\n",
      "  Dynamics Model 3 Loss: 1227.8343\n",
      "  Behavior Model 1 Loss: 485.4083\n",
      "  Behavior Model 2 Loss: 476.8639\n",
      "  Behavior Model 3 Loss: 514.1788\n",
      "Epoch 215/500\n",
      "  Dynamics Model 1 Loss: 1163.3229\n",
      "  Dynamics Model 2 Loss: 1713.0385\n",
      "  Dynamics Model 3 Loss: 1225.4392\n",
      "  Behavior Model 1 Loss: 487.7542\n",
      "  Behavior Model 2 Loss: 476.5206\n",
      "  Behavior Model 3 Loss: 512.2592\n",
      "Epoch 216/500\n",
      "  Dynamics Model 1 Loss: 1162.8400\n",
      "  Dynamics Model 2 Loss: 1714.9534\n",
      "  Dynamics Model 3 Loss: 1226.2355\n",
      "  Behavior Model 1 Loss: 486.9749\n",
      "  Behavior Model 2 Loss: 478.4561\n",
      "  Behavior Model 3 Loss: 514.4598\n",
      "Epoch 217/500\n",
      "  Dynamics Model 1 Loss: 1160.6278\n",
      "  Dynamics Model 2 Loss: 1714.6971\n",
      "  Dynamics Model 3 Loss: 1227.9762\n",
      "  Behavior Model 1 Loss: 486.3935\n",
      "  Behavior Model 2 Loss: 480.6970\n",
      "  Behavior Model 3 Loss: 513.6260\n",
      "Epoch 218/500\n",
      "  Dynamics Model 1 Loss: 1164.3891\n",
      "  Dynamics Model 2 Loss: 1715.3883\n",
      "  Dynamics Model 3 Loss: 1227.9686\n",
      "  Behavior Model 1 Loss: 487.7042\n",
      "  Behavior Model 2 Loss: 479.2177\n",
      "  Behavior Model 3 Loss: 511.5745\n",
      "Epoch 219/500\n",
      "  Dynamics Model 1 Loss: 1164.4385\n",
      "  Dynamics Model 2 Loss: 1714.5078\n",
      "  Dynamics Model 3 Loss: 1227.0660\n",
      "  Behavior Model 1 Loss: 487.4313\n",
      "  Behavior Model 2 Loss: 479.9177\n",
      "  Behavior Model 3 Loss: 512.8191\n",
      "Epoch 220/500\n",
      "  Dynamics Model 1 Loss: 1161.7642\n",
      "  Dynamics Model 2 Loss: 1714.7497\n",
      "  Dynamics Model 3 Loss: 1226.2068\n",
      "  Behavior Model 1 Loss: 488.2671\n",
      "  Behavior Model 2 Loss: 479.0206\n",
      "  Behavior Model 3 Loss: 513.7126\n",
      "Epoch 221/500\n",
      "  Dynamics Model 1 Loss: 1160.1498\n",
      "  Dynamics Model 2 Loss: 1717.4594\n",
      "  Dynamics Model 3 Loss: 1227.1185\n",
      "  Behavior Model 1 Loss: 488.7835\n",
      "  Behavior Model 2 Loss: 480.5388\n",
      "  Behavior Model 3 Loss: 514.3690\n",
      "Epoch 222/500\n",
      "  Dynamics Model 1 Loss: 1163.5248\n",
      "  Dynamics Model 2 Loss: 1716.0079\n",
      "  Dynamics Model 3 Loss: 1224.7577\n",
      "  Behavior Model 1 Loss: 488.9130\n",
      "  Behavior Model 2 Loss: 477.4812\n",
      "  Behavior Model 3 Loss: 512.5181\n",
      "Epoch 223/500\n",
      "  Dynamics Model 1 Loss: 1164.8673\n",
      "  Dynamics Model 2 Loss: 1714.2087\n",
      "  Dynamics Model 3 Loss: 1226.9514\n",
      "  Behavior Model 1 Loss: 488.4521\n",
      "  Behavior Model 2 Loss: 479.1931\n",
      "  Behavior Model 3 Loss: 514.5720\n",
      "Epoch 224/500\n",
      "  Dynamics Model 1 Loss: 1165.5249\n",
      "  Dynamics Model 2 Loss: 1714.2660\n",
      "  Dynamics Model 3 Loss: 1224.5573\n",
      "  Behavior Model 1 Loss: 489.1664\n",
      "  Behavior Model 2 Loss: 478.8534\n",
      "  Behavior Model 3 Loss: 512.2781\n",
      "Epoch 225/500\n",
      "  Dynamics Model 1 Loss: 1159.6598\n",
      "  Dynamics Model 2 Loss: 1715.6583\n",
      "  Dynamics Model 3 Loss: 1225.2980\n",
      "  Behavior Model 1 Loss: 489.8060\n",
      "  Behavior Model 2 Loss: 478.5857\n",
      "  Behavior Model 3 Loss: 512.2881\n",
      "Epoch 226/500\n",
      "  Dynamics Model 1 Loss: 1161.5484\n",
      "  Dynamics Model 2 Loss: 1714.7036\n",
      "  Dynamics Model 3 Loss: 1226.6833\n",
      "  Behavior Model 1 Loss: 486.5808\n",
      "  Behavior Model 2 Loss: 477.8088\n",
      "  Behavior Model 3 Loss: 515.5741\n",
      "Epoch 227/500\n",
      "  Dynamics Model 1 Loss: 1162.4538\n",
      "  Dynamics Model 2 Loss: 1715.6598\n",
      "  Dynamics Model 3 Loss: 1225.8153\n",
      "  Behavior Model 1 Loss: 487.4816\n",
      "  Behavior Model 2 Loss: 479.4442\n",
      "  Behavior Model 3 Loss: 512.7316\n",
      "Epoch 228/500\n",
      "  Dynamics Model 1 Loss: 1165.5427\n",
      "  Dynamics Model 2 Loss: 1713.8661\n",
      "  Dynamics Model 3 Loss: 1226.9478\n",
      "  Behavior Model 1 Loss: 486.7523\n",
      "  Behavior Model 2 Loss: 478.4535\n",
      "  Behavior Model 3 Loss: 512.5619\n",
      "Epoch 229/500\n",
      "  Dynamics Model 1 Loss: 1162.9311\n",
      "  Dynamics Model 2 Loss: 1711.1075\n",
      "  Dynamics Model 3 Loss: 1226.7408\n",
      "  Behavior Model 1 Loss: 489.4643\n",
      "  Behavior Model 2 Loss: 480.2093\n",
      "  Behavior Model 3 Loss: 514.1038\n",
      "Epoch 230/500\n",
      "  Dynamics Model 1 Loss: 1162.9561\n",
      "  Dynamics Model 2 Loss: 1716.2493\n",
      "  Dynamics Model 3 Loss: 1225.7292\n",
      "  Behavior Model 1 Loss: 486.7441\n",
      "  Behavior Model 2 Loss: 477.9155\n",
      "  Behavior Model 3 Loss: 514.1942\n",
      "Epoch 231/500\n",
      "  Dynamics Model 1 Loss: 1162.1629\n",
      "  Dynamics Model 2 Loss: 1715.2614\n",
      "  Dynamics Model 3 Loss: 1226.4365\n",
      "  Behavior Model 1 Loss: 487.3567\n",
      "  Behavior Model 2 Loss: 477.0536\n",
      "  Behavior Model 3 Loss: 512.7367\n",
      "Epoch 232/500\n",
      "  Dynamics Model 1 Loss: 1161.1923\n",
      "  Dynamics Model 2 Loss: 1714.2503\n",
      "  Dynamics Model 3 Loss: 1223.5784\n",
      "  Behavior Model 1 Loss: 487.8644\n",
      "  Behavior Model 2 Loss: 479.9172\n",
      "  Behavior Model 3 Loss: 509.9933\n",
      "Epoch 233/500\n",
      "  Dynamics Model 1 Loss: 1163.3040\n",
      "  Dynamics Model 2 Loss: 1715.8010\n",
      "  Dynamics Model 3 Loss: 1227.1052\n",
      "  Behavior Model 1 Loss: 488.0423\n",
      "  Behavior Model 2 Loss: 479.5648\n",
      "  Behavior Model 3 Loss: 511.7000\n",
      "Epoch 234/500\n",
      "  Dynamics Model 1 Loss: 1165.0835\n",
      "  Dynamics Model 2 Loss: 1714.0883\n",
      "  Dynamics Model 3 Loss: 1223.7203\n",
      "  Behavior Model 1 Loss: 488.3304\n",
      "  Behavior Model 2 Loss: 479.7626\n",
      "  Behavior Model 3 Loss: 513.7354\n",
      "Epoch 235/500\n",
      "  Dynamics Model 1 Loss: 1162.6190\n",
      "  Dynamics Model 2 Loss: 1712.4569\n",
      "  Dynamics Model 3 Loss: 1228.1242\n",
      "  Behavior Model 1 Loss: 488.8506\n",
      "  Behavior Model 2 Loss: 478.5874\n",
      "  Behavior Model 3 Loss: 514.3371\n",
      "Epoch 236/500\n",
      "  Dynamics Model 1 Loss: 1163.7275\n",
      "  Dynamics Model 2 Loss: 1714.7939\n",
      "  Dynamics Model 3 Loss: 1225.7146\n",
      "  Behavior Model 1 Loss: 489.9697\n",
      "  Behavior Model 2 Loss: 479.5559\n",
      "  Behavior Model 3 Loss: 513.1657\n",
      "Epoch 237/500\n",
      "  Dynamics Model 1 Loss: 1162.1856\n",
      "  Dynamics Model 2 Loss: 1714.7784\n",
      "  Dynamics Model 3 Loss: 1226.9715\n",
      "  Behavior Model 1 Loss: 489.5524\n",
      "  Behavior Model 2 Loss: 475.5818\n",
      "  Behavior Model 3 Loss: 512.0285\n",
      "Epoch 238/500\n",
      "  Dynamics Model 1 Loss: 1162.3711\n",
      "  Dynamics Model 2 Loss: 1713.6032\n",
      "  Dynamics Model 3 Loss: 1226.4244\n",
      "  Behavior Model 1 Loss: 487.2353\n",
      "  Behavior Model 2 Loss: 480.0415\n",
      "  Behavior Model 3 Loss: 513.2286\n",
      "Epoch 239/500\n",
      "  Dynamics Model 1 Loss: 1161.7589\n",
      "  Dynamics Model 2 Loss: 1717.2629\n",
      "  Dynamics Model 3 Loss: 1225.2952\n",
      "  Behavior Model 1 Loss: 486.2769\n",
      "  Behavior Model 2 Loss: 479.0274\n",
      "  Behavior Model 3 Loss: 512.7584\n",
      "Epoch 240/500\n",
      "  Dynamics Model 1 Loss: 1162.0014\n",
      "  Dynamics Model 2 Loss: 1714.0793\n",
      "  Dynamics Model 3 Loss: 1224.2282\n",
      "  Behavior Model 1 Loss: 488.4110\n",
      "  Behavior Model 2 Loss: 479.0754\n",
      "  Behavior Model 3 Loss: 510.0107\n",
      "Epoch 241/500\n",
      "  Dynamics Model 1 Loss: 1163.1018\n",
      "  Dynamics Model 2 Loss: 1713.3001\n",
      "  Dynamics Model 3 Loss: 1226.4091\n",
      "  Behavior Model 1 Loss: 488.8368\n",
      "  Behavior Model 2 Loss: 478.2844\n",
      "  Behavior Model 3 Loss: 512.8924\n",
      "Epoch 242/500\n",
      "  Dynamics Model 1 Loss: 1164.1427\n",
      "  Dynamics Model 2 Loss: 1711.5409\n",
      "  Dynamics Model 3 Loss: 1225.8674\n",
      "  Behavior Model 1 Loss: 488.5312\n",
      "  Behavior Model 2 Loss: 479.2734\n",
      "  Behavior Model 3 Loss: 511.1461\n",
      "Epoch 243/500\n",
      "  Dynamics Model 1 Loss: 1162.2004\n",
      "  Dynamics Model 2 Loss: 1716.1147\n",
      "  Dynamics Model 3 Loss: 1225.5878\n",
      "  Behavior Model 1 Loss: 487.2191\n",
      "  Behavior Model 2 Loss: 480.6226\n",
      "  Behavior Model 3 Loss: 513.9251\n",
      "Epoch 244/500\n",
      "  Dynamics Model 1 Loss: 1162.0575\n",
      "  Dynamics Model 2 Loss: 1715.4898\n",
      "  Dynamics Model 3 Loss: 1228.6751\n",
      "  Behavior Model 1 Loss: 488.9680\n",
      "  Behavior Model 2 Loss: 477.5996\n",
      "  Behavior Model 3 Loss: 511.9249\n",
      "Epoch 245/500\n",
      "  Dynamics Model 1 Loss: 1163.9533\n",
      "  Dynamics Model 2 Loss: 1713.3566\n",
      "  Dynamics Model 3 Loss: 1225.1529\n",
      "  Behavior Model 1 Loss: 487.4531\n",
      "  Behavior Model 2 Loss: 478.7637\n",
      "  Behavior Model 3 Loss: 512.7852\n",
      "Epoch 246/500\n",
      "  Dynamics Model 1 Loss: 1162.0586\n",
      "  Dynamics Model 2 Loss: 1715.3490\n",
      "  Dynamics Model 3 Loss: 1227.3018\n",
      "  Behavior Model 1 Loss: 487.4605\n",
      "  Behavior Model 2 Loss: 479.6586\n",
      "  Behavior Model 3 Loss: 510.9938\n",
      "Epoch 247/500\n",
      "  Dynamics Model 1 Loss: 1161.7694\n",
      "  Dynamics Model 2 Loss: 1718.0844\n",
      "  Dynamics Model 3 Loss: 1228.4160\n",
      "  Behavior Model 1 Loss: 486.5519\n",
      "  Behavior Model 2 Loss: 479.9641\n",
      "  Behavior Model 3 Loss: 510.7202\n",
      "Epoch 248/500\n",
      "  Dynamics Model 1 Loss: 1163.1183\n",
      "  Dynamics Model 2 Loss: 1716.4482\n",
      "  Dynamics Model 3 Loss: 1227.0143\n",
      "  Behavior Model 1 Loss: 487.9791\n",
      "  Behavior Model 2 Loss: 481.2880\n",
      "  Behavior Model 3 Loss: 512.8347\n",
      "Epoch 249/500\n",
      "  Dynamics Model 1 Loss: 1161.9596\n",
      "  Dynamics Model 2 Loss: 1715.6891\n",
      "  Dynamics Model 3 Loss: 1227.5281\n",
      "  Behavior Model 1 Loss: 489.7609\n",
      "  Behavior Model 2 Loss: 478.6928\n",
      "  Behavior Model 3 Loss: 513.6356\n",
      "Epoch 250/500\n",
      "  Dynamics Model 1 Loss: 1163.1599\n",
      "  Dynamics Model 2 Loss: 1715.1164\n",
      "  Dynamics Model 3 Loss: 1226.9006\n",
      "  Behavior Model 1 Loss: 485.5998\n",
      "  Behavior Model 2 Loss: 479.6834\n",
      "  Behavior Model 3 Loss: 514.2592\n",
      "Epoch 251/500\n",
      "  Dynamics Model 1 Loss: 1162.3660\n",
      "  Dynamics Model 2 Loss: 1714.9270\n",
      "  Dynamics Model 3 Loss: 1226.7023\n",
      "  Behavior Model 1 Loss: 488.0115\n",
      "  Behavior Model 2 Loss: 481.3775\n",
      "  Behavior Model 3 Loss: 510.5349\n",
      "Epoch 252/500\n",
      "  Dynamics Model 1 Loss: 1163.2251\n",
      "  Dynamics Model 2 Loss: 1712.1061\n",
      "  Dynamics Model 3 Loss: 1226.0679\n",
      "  Behavior Model 1 Loss: 490.1908\n",
      "  Behavior Model 2 Loss: 479.8058\n",
      "  Behavior Model 3 Loss: 510.5588\n",
      "Epoch 253/500\n",
      "  Dynamics Model 1 Loss: 1160.7934\n",
      "  Dynamics Model 2 Loss: 1714.6569\n",
      "  Dynamics Model 3 Loss: 1225.4525\n",
      "  Behavior Model 1 Loss: 487.4814\n",
      "  Behavior Model 2 Loss: 479.6305\n",
      "  Behavior Model 3 Loss: 512.5141\n",
      "Epoch 254/500\n",
      "  Dynamics Model 1 Loss: 1163.4918\n",
      "  Dynamics Model 2 Loss: 1714.7491\n",
      "  Dynamics Model 3 Loss: 1224.3152\n",
      "  Behavior Model 1 Loss: 490.5016\n",
      "  Behavior Model 2 Loss: 479.2751\n",
      "  Behavior Model 3 Loss: 513.0164\n",
      "Epoch 255/500\n",
      "  Dynamics Model 1 Loss: 1162.7817\n",
      "  Dynamics Model 2 Loss: 1715.3926\n",
      "  Dynamics Model 3 Loss: 1225.2359\n",
      "  Behavior Model 1 Loss: 485.7381\n",
      "  Behavior Model 2 Loss: 479.1148\n",
      "  Behavior Model 3 Loss: 510.9173\n",
      "Epoch 256/500\n",
      "  Dynamics Model 1 Loss: 1161.4927\n",
      "  Dynamics Model 2 Loss: 1713.4876\n",
      "  Dynamics Model 3 Loss: 1224.2031\n",
      "  Behavior Model 1 Loss: 489.1917\n",
      "  Behavior Model 2 Loss: 478.5950\n",
      "  Behavior Model 3 Loss: 514.2514\n",
      "Epoch 257/500\n",
      "  Dynamics Model 1 Loss: 1161.4545\n",
      "  Dynamics Model 2 Loss: 1714.3904\n",
      "  Dynamics Model 3 Loss: 1224.2295\n",
      "  Behavior Model 1 Loss: 487.1542\n",
      "  Behavior Model 2 Loss: 480.7930\n",
      "  Behavior Model 3 Loss: 512.0396\n",
      "Epoch 258/500\n",
      "  Dynamics Model 1 Loss: 1162.1794\n",
      "  Dynamics Model 2 Loss: 1716.7555\n",
      "  Dynamics Model 3 Loss: 1226.4132\n",
      "  Behavior Model 1 Loss: 489.3638\n",
      "  Behavior Model 2 Loss: 479.5762\n",
      "  Behavior Model 3 Loss: 513.3156\n",
      "Epoch 259/500\n",
      "  Dynamics Model 1 Loss: 1162.2460\n",
      "  Dynamics Model 2 Loss: 1715.0036\n",
      "  Dynamics Model 3 Loss: 1227.3439\n",
      "  Behavior Model 1 Loss: 487.9604\n",
      "  Behavior Model 2 Loss: 479.6301\n",
      "  Behavior Model 3 Loss: 513.3953\n",
      "Epoch 260/500\n",
      "  Dynamics Model 1 Loss: 1163.6219\n",
      "  Dynamics Model 2 Loss: 1716.4518\n",
      "  Dynamics Model 3 Loss: 1226.2492\n",
      "  Behavior Model 1 Loss: 488.2762\n",
      "  Behavior Model 2 Loss: 479.3184\n",
      "  Behavior Model 3 Loss: 512.2241\n",
      "Epoch 261/500\n",
      "  Dynamics Model 1 Loss: 1161.6124\n",
      "  Dynamics Model 2 Loss: 1714.9638\n",
      "  Dynamics Model 3 Loss: 1224.0129\n",
      "  Behavior Model 1 Loss: 486.1393\n",
      "  Behavior Model 2 Loss: 479.4205\n",
      "  Behavior Model 3 Loss: 512.8993\n",
      "Epoch 262/500\n",
      "  Dynamics Model 1 Loss: 1162.0258\n",
      "  Dynamics Model 2 Loss: 1715.2221\n",
      "  Dynamics Model 3 Loss: 1225.1096\n",
      "  Behavior Model 1 Loss: 487.5229\n",
      "  Behavior Model 2 Loss: 478.3103\n",
      "  Behavior Model 3 Loss: 511.6275\n",
      "Epoch 263/500\n",
      "  Dynamics Model 1 Loss: 1161.8069\n",
      "  Dynamics Model 2 Loss: 1715.7693\n",
      "  Dynamics Model 3 Loss: 1226.2653\n",
      "  Behavior Model 1 Loss: 487.0869\n",
      "  Behavior Model 2 Loss: 478.5682\n",
      "  Behavior Model 3 Loss: 512.7795\n",
      "Epoch 264/500\n",
      "  Dynamics Model 1 Loss: 1162.1881\n",
      "  Dynamics Model 2 Loss: 1717.3726\n",
      "  Dynamics Model 3 Loss: 1227.1282\n",
      "  Behavior Model 1 Loss: 487.9817\n",
      "  Behavior Model 2 Loss: 479.6678\n",
      "  Behavior Model 3 Loss: 513.9631\n",
      "Epoch 265/500\n",
      "  Dynamics Model 1 Loss: 1161.4149\n",
      "  Dynamics Model 2 Loss: 1715.0241\n",
      "  Dynamics Model 3 Loss: 1226.7156\n",
      "  Behavior Model 1 Loss: 488.6803\n",
      "  Behavior Model 2 Loss: 480.1121\n",
      "  Behavior Model 3 Loss: 514.6812\n",
      "Epoch 266/500\n",
      "  Dynamics Model 1 Loss: 1162.5709\n",
      "  Dynamics Model 2 Loss: 1714.1539\n",
      "  Dynamics Model 3 Loss: 1226.9559\n",
      "  Behavior Model 1 Loss: 487.6004\n",
      "  Behavior Model 2 Loss: 479.8365\n",
      "  Behavior Model 3 Loss: 512.9637\n",
      "Epoch 267/500\n",
      "  Dynamics Model 1 Loss: 1162.1328\n",
      "  Dynamics Model 2 Loss: 1715.9149\n",
      "  Dynamics Model 3 Loss: 1225.8951\n",
      "  Behavior Model 1 Loss: 488.8440\n",
      "  Behavior Model 2 Loss: 479.8124\n",
      "  Behavior Model 3 Loss: 512.1227\n",
      "Epoch 268/500\n",
      "  Dynamics Model 1 Loss: 1162.1674\n",
      "  Dynamics Model 2 Loss: 1716.6523\n",
      "  Dynamics Model 3 Loss: 1225.2831\n",
      "  Behavior Model 1 Loss: 488.0307\n",
      "  Behavior Model 2 Loss: 481.2344\n",
      "  Behavior Model 3 Loss: 511.5323\n",
      "Epoch 269/500\n",
      "  Dynamics Model 1 Loss: 1162.2290\n",
      "  Dynamics Model 2 Loss: 1714.2686\n",
      "  Dynamics Model 3 Loss: 1226.2538\n",
      "  Behavior Model 1 Loss: 491.4593\n",
      "  Behavior Model 2 Loss: 478.8280\n",
      "  Behavior Model 3 Loss: 512.5190\n",
      "Epoch 270/500\n",
      "  Dynamics Model 1 Loss: 1161.6413\n",
      "  Dynamics Model 2 Loss: 1712.3963\n",
      "  Dynamics Model 3 Loss: 1226.4595\n",
      "  Behavior Model 1 Loss: 487.7252\n",
      "  Behavior Model 2 Loss: 479.1710\n",
      "  Behavior Model 3 Loss: 513.5739\n",
      "Epoch 271/500\n",
      "  Dynamics Model 1 Loss: 1161.1886\n",
      "  Dynamics Model 2 Loss: 1716.3714\n",
      "  Dynamics Model 3 Loss: 1226.6571\n",
      "  Behavior Model 1 Loss: 488.7715\n",
      "  Behavior Model 2 Loss: 477.7120\n",
      "  Behavior Model 3 Loss: 513.6382\n",
      "Epoch 272/500\n",
      "  Dynamics Model 1 Loss: 1160.1595\n",
      "  Dynamics Model 2 Loss: 1714.8974\n",
      "  Dynamics Model 3 Loss: 1225.4096\n",
      "  Behavior Model 1 Loss: 487.6031\n",
      "  Behavior Model 2 Loss: 478.9340\n",
      "  Behavior Model 3 Loss: 512.4092\n",
      "Epoch 273/500\n",
      "  Dynamics Model 1 Loss: 1162.9759\n",
      "  Dynamics Model 2 Loss: 1716.0538\n",
      "  Dynamics Model 3 Loss: 1226.3324\n",
      "  Behavior Model 1 Loss: 487.4568\n",
      "  Behavior Model 2 Loss: 479.2825\n",
      "  Behavior Model 3 Loss: 512.2414\n",
      "Epoch 274/500\n",
      "  Dynamics Model 1 Loss: 1161.9433\n",
      "  Dynamics Model 2 Loss: 1715.8638\n",
      "  Dynamics Model 3 Loss: 1226.1032\n",
      "  Behavior Model 1 Loss: 488.4034\n",
      "  Behavior Model 2 Loss: 476.8544\n",
      "  Behavior Model 3 Loss: 511.0364\n",
      "Epoch 275/500\n",
      "  Dynamics Model 1 Loss: 1162.3169\n",
      "  Dynamics Model 2 Loss: 1713.1565\n",
      "  Dynamics Model 3 Loss: 1226.3310\n",
      "  Behavior Model 1 Loss: 488.8918\n",
      "  Behavior Model 2 Loss: 479.1450\n",
      "  Behavior Model 3 Loss: 510.6435\n",
      "Epoch 276/500\n",
      "  Dynamics Model 1 Loss: 1162.7341\n",
      "  Dynamics Model 2 Loss: 1713.1681\n",
      "  Dynamics Model 3 Loss: 1224.3362\n",
      "  Behavior Model 1 Loss: 488.7974\n",
      "  Behavior Model 2 Loss: 478.5846\n",
      "  Behavior Model 3 Loss: 511.5833\n",
      "Epoch 277/500\n",
      "  Dynamics Model 1 Loss: 1163.7472\n",
      "  Dynamics Model 2 Loss: 1713.4966\n",
      "  Dynamics Model 3 Loss: 1226.6640\n",
      "  Behavior Model 1 Loss: 488.8652\n",
      "  Behavior Model 2 Loss: 479.8755\n",
      "  Behavior Model 3 Loss: 514.9334\n",
      "Epoch 278/500\n",
      "  Dynamics Model 1 Loss: 1163.5720\n",
      "  Dynamics Model 2 Loss: 1712.9598\n",
      "  Dynamics Model 3 Loss: 1226.0611\n",
      "  Behavior Model 1 Loss: 487.9823\n",
      "  Behavior Model 2 Loss: 478.1031\n",
      "  Behavior Model 3 Loss: 512.5449\n",
      "Epoch 279/500\n",
      "  Dynamics Model 1 Loss: 1162.6408\n",
      "  Dynamics Model 2 Loss: 1714.1569\n",
      "  Dynamics Model 3 Loss: 1225.5090\n",
      "  Behavior Model 1 Loss: 490.3149\n",
      "  Behavior Model 2 Loss: 480.0929\n",
      "  Behavior Model 3 Loss: 511.9258\n",
      "Epoch 280/500\n",
      "  Dynamics Model 1 Loss: 1161.0120\n",
      "  Dynamics Model 2 Loss: 1714.8036\n",
      "  Dynamics Model 3 Loss: 1224.8952\n",
      "  Behavior Model 1 Loss: 490.5200\n",
      "  Behavior Model 2 Loss: 478.7975\n",
      "  Behavior Model 3 Loss: 513.3936\n",
      "Epoch 281/500\n",
      "  Dynamics Model 1 Loss: 1164.3462\n",
      "  Dynamics Model 2 Loss: 1713.9007\n",
      "  Dynamics Model 3 Loss: 1224.7579\n",
      "  Behavior Model 1 Loss: 488.6752\n",
      "  Behavior Model 2 Loss: 478.8576\n",
      "  Behavior Model 3 Loss: 510.5806\n",
      "Epoch 282/500\n",
      "  Dynamics Model 1 Loss: 1163.0113\n",
      "  Dynamics Model 2 Loss: 1715.0155\n",
      "  Dynamics Model 3 Loss: 1225.8961\n",
      "  Behavior Model 1 Loss: 488.9963\n",
      "  Behavior Model 2 Loss: 477.7806\n",
      "  Behavior Model 3 Loss: 512.8045\n",
      "Epoch 283/500\n",
      "  Dynamics Model 1 Loss: 1163.0481\n",
      "  Dynamics Model 2 Loss: 1715.3681\n",
      "  Dynamics Model 3 Loss: 1225.0398\n",
      "  Behavior Model 1 Loss: 489.6027\n",
      "  Behavior Model 2 Loss: 479.0189\n",
      "  Behavior Model 3 Loss: 511.7247\n",
      "Epoch 284/500\n",
      "  Dynamics Model 1 Loss: 1162.2012\n",
      "  Dynamics Model 2 Loss: 1717.6029\n",
      "  Dynamics Model 3 Loss: 1225.2766\n",
      "  Behavior Model 1 Loss: 488.1440\n",
      "  Behavior Model 2 Loss: 479.9117\n",
      "  Behavior Model 3 Loss: 512.8481\n",
      "Epoch 285/500\n",
      "  Dynamics Model 1 Loss: 1163.9482\n",
      "  Dynamics Model 2 Loss: 1715.7576\n",
      "  Dynamics Model 3 Loss: 1225.7636\n",
      "  Behavior Model 1 Loss: 487.0650\n",
      "  Behavior Model 2 Loss: 479.7186\n",
      "  Behavior Model 3 Loss: 510.6858\n",
      "Epoch 286/500\n",
      "  Dynamics Model 1 Loss: 1163.6153\n",
      "  Dynamics Model 2 Loss: 1715.9468\n",
      "  Dynamics Model 3 Loss: 1227.3206\n",
      "  Behavior Model 1 Loss: 486.7860\n",
      "  Behavior Model 2 Loss: 479.9225\n",
      "  Behavior Model 3 Loss: 513.5878\n",
      "Epoch 287/500\n",
      "  Dynamics Model 1 Loss: 1162.6020\n",
      "  Dynamics Model 2 Loss: 1715.8870\n",
      "  Dynamics Model 3 Loss: 1225.1277\n",
      "  Behavior Model 1 Loss: 487.8973\n",
      "  Behavior Model 2 Loss: 479.4591\n",
      "  Behavior Model 3 Loss: 513.3307\n",
      "Epoch 288/500\n",
      "  Dynamics Model 1 Loss: 1162.6245\n",
      "  Dynamics Model 2 Loss: 1714.9004\n",
      "  Dynamics Model 3 Loss: 1224.3343\n",
      "  Behavior Model 1 Loss: 487.0876\n",
      "  Behavior Model 2 Loss: 478.4045\n",
      "  Behavior Model 3 Loss: 512.6695\n",
      "Epoch 289/500\n",
      "  Dynamics Model 1 Loss: 1163.0568\n",
      "  Dynamics Model 2 Loss: 1713.6310\n",
      "  Dynamics Model 3 Loss: 1224.4004\n",
      "  Behavior Model 1 Loss: 488.7349\n",
      "  Behavior Model 2 Loss: 478.4156\n",
      "  Behavior Model 3 Loss: 513.0893\n",
      "Epoch 290/500\n",
      "  Dynamics Model 1 Loss: 1161.5108\n",
      "  Dynamics Model 2 Loss: 1712.7463\n",
      "  Dynamics Model 3 Loss: 1226.7844\n",
      "  Behavior Model 1 Loss: 487.0738\n",
      "  Behavior Model 2 Loss: 478.0594\n",
      "  Behavior Model 3 Loss: 510.3990\n",
      "Epoch 291/500\n",
      "  Dynamics Model 1 Loss: 1162.5622\n",
      "  Dynamics Model 2 Loss: 1714.1829\n",
      "  Dynamics Model 3 Loss: 1226.5307\n",
      "  Behavior Model 1 Loss: 490.0179\n",
      "  Behavior Model 2 Loss: 480.2598\n",
      "  Behavior Model 3 Loss: 509.5377\n",
      "Epoch 292/500\n",
      "  Dynamics Model 1 Loss: 1161.6792\n",
      "  Dynamics Model 2 Loss: 1714.3929\n",
      "  Dynamics Model 3 Loss: 1226.3499\n",
      "  Behavior Model 1 Loss: 488.8230\n",
      "  Behavior Model 2 Loss: 480.7650\n",
      "  Behavior Model 3 Loss: 514.3024\n",
      "Epoch 293/500\n",
      "  Dynamics Model 1 Loss: 1163.4366\n",
      "  Dynamics Model 2 Loss: 1712.5312\n",
      "  Dynamics Model 3 Loss: 1226.4539\n",
      "  Behavior Model 1 Loss: 487.0310\n",
      "  Behavior Model 2 Loss: 478.3114\n",
      "  Behavior Model 3 Loss: 512.4287\n",
      "Epoch 294/500\n",
      "  Dynamics Model 1 Loss: 1160.1809\n",
      "  Dynamics Model 2 Loss: 1714.5948\n",
      "  Dynamics Model 3 Loss: 1226.4468\n",
      "  Behavior Model 1 Loss: 487.6856\n",
      "  Behavior Model 2 Loss: 479.8124\n",
      "  Behavior Model 3 Loss: 515.5950\n",
      "Epoch 295/500\n",
      "  Dynamics Model 1 Loss: 1162.5190\n",
      "  Dynamics Model 2 Loss: 1713.6863\n",
      "  Dynamics Model 3 Loss: 1226.1880\n",
      "  Behavior Model 1 Loss: 487.3879\n",
      "  Behavior Model 2 Loss: 478.2035\n",
      "  Behavior Model 3 Loss: 512.9922\n",
      "Epoch 296/500\n",
      "  Dynamics Model 1 Loss: 1160.9448\n",
      "  Dynamics Model 2 Loss: 1714.7603\n",
      "  Dynamics Model 3 Loss: 1226.6141\n",
      "  Behavior Model 1 Loss: 486.8951\n",
      "  Behavior Model 2 Loss: 481.1313\n",
      "  Behavior Model 3 Loss: 514.3468\n",
      "Epoch 297/500\n",
      "  Dynamics Model 1 Loss: 1161.2442\n",
      "  Dynamics Model 2 Loss: 1714.6663\n",
      "  Dynamics Model 3 Loss: 1226.7179\n",
      "  Behavior Model 1 Loss: 487.8622\n",
      "  Behavior Model 2 Loss: 479.0363\n",
      "  Behavior Model 3 Loss: 512.6155\n",
      "Epoch 298/500\n",
      "  Dynamics Model 1 Loss: 1164.8473\n",
      "  Dynamics Model 2 Loss: 1715.7294\n",
      "  Dynamics Model 3 Loss: 1225.6101\n",
      "  Behavior Model 1 Loss: 484.7538\n",
      "  Behavior Model 2 Loss: 476.5013\n",
      "  Behavior Model 3 Loss: 511.8165\n",
      "Epoch 299/500\n",
      "  Dynamics Model 1 Loss: 1162.8509\n",
      "  Dynamics Model 2 Loss: 1717.2330\n",
      "  Dynamics Model 3 Loss: 1227.3169\n",
      "  Behavior Model 1 Loss: 488.8199\n",
      "  Behavior Model 2 Loss: 478.8147\n",
      "  Behavior Model 3 Loss: 512.6905\n",
      "Epoch 300/500\n",
      "  Dynamics Model 1 Loss: 1162.0744\n",
      "  Dynamics Model 2 Loss: 1714.2158\n",
      "  Dynamics Model 3 Loss: 1227.0968\n",
      "  Behavior Model 1 Loss: 488.0499\n",
      "  Behavior Model 2 Loss: 479.6474\n",
      "  Behavior Model 3 Loss: 514.3876\n",
      "Epoch 301/500\n",
      "  Dynamics Model 1 Loss: 1161.4602\n",
      "  Dynamics Model 2 Loss: 1714.6951\n",
      "  Dynamics Model 3 Loss: 1227.7045\n",
      "  Behavior Model 1 Loss: 487.9642\n",
      "  Behavior Model 2 Loss: 478.2486\n",
      "  Behavior Model 3 Loss: 515.4654\n",
      "Epoch 302/500\n",
      "  Dynamics Model 1 Loss: 1161.3780\n",
      "  Dynamics Model 2 Loss: 1715.4098\n",
      "  Dynamics Model 3 Loss: 1225.8475\n",
      "  Behavior Model 1 Loss: 485.7058\n",
      "  Behavior Model 2 Loss: 477.0779\n",
      "  Behavior Model 3 Loss: 512.3471\n",
      "Epoch 303/500\n",
      "  Dynamics Model 1 Loss: 1162.0683\n",
      "  Dynamics Model 2 Loss: 1715.3339\n",
      "  Dynamics Model 3 Loss: 1225.4967\n",
      "  Behavior Model 1 Loss: 487.4345\n",
      "  Behavior Model 2 Loss: 479.0030\n",
      "  Behavior Model 3 Loss: 515.5909\n",
      "Epoch 304/500\n",
      "  Dynamics Model 1 Loss: 1160.4416\n",
      "  Dynamics Model 2 Loss: 1714.4335\n",
      "  Dynamics Model 3 Loss: 1226.9119\n",
      "  Behavior Model 1 Loss: 488.7693\n",
      "  Behavior Model 2 Loss: 478.7482\n",
      "  Behavior Model 3 Loss: 511.9467\n",
      "Epoch 305/500\n",
      "  Dynamics Model 1 Loss: 1164.6480\n",
      "  Dynamics Model 2 Loss: 1713.7622\n",
      "  Dynamics Model 3 Loss: 1225.5159\n",
      "  Behavior Model 1 Loss: 485.1190\n",
      "  Behavior Model 2 Loss: 478.4676\n",
      "  Behavior Model 3 Loss: 513.7147\n",
      "Epoch 306/500\n",
      "  Dynamics Model 1 Loss: 1161.4229\n",
      "  Dynamics Model 2 Loss: 1713.6761\n",
      "  Dynamics Model 3 Loss: 1226.7551\n",
      "  Behavior Model 1 Loss: 488.3799\n",
      "  Behavior Model 2 Loss: 479.7159\n",
      "  Behavior Model 3 Loss: 512.0450\n",
      "Epoch 307/500\n",
      "  Dynamics Model 1 Loss: 1162.4755\n",
      "  Dynamics Model 2 Loss: 1715.7471\n",
      "  Dynamics Model 3 Loss: 1225.9843\n",
      "  Behavior Model 1 Loss: 487.0285\n",
      "  Behavior Model 2 Loss: 479.5153\n",
      "  Behavior Model 3 Loss: 510.8364\n",
      "Epoch 308/500\n",
      "  Dynamics Model 1 Loss: 1162.2431\n",
      "  Dynamics Model 2 Loss: 1715.1324\n",
      "  Dynamics Model 3 Loss: 1227.8683\n",
      "  Behavior Model 1 Loss: 489.2710\n",
      "  Behavior Model 2 Loss: 480.0908\n",
      "  Behavior Model 3 Loss: 512.4754\n",
      "Epoch 309/500\n",
      "  Dynamics Model 1 Loss: 1163.8365\n",
      "  Dynamics Model 2 Loss: 1713.4000\n",
      "  Dynamics Model 3 Loss: 1227.4037\n",
      "  Behavior Model 1 Loss: 485.3834\n",
      "  Behavior Model 2 Loss: 479.7263\n",
      "  Behavior Model 3 Loss: 511.3184\n",
      "Epoch 310/500\n",
      "  Dynamics Model 1 Loss: 1161.9410\n",
      "  Dynamics Model 2 Loss: 1715.0237\n",
      "  Dynamics Model 3 Loss: 1225.3293\n",
      "  Behavior Model 1 Loss: 488.7031\n",
      "  Behavior Model 2 Loss: 479.9139\n",
      "  Behavior Model 3 Loss: 512.1442\n",
      "Epoch 311/500\n",
      "  Dynamics Model 1 Loss: 1162.3080\n",
      "  Dynamics Model 2 Loss: 1715.9293\n",
      "  Dynamics Model 3 Loss: 1227.0537\n",
      "  Behavior Model 1 Loss: 487.1236\n",
      "  Behavior Model 2 Loss: 478.9601\n",
      "  Behavior Model 3 Loss: 511.4339\n",
      "Epoch 312/500\n",
      "  Dynamics Model 1 Loss: 1159.7533\n",
      "  Dynamics Model 2 Loss: 1714.4614\n",
      "  Dynamics Model 3 Loss: 1225.5366\n",
      "  Behavior Model 1 Loss: 489.1515\n",
      "  Behavior Model 2 Loss: 477.5031\n",
      "  Behavior Model 3 Loss: 511.4867\n",
      "Epoch 313/500\n",
      "  Dynamics Model 1 Loss: 1164.9488\n",
      "  Dynamics Model 2 Loss: 1716.2088\n",
      "  Dynamics Model 3 Loss: 1227.1482\n",
      "  Behavior Model 1 Loss: 489.3122\n",
      "  Behavior Model 2 Loss: 478.5580\n",
      "  Behavior Model 3 Loss: 512.9085\n",
      "Epoch 314/500\n",
      "  Dynamics Model 1 Loss: 1163.9601\n",
      "  Dynamics Model 2 Loss: 1713.6160\n",
      "  Dynamics Model 3 Loss: 1226.8525\n",
      "  Behavior Model 1 Loss: 489.8948\n",
      "  Behavior Model 2 Loss: 479.3753\n",
      "  Behavior Model 3 Loss: 512.9419\n",
      "Epoch 315/500\n",
      "  Dynamics Model 1 Loss: 1162.1409\n",
      "  Dynamics Model 2 Loss: 1715.6300\n",
      "  Dynamics Model 3 Loss: 1225.8791\n",
      "  Behavior Model 1 Loss: 490.6741\n",
      "  Behavior Model 2 Loss: 478.5564\n",
      "  Behavior Model 3 Loss: 511.9724\n",
      "Epoch 316/500\n",
      "  Dynamics Model 1 Loss: 1163.3999\n",
      "  Dynamics Model 2 Loss: 1712.4104\n",
      "  Dynamics Model 3 Loss: 1227.8235\n",
      "  Behavior Model 1 Loss: 488.1331\n",
      "  Behavior Model 2 Loss: 477.5225\n",
      "  Behavior Model 3 Loss: 512.6106\n",
      "Epoch 317/500\n",
      "  Dynamics Model 1 Loss: 1162.5377\n",
      "  Dynamics Model 2 Loss: 1716.4510\n",
      "  Dynamics Model 3 Loss: 1227.4683\n",
      "  Behavior Model 1 Loss: 487.2264\n",
      "  Behavior Model 2 Loss: 480.2464\n",
      "  Behavior Model 3 Loss: 512.3864\n",
      "Epoch 318/500\n",
      "  Dynamics Model 1 Loss: 1162.9207\n",
      "  Dynamics Model 2 Loss: 1716.6906\n",
      "  Dynamics Model 3 Loss: 1225.7404\n",
      "  Behavior Model 1 Loss: 487.6589\n",
      "  Behavior Model 2 Loss: 479.1312\n",
      "  Behavior Model 3 Loss: 512.8626\n",
      "Epoch 319/500\n",
      "  Dynamics Model 1 Loss: 1162.3234\n",
      "  Dynamics Model 2 Loss: 1711.5781\n",
      "  Dynamics Model 3 Loss: 1225.8519\n",
      "  Behavior Model 1 Loss: 488.6655\n",
      "  Behavior Model 2 Loss: 478.5280\n",
      "  Behavior Model 3 Loss: 513.2078\n",
      "Epoch 320/500\n",
      "  Dynamics Model 1 Loss: 1162.9622\n",
      "  Dynamics Model 2 Loss: 1715.4284\n",
      "  Dynamics Model 3 Loss: 1225.5774\n",
      "  Behavior Model 1 Loss: 486.5955\n",
      "  Behavior Model 2 Loss: 479.7316\n",
      "  Behavior Model 3 Loss: 511.8996\n",
      "Epoch 321/500\n",
      "  Dynamics Model 1 Loss: 1163.9709\n",
      "  Dynamics Model 2 Loss: 1715.1166\n",
      "  Dynamics Model 3 Loss: 1226.3646\n",
      "  Behavior Model 1 Loss: 488.9211\n",
      "  Behavior Model 2 Loss: 479.9621\n",
      "  Behavior Model 3 Loss: 510.9461\n",
      "Epoch 322/500\n",
      "  Dynamics Model 1 Loss: 1162.9545\n",
      "  Dynamics Model 2 Loss: 1715.2306\n",
      "  Dynamics Model 3 Loss: 1226.3538\n",
      "  Behavior Model 1 Loss: 488.9073\n",
      "  Behavior Model 2 Loss: 478.2630\n",
      "  Behavior Model 3 Loss: 511.4274\n",
      "Epoch 323/500\n",
      "  Dynamics Model 1 Loss: 1162.5418\n",
      "  Dynamics Model 2 Loss: 1715.3769\n",
      "  Dynamics Model 3 Loss: 1226.3075\n",
      "  Behavior Model 1 Loss: 488.3819\n",
      "  Behavior Model 2 Loss: 480.3190\n",
      "  Behavior Model 3 Loss: 513.8547\n",
      "Epoch 324/500\n",
      "  Dynamics Model 1 Loss: 1161.4990\n",
      "  Dynamics Model 2 Loss: 1714.9829\n",
      "  Dynamics Model 3 Loss: 1225.1061\n",
      "  Behavior Model 1 Loss: 489.2838\n",
      "  Behavior Model 2 Loss: 478.9539\n",
      "  Behavior Model 3 Loss: 513.7244\n",
      "Epoch 325/500\n",
      "  Dynamics Model 1 Loss: 1160.1260\n",
      "  Dynamics Model 2 Loss: 1715.0647\n",
      "  Dynamics Model 3 Loss: 1225.5046\n",
      "  Behavior Model 1 Loss: 488.2319\n",
      "  Behavior Model 2 Loss: 481.1116\n",
      "  Behavior Model 3 Loss: 514.6316\n",
      "Epoch 326/500\n",
      "  Dynamics Model 1 Loss: 1162.7049\n",
      "  Dynamics Model 2 Loss: 1716.0638\n",
      "  Dynamics Model 3 Loss: 1225.7894\n",
      "  Behavior Model 1 Loss: 487.0918\n",
      "  Behavior Model 2 Loss: 478.5214\n",
      "  Behavior Model 3 Loss: 511.6934\n",
      "Epoch 327/500\n",
      "  Dynamics Model 1 Loss: 1161.6215\n",
      "  Dynamics Model 2 Loss: 1715.5626\n",
      "  Dynamics Model 3 Loss: 1228.0666\n",
      "  Behavior Model 1 Loss: 488.4377\n",
      "  Behavior Model 2 Loss: 478.5311\n",
      "  Behavior Model 3 Loss: 512.7816\n",
      "Epoch 328/500\n",
      "  Dynamics Model 1 Loss: 1163.7441\n",
      "  Dynamics Model 2 Loss: 1718.2174\n",
      "  Dynamics Model 3 Loss: 1225.5835\n",
      "  Behavior Model 1 Loss: 486.3384\n",
      "  Behavior Model 2 Loss: 478.0027\n",
      "  Behavior Model 3 Loss: 512.0721\n",
      "Epoch 329/500\n",
      "  Dynamics Model 1 Loss: 1162.9167\n",
      "  Dynamics Model 2 Loss: 1716.5285\n",
      "  Dynamics Model 3 Loss: 1226.0452\n",
      "  Behavior Model 1 Loss: 490.8480\n",
      "  Behavior Model 2 Loss: 479.8878\n",
      "  Behavior Model 3 Loss: 513.8836\n",
      "Epoch 330/500\n",
      "  Dynamics Model 1 Loss: 1162.0085\n",
      "  Dynamics Model 2 Loss: 1715.7102\n",
      "  Dynamics Model 3 Loss: 1226.3034\n",
      "  Behavior Model 1 Loss: 487.5940\n",
      "  Behavior Model 2 Loss: 478.7284\n",
      "  Behavior Model 3 Loss: 512.5480\n",
      "Epoch 331/500\n",
      "  Dynamics Model 1 Loss: 1161.7997\n",
      "  Dynamics Model 2 Loss: 1715.1392\n",
      "  Dynamics Model 3 Loss: 1225.4014\n",
      "  Behavior Model 1 Loss: 486.4510\n",
      "  Behavior Model 2 Loss: 478.2181\n",
      "  Behavior Model 3 Loss: 513.1824\n",
      "Epoch 332/500\n",
      "  Dynamics Model 1 Loss: 1163.8400\n",
      "  Dynamics Model 2 Loss: 1715.4094\n",
      "  Dynamics Model 3 Loss: 1227.4048\n",
      "  Behavior Model 1 Loss: 487.4252\n",
      "  Behavior Model 2 Loss: 477.7478\n",
      "  Behavior Model 3 Loss: 512.9624\n",
      "Epoch 333/500\n",
      "  Dynamics Model 1 Loss: 1163.9252\n",
      "  Dynamics Model 2 Loss: 1716.3821\n",
      "  Dynamics Model 3 Loss: 1225.9224\n",
      "  Behavior Model 1 Loss: 486.8289\n",
      "  Behavior Model 2 Loss: 479.4644\n",
      "  Behavior Model 3 Loss: 513.4580\n",
      "Epoch 334/500\n",
      "  Dynamics Model 1 Loss: 1161.8872\n",
      "  Dynamics Model 2 Loss: 1715.1031\n",
      "  Dynamics Model 3 Loss: 1226.1063\n",
      "  Behavior Model 1 Loss: 489.2805\n",
      "  Behavior Model 2 Loss: 480.7270\n",
      "  Behavior Model 3 Loss: 512.8921\n",
      "Epoch 335/500\n",
      "  Dynamics Model 1 Loss: 1163.3578\n",
      "  Dynamics Model 2 Loss: 1714.9548\n",
      "  Dynamics Model 3 Loss: 1224.2439\n",
      "  Behavior Model 1 Loss: 486.7801\n",
      "  Behavior Model 2 Loss: 476.7616\n",
      "  Behavior Model 3 Loss: 512.5521\n",
      "Epoch 336/500\n",
      "  Dynamics Model 1 Loss: 1163.4727\n",
      "  Dynamics Model 2 Loss: 1715.2461\n",
      "  Dynamics Model 3 Loss: 1225.9185\n",
      "  Behavior Model 1 Loss: 486.4943\n",
      "  Behavior Model 2 Loss: 481.4337\n",
      "  Behavior Model 3 Loss: 512.6692\n",
      "Epoch 337/500\n",
      "  Dynamics Model 1 Loss: 1161.8144\n",
      "  Dynamics Model 2 Loss: 1716.1818\n",
      "  Dynamics Model 3 Loss: 1225.6009\n",
      "  Behavior Model 1 Loss: 488.0020\n",
      "  Behavior Model 2 Loss: 477.8176\n",
      "  Behavior Model 3 Loss: 513.3871\n",
      "Epoch 338/500\n",
      "  Dynamics Model 1 Loss: 1161.9904\n",
      "  Dynamics Model 2 Loss: 1713.7841\n",
      "  Dynamics Model 3 Loss: 1226.7893\n",
      "  Behavior Model 1 Loss: 489.5896\n",
      "  Behavior Model 2 Loss: 478.8853\n",
      "  Behavior Model 3 Loss: 514.6680\n",
      "Epoch 339/500\n",
      "  Dynamics Model 1 Loss: 1162.0018\n",
      "  Dynamics Model 2 Loss: 1717.0112\n",
      "  Dynamics Model 3 Loss: 1226.8551\n",
      "  Behavior Model 1 Loss: 487.9620\n",
      "  Behavior Model 2 Loss: 479.8735\n",
      "  Behavior Model 3 Loss: 512.8975\n",
      "Epoch 340/500\n",
      "  Dynamics Model 1 Loss: 1163.5798\n",
      "  Dynamics Model 2 Loss: 1715.8365\n",
      "  Dynamics Model 3 Loss: 1224.7933\n",
      "  Behavior Model 1 Loss: 490.1717\n",
      "  Behavior Model 2 Loss: 477.1041\n",
      "  Behavior Model 3 Loss: 511.4171\n",
      "Epoch 341/500\n",
      "  Dynamics Model 1 Loss: 1162.3228\n",
      "  Dynamics Model 2 Loss: 1714.0780\n",
      "  Dynamics Model 3 Loss: 1227.0654\n",
      "  Behavior Model 1 Loss: 486.5603\n",
      "  Behavior Model 2 Loss: 479.8722\n",
      "  Behavior Model 3 Loss: 512.1869\n",
      "Epoch 342/500\n",
      "  Dynamics Model 1 Loss: 1162.1540\n",
      "  Dynamics Model 2 Loss: 1715.3503\n",
      "  Dynamics Model 3 Loss: 1226.3597\n",
      "  Behavior Model 1 Loss: 488.3082\n",
      "  Behavior Model 2 Loss: 479.0615\n",
      "  Behavior Model 3 Loss: 511.5993\n",
      "Epoch 343/500\n",
      "  Dynamics Model 1 Loss: 1163.6907\n",
      "  Dynamics Model 2 Loss: 1714.7557\n",
      "  Dynamics Model 3 Loss: 1226.2514\n",
      "  Behavior Model 1 Loss: 488.0458\n",
      "  Behavior Model 2 Loss: 477.9530\n",
      "  Behavior Model 3 Loss: 514.0073\n",
      "Epoch 344/500\n",
      "  Dynamics Model 1 Loss: 1162.2467\n",
      "  Dynamics Model 2 Loss: 1714.4353\n",
      "  Dynamics Model 3 Loss: 1224.9338\n",
      "  Behavior Model 1 Loss: 488.1077\n",
      "  Behavior Model 2 Loss: 478.2948\n",
      "  Behavior Model 3 Loss: 512.9638\n",
      "Epoch 345/500\n",
      "  Dynamics Model 1 Loss: 1163.4562\n",
      "  Dynamics Model 2 Loss: 1716.8297\n",
      "  Dynamics Model 3 Loss: 1226.5861\n",
      "  Behavior Model 1 Loss: 486.9684\n",
      "  Behavior Model 2 Loss: 480.9807\n",
      "  Behavior Model 3 Loss: 512.7570\n",
      "Epoch 346/500\n",
      "  Dynamics Model 1 Loss: 1161.1253\n",
      "  Dynamics Model 2 Loss: 1713.4956\n",
      "  Dynamics Model 3 Loss: 1225.7193\n",
      "  Behavior Model 1 Loss: 486.6386\n",
      "  Behavior Model 2 Loss: 477.4208\n",
      "  Behavior Model 3 Loss: 510.8170\n",
      "Epoch 347/500\n",
      "  Dynamics Model 1 Loss: 1161.6431\n",
      "  Dynamics Model 2 Loss: 1712.9892\n",
      "  Dynamics Model 3 Loss: 1225.6182\n",
      "  Behavior Model 1 Loss: 486.7574\n",
      "  Behavior Model 2 Loss: 479.5647\n",
      "  Behavior Model 3 Loss: 510.9801\n",
      "Epoch 348/500\n",
      "  Dynamics Model 1 Loss: 1161.5698\n",
      "  Dynamics Model 2 Loss: 1713.0324\n",
      "  Dynamics Model 3 Loss: 1226.4619\n",
      "  Behavior Model 1 Loss: 489.7123\n",
      "  Behavior Model 2 Loss: 478.9565\n",
      "  Behavior Model 3 Loss: 510.8211\n",
      "Epoch 349/500\n",
      "  Dynamics Model 1 Loss: 1162.3666\n",
      "  Dynamics Model 2 Loss: 1714.0110\n",
      "  Dynamics Model 3 Loss: 1226.9947\n",
      "  Behavior Model 1 Loss: 488.4257\n",
      "  Behavior Model 2 Loss: 478.5971\n",
      "  Behavior Model 3 Loss: 510.4176\n",
      "Epoch 350/500\n",
      "  Dynamics Model 1 Loss: 1160.0784\n",
      "  Dynamics Model 2 Loss: 1713.2203\n",
      "  Dynamics Model 3 Loss: 1225.6906\n",
      "  Behavior Model 1 Loss: 487.1178\n",
      "  Behavior Model 2 Loss: 475.9617\n",
      "  Behavior Model 3 Loss: 512.4054\n",
      "Epoch 351/500\n",
      "  Dynamics Model 1 Loss: 1162.9812\n",
      "  Dynamics Model 2 Loss: 1714.8477\n",
      "  Dynamics Model 3 Loss: 1224.5082\n",
      "  Behavior Model 1 Loss: 485.7336\n",
      "  Behavior Model 2 Loss: 481.0745\n",
      "  Behavior Model 3 Loss: 512.1618\n",
      "Epoch 352/500\n",
      "  Dynamics Model 1 Loss: 1163.2995\n",
      "  Dynamics Model 2 Loss: 1714.7714\n",
      "  Dynamics Model 3 Loss: 1225.5733\n",
      "  Behavior Model 1 Loss: 486.8667\n",
      "  Behavior Model 2 Loss: 480.0293\n",
      "  Behavior Model 3 Loss: 512.5850\n",
      "Epoch 353/500\n",
      "  Dynamics Model 1 Loss: 1164.7606\n",
      "  Dynamics Model 2 Loss: 1717.1641\n",
      "  Dynamics Model 3 Loss: 1226.8507\n",
      "  Behavior Model 1 Loss: 489.5034\n",
      "  Behavior Model 2 Loss: 478.9310\n",
      "  Behavior Model 3 Loss: 513.3847\n",
      "Epoch 354/500\n",
      "  Dynamics Model 1 Loss: 1162.3249\n",
      "  Dynamics Model 2 Loss: 1714.9553\n",
      "  Dynamics Model 3 Loss: 1227.2341\n",
      "  Behavior Model 1 Loss: 487.3097\n",
      "  Behavior Model 2 Loss: 479.1263\n",
      "  Behavior Model 3 Loss: 511.9515\n",
      "Epoch 355/500\n",
      "  Dynamics Model 1 Loss: 1162.3678\n",
      "  Dynamics Model 2 Loss: 1713.4051\n",
      "  Dynamics Model 3 Loss: 1224.7232\n",
      "  Behavior Model 1 Loss: 488.1302\n",
      "  Behavior Model 2 Loss: 479.7451\n",
      "  Behavior Model 3 Loss: 510.0736\n",
      "Epoch 356/500\n",
      "  Dynamics Model 1 Loss: 1161.4806\n",
      "  Dynamics Model 2 Loss: 1712.6669\n",
      "  Dynamics Model 3 Loss: 1224.6149\n",
      "  Behavior Model 1 Loss: 487.4815\n",
      "  Behavior Model 2 Loss: 478.3546\n",
      "  Behavior Model 3 Loss: 512.5767\n",
      "Epoch 357/500\n",
      "  Dynamics Model 1 Loss: 1162.2903\n",
      "  Dynamics Model 2 Loss: 1715.9005\n",
      "  Dynamics Model 3 Loss: 1226.2860\n",
      "  Behavior Model 1 Loss: 487.4785\n",
      "  Behavior Model 2 Loss: 478.8637\n",
      "  Behavior Model 3 Loss: 512.7559\n",
      "Epoch 358/500\n",
      "  Dynamics Model 1 Loss: 1162.6688\n",
      "  Dynamics Model 2 Loss: 1715.8665\n",
      "  Dynamics Model 3 Loss: 1227.6909\n",
      "  Behavior Model 1 Loss: 488.5059\n",
      "  Behavior Model 2 Loss: 478.0969\n",
      "  Behavior Model 3 Loss: 511.3203\n",
      "Epoch 359/500\n",
      "  Dynamics Model 1 Loss: 1162.0873\n",
      "  Dynamics Model 2 Loss: 1715.4383\n",
      "  Dynamics Model 3 Loss: 1226.9320\n",
      "  Behavior Model 1 Loss: 488.7144\n",
      "  Behavior Model 2 Loss: 478.7639\n",
      "  Behavior Model 3 Loss: 511.9881\n",
      "Epoch 360/500\n",
      "  Dynamics Model 1 Loss: 1162.3079\n",
      "  Dynamics Model 2 Loss: 1713.0301\n",
      "  Dynamics Model 3 Loss: 1225.0361\n",
      "  Behavior Model 1 Loss: 488.1667\n",
      "  Behavior Model 2 Loss: 477.7426\n",
      "  Behavior Model 3 Loss: 511.6857\n",
      "Epoch 361/500\n",
      "  Dynamics Model 1 Loss: 1163.5250\n",
      "  Dynamics Model 2 Loss: 1714.7935\n",
      "  Dynamics Model 3 Loss: 1226.0738\n",
      "  Behavior Model 1 Loss: 484.9471\n",
      "  Behavior Model 2 Loss: 478.8206\n",
      "  Behavior Model 3 Loss: 510.3974\n",
      "Epoch 362/500\n",
      "  Dynamics Model 1 Loss: 1163.9907\n",
      "  Dynamics Model 2 Loss: 1714.2279\n",
      "  Dynamics Model 3 Loss: 1228.7350\n",
      "  Behavior Model 1 Loss: 487.2038\n",
      "  Behavior Model 2 Loss: 478.2515\n",
      "  Behavior Model 3 Loss: 509.7269\n",
      "Epoch 363/500\n",
      "  Dynamics Model 1 Loss: 1161.7977\n",
      "  Dynamics Model 2 Loss: 1716.9176\n",
      "  Dynamics Model 3 Loss: 1225.7015\n",
      "  Behavior Model 1 Loss: 488.2582\n",
      "  Behavior Model 2 Loss: 477.5570\n",
      "  Behavior Model 3 Loss: 511.5239\n",
      "Epoch 364/500\n",
      "  Dynamics Model 1 Loss: 1161.6552\n",
      "  Dynamics Model 2 Loss: 1715.6007\n",
      "  Dynamics Model 3 Loss: 1226.9613\n",
      "  Behavior Model 1 Loss: 488.3987\n",
      "  Behavior Model 2 Loss: 476.4648\n",
      "  Behavior Model 3 Loss: 512.1560\n",
      "Epoch 365/500\n",
      "  Dynamics Model 1 Loss: 1163.4266\n",
      "  Dynamics Model 2 Loss: 1716.0754\n",
      "  Dynamics Model 3 Loss: 1226.0086\n",
      "  Behavior Model 1 Loss: 487.4638\n",
      "  Behavior Model 2 Loss: 478.7136\n",
      "  Behavior Model 3 Loss: 511.0877\n",
      "Epoch 366/500\n",
      "  Dynamics Model 1 Loss: 1163.5771\n",
      "  Dynamics Model 2 Loss: 1716.1814\n",
      "  Dynamics Model 3 Loss: 1226.7145\n",
      "  Behavior Model 1 Loss: 488.9629\n",
      "  Behavior Model 2 Loss: 479.6304\n",
      "  Behavior Model 3 Loss: 512.6465\n",
      "Epoch 367/500\n",
      "  Dynamics Model 1 Loss: 1163.3583\n",
      "  Dynamics Model 2 Loss: 1714.5230\n",
      "  Dynamics Model 3 Loss: 1226.1222\n",
      "  Behavior Model 1 Loss: 489.1053\n",
      "  Behavior Model 2 Loss: 478.3474\n",
      "  Behavior Model 3 Loss: 509.0496\n",
      "Epoch 368/500\n",
      "  Dynamics Model 1 Loss: 1164.3088\n",
      "  Dynamics Model 2 Loss: 1714.2373\n",
      "  Dynamics Model 3 Loss: 1225.2971\n",
      "  Behavior Model 1 Loss: 490.0110\n",
      "  Behavior Model 2 Loss: 476.9542\n",
      "  Behavior Model 3 Loss: 512.0272\n",
      "Epoch 369/500\n",
      "  Dynamics Model 1 Loss: 1161.5047\n",
      "  Dynamics Model 2 Loss: 1716.2586\n",
      "  Dynamics Model 3 Loss: 1225.3142\n",
      "  Behavior Model 1 Loss: 487.6732\n",
      "  Behavior Model 2 Loss: 478.1726\n",
      "  Behavior Model 3 Loss: 513.3945\n",
      "Epoch 370/500\n",
      "  Dynamics Model 1 Loss: 1161.3103\n",
      "  Dynamics Model 2 Loss: 1714.8004\n",
      "  Dynamics Model 3 Loss: 1225.0831\n",
      "  Behavior Model 1 Loss: 488.0017\n",
      "  Behavior Model 2 Loss: 477.8110\n",
      "  Behavior Model 3 Loss: 513.0177\n",
      "Epoch 371/500\n",
      "  Dynamics Model 1 Loss: 1160.8371\n",
      "  Dynamics Model 2 Loss: 1715.1261\n",
      "  Dynamics Model 3 Loss: 1223.5935\n",
      "  Behavior Model 1 Loss: 487.2105\n",
      "  Behavior Model 2 Loss: 478.1674\n",
      "  Behavior Model 3 Loss: 514.2808\n",
      "Epoch 372/500\n",
      "  Dynamics Model 1 Loss: 1164.6381\n",
      "  Dynamics Model 2 Loss: 1715.6653\n",
      "  Dynamics Model 3 Loss: 1225.5130\n",
      "  Behavior Model 1 Loss: 488.6101\n",
      "  Behavior Model 2 Loss: 478.9893\n",
      "  Behavior Model 3 Loss: 512.3830\n",
      "Epoch 373/500\n",
      "  Dynamics Model 1 Loss: 1164.2096\n",
      "  Dynamics Model 2 Loss: 1713.7186\n",
      "  Dynamics Model 3 Loss: 1226.0217\n",
      "  Behavior Model 1 Loss: 488.8027\n",
      "  Behavior Model 2 Loss: 479.6442\n",
      "  Behavior Model 3 Loss: 511.0284\n",
      "Epoch 374/500\n",
      "  Dynamics Model 1 Loss: 1164.2551\n",
      "  Dynamics Model 2 Loss: 1715.7119\n",
      "  Dynamics Model 3 Loss: 1225.6030\n",
      "  Behavior Model 1 Loss: 487.6146\n",
      "  Behavior Model 2 Loss: 479.0784\n",
      "  Behavior Model 3 Loss: 513.2382\n",
      "Epoch 375/500\n",
      "  Dynamics Model 1 Loss: 1161.5606\n",
      "  Dynamics Model 2 Loss: 1716.4641\n",
      "  Dynamics Model 3 Loss: 1226.3078\n",
      "  Behavior Model 1 Loss: 489.5993\n",
      "  Behavior Model 2 Loss: 476.9287\n",
      "  Behavior Model 3 Loss: 513.9351\n",
      "Epoch 376/500\n",
      "  Dynamics Model 1 Loss: 1162.9684\n",
      "  Dynamics Model 2 Loss: 1714.2413\n",
      "  Dynamics Model 3 Loss: 1227.7506\n",
      "  Behavior Model 1 Loss: 487.3727\n",
      "  Behavior Model 2 Loss: 478.8648\n",
      "  Behavior Model 3 Loss: 512.4996\n",
      "Epoch 377/500\n",
      "  Dynamics Model 1 Loss: 1160.3925\n",
      "  Dynamics Model 2 Loss: 1714.9321\n",
      "  Dynamics Model 3 Loss: 1225.6293\n",
      "  Behavior Model 1 Loss: 487.0098\n",
      "  Behavior Model 2 Loss: 479.2665\n",
      "  Behavior Model 3 Loss: 513.7167\n",
      "Epoch 378/500\n",
      "  Dynamics Model 1 Loss: 1161.3320\n",
      "  Dynamics Model 2 Loss: 1715.7186\n",
      "  Dynamics Model 3 Loss: 1225.0970\n",
      "  Behavior Model 1 Loss: 488.6685\n",
      "  Behavior Model 2 Loss: 480.4572\n",
      "  Behavior Model 3 Loss: 511.8814\n",
      "Epoch 379/500\n",
      "  Dynamics Model 1 Loss: 1163.5299\n",
      "  Dynamics Model 2 Loss: 1715.0474\n",
      "  Dynamics Model 3 Loss: 1227.5669\n",
      "  Behavior Model 1 Loss: 486.0431\n",
      "  Behavior Model 2 Loss: 477.5626\n",
      "  Behavior Model 3 Loss: 511.2892\n",
      "Epoch 380/500\n",
      "  Dynamics Model 1 Loss: 1162.4852\n",
      "  Dynamics Model 2 Loss: 1715.8512\n",
      "  Dynamics Model 3 Loss: 1227.0165\n",
      "  Behavior Model 1 Loss: 488.3175\n",
      "  Behavior Model 2 Loss: 480.9527\n",
      "  Behavior Model 3 Loss: 512.8825\n",
      "Epoch 381/500\n",
      "  Dynamics Model 1 Loss: 1163.5214\n",
      "  Dynamics Model 2 Loss: 1713.7228\n",
      "  Dynamics Model 3 Loss: 1227.0878\n",
      "  Behavior Model 1 Loss: 489.7092\n",
      "  Behavior Model 2 Loss: 480.5575\n",
      "  Behavior Model 3 Loss: 512.1486\n",
      "Epoch 382/500\n",
      "  Dynamics Model 1 Loss: 1163.0624\n",
      "  Dynamics Model 2 Loss: 1715.5169\n",
      "  Dynamics Model 3 Loss: 1225.4889\n",
      "  Behavior Model 1 Loss: 488.1255\n",
      "  Behavior Model 2 Loss: 478.1690\n",
      "  Behavior Model 3 Loss: 513.1690\n",
      "Epoch 383/500\n",
      "  Dynamics Model 1 Loss: 1162.6905\n",
      "  Dynamics Model 2 Loss: 1712.9992\n",
      "  Dynamics Model 3 Loss: 1229.0431\n",
      "  Behavior Model 1 Loss: 489.3367\n",
      "  Behavior Model 2 Loss: 479.7672\n",
      "  Behavior Model 3 Loss: 513.0052\n",
      "Epoch 384/500\n",
      "  Dynamics Model 1 Loss: 1161.8486\n",
      "  Dynamics Model 2 Loss: 1713.8341\n",
      "  Dynamics Model 3 Loss: 1225.3642\n",
      "  Behavior Model 1 Loss: 487.8800\n",
      "  Behavior Model 2 Loss: 479.8462\n",
      "  Behavior Model 3 Loss: 512.4307\n",
      "Epoch 385/500\n",
      "  Dynamics Model 1 Loss: 1163.4492\n",
      "  Dynamics Model 2 Loss: 1715.3293\n",
      "  Dynamics Model 3 Loss: 1225.8244\n",
      "  Behavior Model 1 Loss: 488.3564\n",
      "  Behavior Model 2 Loss: 478.6921\n",
      "  Behavior Model 3 Loss: 510.5431\n",
      "Epoch 386/500\n",
      "  Dynamics Model 1 Loss: 1163.8440\n",
      "  Dynamics Model 2 Loss: 1713.8548\n",
      "  Dynamics Model 3 Loss: 1228.1191\n",
      "  Behavior Model 1 Loss: 488.8275\n",
      "  Behavior Model 2 Loss: 478.4283\n",
      "  Behavior Model 3 Loss: 512.1369\n",
      "Epoch 387/500\n",
      "  Dynamics Model 1 Loss: 1160.1275\n",
      "  Dynamics Model 2 Loss: 1713.5319\n",
      "  Dynamics Model 3 Loss: 1224.3371\n",
      "  Behavior Model 1 Loss: 488.1503\n",
      "  Behavior Model 2 Loss: 477.1338\n",
      "  Behavior Model 3 Loss: 512.6418\n",
      "Epoch 388/500\n",
      "  Dynamics Model 1 Loss: 1163.0875\n",
      "  Dynamics Model 2 Loss: 1714.6824\n",
      "  Dynamics Model 3 Loss: 1226.0414\n",
      "  Behavior Model 1 Loss: 488.8243\n",
      "  Behavior Model 2 Loss: 477.7805\n",
      "  Behavior Model 3 Loss: 511.4422\n",
      "Epoch 389/500\n",
      "  Dynamics Model 1 Loss: 1162.9898\n",
      "  Dynamics Model 2 Loss: 1716.5664\n",
      "  Dynamics Model 3 Loss: 1225.4531\n",
      "  Behavior Model 1 Loss: 488.8073\n",
      "  Behavior Model 2 Loss: 479.8758\n",
      "  Behavior Model 3 Loss: 511.3310\n",
      "Epoch 390/500\n",
      "  Dynamics Model 1 Loss: 1163.1752\n",
      "  Dynamics Model 2 Loss: 1715.6254\n",
      "  Dynamics Model 3 Loss: 1225.4856\n",
      "  Behavior Model 1 Loss: 491.9113\n",
      "  Behavior Model 2 Loss: 478.4224\n",
      "  Behavior Model 3 Loss: 510.2389\n",
      "Epoch 391/500\n",
      "  Dynamics Model 1 Loss: 1163.1025\n",
      "  Dynamics Model 2 Loss: 1716.3802\n",
      "  Dynamics Model 3 Loss: 1225.7961\n",
      "  Behavior Model 1 Loss: 486.5608\n",
      "  Behavior Model 2 Loss: 477.6474\n",
      "  Behavior Model 3 Loss: 515.2148\n",
      "Epoch 392/500\n",
      "  Dynamics Model 1 Loss: 1162.6481\n",
      "  Dynamics Model 2 Loss: 1713.9665\n",
      "  Dynamics Model 3 Loss: 1225.4109\n",
      "  Behavior Model 1 Loss: 488.8800\n",
      "  Behavior Model 2 Loss: 477.6186\n",
      "  Behavior Model 3 Loss: 512.3945\n",
      "Epoch 393/500\n",
      "  Dynamics Model 1 Loss: 1161.4684\n",
      "  Dynamics Model 2 Loss: 1714.3931\n",
      "  Dynamics Model 3 Loss: 1227.3661\n",
      "  Behavior Model 1 Loss: 486.7026\n",
      "  Behavior Model 2 Loss: 479.5749\n",
      "  Behavior Model 3 Loss: 513.4872\n",
      "Epoch 394/500\n",
      "  Dynamics Model 1 Loss: 1159.9865\n",
      "  Dynamics Model 2 Loss: 1715.7363\n",
      "  Dynamics Model 3 Loss: 1226.6522\n",
      "  Behavior Model 1 Loss: 486.5951\n",
      "  Behavior Model 2 Loss: 479.2348\n",
      "  Behavior Model 3 Loss: 512.3977\n",
      "Epoch 395/500\n",
      "  Dynamics Model 1 Loss: 1162.6587\n",
      "  Dynamics Model 2 Loss: 1714.7928\n",
      "  Dynamics Model 3 Loss: 1226.2917\n",
      "  Behavior Model 1 Loss: 488.0548\n",
      "  Behavior Model 2 Loss: 479.5210\n",
      "  Behavior Model 3 Loss: 513.0406\n",
      "Epoch 396/500\n",
      "  Dynamics Model 1 Loss: 1163.6945\n",
      "  Dynamics Model 2 Loss: 1715.2082\n",
      "  Dynamics Model 3 Loss: 1225.0594\n",
      "  Behavior Model 1 Loss: 487.6344\n",
      "  Behavior Model 2 Loss: 479.1921\n",
      "  Behavior Model 3 Loss: 513.0048\n",
      "Epoch 397/500\n",
      "  Dynamics Model 1 Loss: 1159.4875\n",
      "  Dynamics Model 2 Loss: 1717.1631\n",
      "  Dynamics Model 3 Loss: 1227.1509\n",
      "  Behavior Model 1 Loss: 488.9931\n",
      "  Behavior Model 2 Loss: 479.7740\n",
      "  Behavior Model 3 Loss: 513.0941\n",
      "Epoch 398/500\n",
      "  Dynamics Model 1 Loss: 1161.5896\n",
      "  Dynamics Model 2 Loss: 1716.9330\n",
      "  Dynamics Model 3 Loss: 1226.4386\n",
      "  Behavior Model 1 Loss: 486.7689\n",
      "  Behavior Model 2 Loss: 478.2663\n",
      "  Behavior Model 3 Loss: 512.9485\n",
      "Epoch 399/500\n",
      "  Dynamics Model 1 Loss: 1160.7504\n",
      "  Dynamics Model 2 Loss: 1714.0164\n",
      "  Dynamics Model 3 Loss: 1227.2775\n",
      "  Behavior Model 1 Loss: 486.7181\n",
      "  Behavior Model 2 Loss: 478.4192\n",
      "  Behavior Model 3 Loss: 513.3422\n",
      "Epoch 400/500\n",
      "  Dynamics Model 1 Loss: 1163.2546\n",
      "  Dynamics Model 2 Loss: 1714.8939\n",
      "  Dynamics Model 3 Loss: 1226.1128\n",
      "  Behavior Model 1 Loss: 487.6111\n",
      "  Behavior Model 2 Loss: 478.6992\n",
      "  Behavior Model 3 Loss: 513.0777\n",
      "Epoch 401/500\n",
      "  Dynamics Model 1 Loss: 1163.4337\n",
      "  Dynamics Model 2 Loss: 1713.5155\n",
      "  Dynamics Model 3 Loss: 1225.9956\n",
      "  Behavior Model 1 Loss: 486.4250\n",
      "  Behavior Model 2 Loss: 478.3193\n",
      "  Behavior Model 3 Loss: 514.3857\n",
      "Epoch 402/500\n",
      "  Dynamics Model 1 Loss: 1161.4547\n",
      "  Dynamics Model 2 Loss: 1716.9428\n",
      "  Dynamics Model 3 Loss: 1225.8625\n",
      "  Behavior Model 1 Loss: 487.3290\n",
      "  Behavior Model 2 Loss: 478.0150\n",
      "  Behavior Model 3 Loss: 513.0163\n",
      "Epoch 403/500\n",
      "  Dynamics Model 1 Loss: 1163.1289\n",
      "  Dynamics Model 2 Loss: 1711.8558\n",
      "  Dynamics Model 3 Loss: 1225.9553\n",
      "  Behavior Model 1 Loss: 487.1976\n",
      "  Behavior Model 2 Loss: 479.8184\n",
      "  Behavior Model 3 Loss: 511.9907\n",
      "Epoch 404/500\n",
      "  Dynamics Model 1 Loss: 1162.2328\n",
      "  Dynamics Model 2 Loss: 1713.9842\n",
      "  Dynamics Model 3 Loss: 1225.1488\n",
      "  Behavior Model 1 Loss: 487.9775\n",
      "  Behavior Model 2 Loss: 478.9394\n",
      "  Behavior Model 3 Loss: 512.4158\n",
      "Epoch 405/500\n",
      "  Dynamics Model 1 Loss: 1163.2693\n",
      "  Dynamics Model 2 Loss: 1713.8322\n",
      "  Dynamics Model 3 Loss: 1224.7488\n",
      "  Behavior Model 1 Loss: 485.4024\n",
      "  Behavior Model 2 Loss: 482.1709\n",
      "  Behavior Model 3 Loss: 510.7786\n",
      "Epoch 406/500\n",
      "  Dynamics Model 1 Loss: 1162.9129\n",
      "  Dynamics Model 2 Loss: 1714.6997\n",
      "  Dynamics Model 3 Loss: 1227.8066\n",
      "  Behavior Model 1 Loss: 486.4208\n",
      "  Behavior Model 2 Loss: 477.0208\n",
      "  Behavior Model 3 Loss: 510.9957\n",
      "Epoch 407/500\n",
      "  Dynamics Model 1 Loss: 1161.4852\n",
      "  Dynamics Model 2 Loss: 1715.1592\n",
      "  Dynamics Model 3 Loss: 1225.5945\n",
      "  Behavior Model 1 Loss: 487.7092\n",
      "  Behavior Model 2 Loss: 478.1802\n",
      "  Behavior Model 3 Loss: 511.5786\n",
      "Epoch 408/500\n",
      "  Dynamics Model 1 Loss: 1161.0203\n",
      "  Dynamics Model 2 Loss: 1715.8252\n",
      "  Dynamics Model 3 Loss: 1223.8598\n",
      "  Behavior Model 1 Loss: 489.4401\n",
      "  Behavior Model 2 Loss: 479.7003\n",
      "  Behavior Model 3 Loss: 510.8251\n",
      "Epoch 409/500\n",
      "  Dynamics Model 1 Loss: 1163.3464\n",
      "  Dynamics Model 2 Loss: 1719.1733\n",
      "  Dynamics Model 3 Loss: 1226.4712\n",
      "  Behavior Model 1 Loss: 486.9991\n",
      "  Behavior Model 2 Loss: 480.2568\n",
      "  Behavior Model 3 Loss: 513.1459\n",
      "Epoch 410/500\n",
      "  Dynamics Model 1 Loss: 1163.1232\n",
      "  Dynamics Model 2 Loss: 1712.3381\n",
      "  Dynamics Model 3 Loss: 1228.9538\n",
      "  Behavior Model 1 Loss: 486.5363\n",
      "  Behavior Model 2 Loss: 478.1818\n",
      "  Behavior Model 3 Loss: 513.0332\n",
      "Epoch 411/500\n",
      "  Dynamics Model 1 Loss: 1161.2838\n",
      "  Dynamics Model 2 Loss: 1714.0939\n",
      "  Dynamics Model 3 Loss: 1226.9050\n",
      "  Behavior Model 1 Loss: 489.7268\n",
      "  Behavior Model 2 Loss: 478.9362\n",
      "  Behavior Model 3 Loss: 514.3840\n",
      "Epoch 412/500\n",
      "  Dynamics Model 1 Loss: 1161.3068\n",
      "  Dynamics Model 2 Loss: 1715.1192\n",
      "  Dynamics Model 3 Loss: 1226.6938\n",
      "  Behavior Model 1 Loss: 487.8010\n",
      "  Behavior Model 2 Loss: 477.3547\n",
      "  Behavior Model 3 Loss: 512.6112\n",
      "Epoch 413/500\n",
      "  Dynamics Model 1 Loss: 1163.9166\n",
      "  Dynamics Model 2 Loss: 1712.4003\n",
      "  Dynamics Model 3 Loss: 1228.3781\n",
      "  Behavior Model 1 Loss: 487.7955\n",
      "  Behavior Model 2 Loss: 477.6403\n",
      "  Behavior Model 3 Loss: 511.9411\n",
      "Epoch 414/500\n",
      "  Dynamics Model 1 Loss: 1160.8533\n",
      "  Dynamics Model 2 Loss: 1714.8637\n",
      "  Dynamics Model 3 Loss: 1227.2535\n",
      "  Behavior Model 1 Loss: 488.0932\n",
      "  Behavior Model 2 Loss: 479.9009\n",
      "  Behavior Model 3 Loss: 512.7106\n",
      "Epoch 415/500\n",
      "  Dynamics Model 1 Loss: 1162.6035\n",
      "  Dynamics Model 2 Loss: 1713.6340\n",
      "  Dynamics Model 3 Loss: 1227.1008\n",
      "  Behavior Model 1 Loss: 488.5974\n",
      "  Behavior Model 2 Loss: 479.6808\n",
      "  Behavior Model 3 Loss: 510.8771\n",
      "Epoch 416/500\n",
      "  Dynamics Model 1 Loss: 1162.4617\n",
      "  Dynamics Model 2 Loss: 1716.2552\n",
      "  Dynamics Model 3 Loss: 1224.0390\n",
      "  Behavior Model 1 Loss: 488.0184\n",
      "  Behavior Model 2 Loss: 479.2338\n",
      "  Behavior Model 3 Loss: 512.5752\n",
      "Epoch 417/500\n",
      "  Dynamics Model 1 Loss: 1161.6134\n",
      "  Dynamics Model 2 Loss: 1713.9177\n",
      "  Dynamics Model 3 Loss: 1225.2319\n",
      "  Behavior Model 1 Loss: 490.2512\n",
      "  Behavior Model 2 Loss: 477.6287\n",
      "  Behavior Model 3 Loss: 511.8584\n",
      "Epoch 418/500\n",
      "  Dynamics Model 1 Loss: 1159.6622\n",
      "  Dynamics Model 2 Loss: 1714.5312\n",
      "  Dynamics Model 3 Loss: 1226.6456\n",
      "  Behavior Model 1 Loss: 489.6270\n",
      "  Behavior Model 2 Loss: 479.0483\n",
      "  Behavior Model 3 Loss: 511.4161\n",
      "Epoch 419/500\n",
      "  Dynamics Model 1 Loss: 1161.3057\n",
      "  Dynamics Model 2 Loss: 1715.9380\n",
      "  Dynamics Model 3 Loss: 1225.2777\n",
      "  Behavior Model 1 Loss: 486.5144\n",
      "  Behavior Model 2 Loss: 477.9415\n",
      "  Behavior Model 3 Loss: 513.9287\n",
      "Epoch 420/500\n",
      "  Dynamics Model 1 Loss: 1162.5385\n",
      "  Dynamics Model 2 Loss: 1717.0160\n",
      "  Dynamics Model 3 Loss: 1228.1107\n",
      "  Behavior Model 1 Loss: 488.5794\n",
      "  Behavior Model 2 Loss: 477.1381\n",
      "  Behavior Model 3 Loss: 513.5430\n",
      "Epoch 421/500\n",
      "  Dynamics Model 1 Loss: 1162.4886\n",
      "  Dynamics Model 2 Loss: 1714.4838\n",
      "  Dynamics Model 3 Loss: 1226.6696\n",
      "  Behavior Model 1 Loss: 487.2194\n",
      "  Behavior Model 2 Loss: 480.5298\n",
      "  Behavior Model 3 Loss: 511.6577\n",
      "Epoch 422/500\n",
      "  Dynamics Model 1 Loss: 1163.5528\n",
      "  Dynamics Model 2 Loss: 1714.9939\n",
      "  Dynamics Model 3 Loss: 1226.4218\n",
      "  Behavior Model 1 Loss: 486.7913\n",
      "  Behavior Model 2 Loss: 478.7905\n",
      "  Behavior Model 3 Loss: 514.5176\n",
      "Epoch 423/500\n",
      "  Dynamics Model 1 Loss: 1161.2068\n",
      "  Dynamics Model 2 Loss: 1717.4901\n",
      "  Dynamics Model 3 Loss: 1226.5706\n",
      "  Behavior Model 1 Loss: 488.1513\n",
      "  Behavior Model 2 Loss: 479.2967\n",
      "  Behavior Model 3 Loss: 511.2625\n",
      "Epoch 424/500\n",
      "  Dynamics Model 1 Loss: 1162.4408\n",
      "  Dynamics Model 2 Loss: 1714.8254\n",
      "  Dynamics Model 3 Loss: 1226.6422\n",
      "  Behavior Model 1 Loss: 489.2390\n",
      "  Behavior Model 2 Loss: 480.8992\n",
      "  Behavior Model 3 Loss: 513.4981\n",
      "Epoch 425/500\n",
      "  Dynamics Model 1 Loss: 1162.5319\n",
      "  Dynamics Model 2 Loss: 1714.4503\n",
      "  Dynamics Model 3 Loss: 1227.1777\n",
      "  Behavior Model 1 Loss: 489.7122\n",
      "  Behavior Model 2 Loss: 478.0755\n",
      "  Behavior Model 3 Loss: 514.1798\n",
      "Epoch 426/500\n",
      "  Dynamics Model 1 Loss: 1162.4915\n",
      "  Dynamics Model 2 Loss: 1713.6136\n",
      "  Dynamics Model 3 Loss: 1225.7079\n",
      "  Behavior Model 1 Loss: 487.5578\n",
      "  Behavior Model 2 Loss: 477.7817\n",
      "  Behavior Model 3 Loss: 513.0660\n",
      "Epoch 427/500\n",
      "  Dynamics Model 1 Loss: 1162.0401\n",
      "  Dynamics Model 2 Loss: 1713.0794\n",
      "  Dynamics Model 3 Loss: 1226.4747\n",
      "  Behavior Model 1 Loss: 488.7768\n",
      "  Behavior Model 2 Loss: 478.2208\n",
      "  Behavior Model 3 Loss: 513.4611\n",
      "Epoch 428/500\n",
      "  Dynamics Model 1 Loss: 1163.0191\n",
      "  Dynamics Model 2 Loss: 1713.0304\n",
      "  Dynamics Model 3 Loss: 1224.4863\n",
      "  Behavior Model 1 Loss: 488.8659\n",
      "  Behavior Model 2 Loss: 478.7328\n",
      "  Behavior Model 3 Loss: 512.9067\n",
      "Epoch 429/500\n",
      "  Dynamics Model 1 Loss: 1164.7392\n",
      "  Dynamics Model 2 Loss: 1716.6177\n",
      "  Dynamics Model 3 Loss: 1225.4615\n",
      "  Behavior Model 1 Loss: 488.7121\n",
      "  Behavior Model 2 Loss: 479.6845\n",
      "  Behavior Model 3 Loss: 513.5198\n",
      "Epoch 430/500\n",
      "  Dynamics Model 1 Loss: 1166.2007\n",
      "  Dynamics Model 2 Loss: 1716.3870\n",
      "  Dynamics Model 3 Loss: 1228.7719\n",
      "  Behavior Model 1 Loss: 486.4182\n",
      "  Behavior Model 2 Loss: 481.4739\n",
      "  Behavior Model 3 Loss: 511.3406\n",
      "Epoch 431/500\n",
      "  Dynamics Model 1 Loss: 1163.0745\n",
      "  Dynamics Model 2 Loss: 1712.7127\n",
      "  Dynamics Model 3 Loss: 1226.1998\n",
      "  Behavior Model 1 Loss: 487.8998\n",
      "  Behavior Model 2 Loss: 478.6919\n",
      "  Behavior Model 3 Loss: 512.6100\n",
      "Epoch 432/500\n",
      "  Dynamics Model 1 Loss: 1160.9223\n",
      "  Dynamics Model 2 Loss: 1714.9168\n",
      "  Dynamics Model 3 Loss: 1227.1528\n",
      "  Behavior Model 1 Loss: 488.3550\n",
      "  Behavior Model 2 Loss: 478.5889\n",
      "  Behavior Model 3 Loss: 512.1841\n",
      "Epoch 433/500\n",
      "  Dynamics Model 1 Loss: 1162.7488\n",
      "  Dynamics Model 2 Loss: 1715.5279\n",
      "  Dynamics Model 3 Loss: 1226.3589\n",
      "  Behavior Model 1 Loss: 487.5880\n",
      "  Behavior Model 2 Loss: 479.8511\n",
      "  Behavior Model 3 Loss: 512.9232\n",
      "Epoch 434/500\n",
      "  Dynamics Model 1 Loss: 1162.6778\n",
      "  Dynamics Model 2 Loss: 1713.1207\n",
      "  Dynamics Model 3 Loss: 1227.5328\n",
      "  Behavior Model 1 Loss: 486.1430\n",
      "  Behavior Model 2 Loss: 478.9923\n",
      "  Behavior Model 3 Loss: 512.1701\n",
      "Epoch 435/500\n",
      "  Dynamics Model 1 Loss: 1162.9253\n",
      "  Dynamics Model 2 Loss: 1715.7134\n",
      "  Dynamics Model 3 Loss: 1223.8748\n",
      "  Behavior Model 1 Loss: 488.4401\n",
      "  Behavior Model 2 Loss: 480.7882\n",
      "  Behavior Model 3 Loss: 510.9900\n",
      "Epoch 436/500\n",
      "  Dynamics Model 1 Loss: 1162.9727\n",
      "  Dynamics Model 2 Loss: 1715.7094\n",
      "  Dynamics Model 3 Loss: 1226.7675\n",
      "  Behavior Model 1 Loss: 489.1310\n",
      "  Behavior Model 2 Loss: 478.5834\n",
      "  Behavior Model 3 Loss: 515.3225\n",
      "Epoch 437/500\n",
      "  Dynamics Model 1 Loss: 1162.6633\n",
      "  Dynamics Model 2 Loss: 1714.7982\n",
      "  Dynamics Model 3 Loss: 1225.9860\n",
      "  Behavior Model 1 Loss: 487.8603\n",
      "  Behavior Model 2 Loss: 478.7200\n",
      "  Behavior Model 3 Loss: 511.0843\n",
      "Epoch 438/500\n",
      "  Dynamics Model 1 Loss: 1162.2180\n",
      "  Dynamics Model 2 Loss: 1713.0557\n",
      "  Dynamics Model 3 Loss: 1225.4229\n",
      "  Behavior Model 1 Loss: 488.7140\n",
      "  Behavior Model 2 Loss: 479.4611\n",
      "  Behavior Model 3 Loss: 511.1916\n",
      "Epoch 439/500\n",
      "  Dynamics Model 1 Loss: 1162.6815\n",
      "  Dynamics Model 2 Loss: 1715.9088\n",
      "  Dynamics Model 3 Loss: 1225.5157\n",
      "  Behavior Model 1 Loss: 487.2697\n",
      "  Behavior Model 2 Loss: 478.2211\n",
      "  Behavior Model 3 Loss: 512.4451\n",
      "Epoch 440/500\n",
      "  Dynamics Model 1 Loss: 1164.7198\n",
      "  Dynamics Model 2 Loss: 1713.3257\n",
      "  Dynamics Model 3 Loss: 1224.1411\n",
      "  Behavior Model 1 Loss: 485.9929\n",
      "  Behavior Model 2 Loss: 479.1693\n",
      "  Behavior Model 3 Loss: 511.5332\n",
      "Epoch 441/500\n",
      "  Dynamics Model 1 Loss: 1164.0061\n",
      "  Dynamics Model 2 Loss: 1714.7863\n",
      "  Dynamics Model 3 Loss: 1227.4361\n",
      "  Behavior Model 1 Loss: 487.4638\n",
      "  Behavior Model 2 Loss: 478.4932\n",
      "  Behavior Model 3 Loss: 512.5775\n",
      "Epoch 442/500\n",
      "  Dynamics Model 1 Loss: 1164.1943\n",
      "  Dynamics Model 2 Loss: 1714.9787\n",
      "  Dynamics Model 3 Loss: 1226.1937\n",
      "  Behavior Model 1 Loss: 488.1138\n",
      "  Behavior Model 2 Loss: 479.4100\n",
      "  Behavior Model 3 Loss: 510.6191\n",
      "Epoch 443/500\n",
      "  Dynamics Model 1 Loss: 1163.0083\n",
      "  Dynamics Model 2 Loss: 1712.7689\n",
      "  Dynamics Model 3 Loss: 1226.4461\n",
      "  Behavior Model 1 Loss: 489.0246\n",
      "  Behavior Model 2 Loss: 477.0595\n",
      "  Behavior Model 3 Loss: 511.8892\n",
      "Epoch 444/500\n",
      "  Dynamics Model 1 Loss: 1161.7863\n",
      "  Dynamics Model 2 Loss: 1715.0130\n",
      "  Dynamics Model 3 Loss: 1227.5726\n",
      "  Behavior Model 1 Loss: 488.7077\n",
      "  Behavior Model 2 Loss: 478.1124\n",
      "  Behavior Model 3 Loss: 511.4413\n",
      "Epoch 445/500\n",
      "  Dynamics Model 1 Loss: 1164.5244\n",
      "  Dynamics Model 2 Loss: 1715.9520\n",
      "  Dynamics Model 3 Loss: 1224.3970\n",
      "  Behavior Model 1 Loss: 487.9793\n",
      "  Behavior Model 2 Loss: 479.1697\n",
      "  Behavior Model 3 Loss: 513.3400\n",
      "Epoch 446/500\n",
      "  Dynamics Model 1 Loss: 1161.9533\n",
      "  Dynamics Model 2 Loss: 1715.1036\n",
      "  Dynamics Model 3 Loss: 1225.3785\n",
      "  Behavior Model 1 Loss: 487.0841\n",
      "  Behavior Model 2 Loss: 477.5875\n",
      "  Behavior Model 3 Loss: 511.2189\n",
      "Epoch 447/500\n",
      "  Dynamics Model 1 Loss: 1163.0610\n",
      "  Dynamics Model 2 Loss: 1715.1264\n",
      "  Dynamics Model 3 Loss: 1225.4516\n",
      "  Behavior Model 1 Loss: 489.0565\n",
      "  Behavior Model 2 Loss: 478.9544\n",
      "  Behavior Model 3 Loss: 512.4019\n",
      "Epoch 448/500\n",
      "  Dynamics Model 1 Loss: 1162.4576\n",
      "  Dynamics Model 2 Loss: 1714.1151\n",
      "  Dynamics Model 3 Loss: 1224.6630\n",
      "  Behavior Model 1 Loss: 486.7885\n",
      "  Behavior Model 2 Loss: 478.4800\n",
      "  Behavior Model 3 Loss: 511.7584\n",
      "Epoch 449/500\n",
      "  Dynamics Model 1 Loss: 1163.3734\n",
      "  Dynamics Model 2 Loss: 1714.6542\n",
      "  Dynamics Model 3 Loss: 1226.7777\n",
      "  Behavior Model 1 Loss: 485.4448\n",
      "  Behavior Model 2 Loss: 479.4380\n",
      "  Behavior Model 3 Loss: 512.6037\n",
      "Epoch 450/500\n",
      "  Dynamics Model 1 Loss: 1161.7711\n",
      "  Dynamics Model 2 Loss: 1714.2013\n",
      "  Dynamics Model 3 Loss: 1223.8189\n",
      "  Behavior Model 1 Loss: 488.9191\n",
      "  Behavior Model 2 Loss: 476.3415\n",
      "  Behavior Model 3 Loss: 510.3560\n",
      "Epoch 451/500\n",
      "  Dynamics Model 1 Loss: 1163.6805\n",
      "  Dynamics Model 2 Loss: 1715.3742\n",
      "  Dynamics Model 3 Loss: 1226.8504\n",
      "  Behavior Model 1 Loss: 488.7972\n",
      "  Behavior Model 2 Loss: 479.6643\n",
      "  Behavior Model 3 Loss: 512.4891\n",
      "Epoch 452/500\n",
      "  Dynamics Model 1 Loss: 1165.5453\n",
      "  Dynamics Model 2 Loss: 1715.5844\n",
      "  Dynamics Model 3 Loss: 1226.9405\n",
      "  Behavior Model 1 Loss: 488.6330\n",
      "  Behavior Model 2 Loss: 478.4067\n",
      "  Behavior Model 3 Loss: 510.5310\n",
      "Epoch 453/500\n",
      "  Dynamics Model 1 Loss: 1164.7625\n",
      "  Dynamics Model 2 Loss: 1712.9652\n",
      "  Dynamics Model 3 Loss: 1226.7162\n",
      "  Behavior Model 1 Loss: 487.2681\n",
      "  Behavior Model 2 Loss: 477.7618\n",
      "  Behavior Model 3 Loss: 510.8335\n",
      "Epoch 454/500\n",
      "  Dynamics Model 1 Loss: 1163.2068\n",
      "  Dynamics Model 2 Loss: 1714.4295\n",
      "  Dynamics Model 3 Loss: 1228.0699\n",
      "  Behavior Model 1 Loss: 487.7915\n",
      "  Behavior Model 2 Loss: 480.2940\n",
      "  Behavior Model 3 Loss: 511.7475\n",
      "Epoch 455/500\n",
      "  Dynamics Model 1 Loss: 1163.9115\n",
      "  Dynamics Model 2 Loss: 1714.4109\n",
      "  Dynamics Model 3 Loss: 1224.5324\n",
      "  Behavior Model 1 Loss: 488.2879\n",
      "  Behavior Model 2 Loss: 479.6310\n",
      "  Behavior Model 3 Loss: 512.8819\n",
      "Epoch 456/500\n",
      "  Dynamics Model 1 Loss: 1160.8031\n",
      "  Dynamics Model 2 Loss: 1713.7915\n",
      "  Dynamics Model 3 Loss: 1226.9722\n",
      "  Behavior Model 1 Loss: 486.5074\n",
      "  Behavior Model 2 Loss: 479.1924\n",
      "  Behavior Model 3 Loss: 515.5310\n",
      "Epoch 457/500\n",
      "  Dynamics Model 1 Loss: 1162.7656\n",
      "  Dynamics Model 2 Loss: 1716.0073\n",
      "  Dynamics Model 3 Loss: 1225.5832\n",
      "  Behavior Model 1 Loss: 487.2253\n",
      "  Behavior Model 2 Loss: 478.2645\n",
      "  Behavior Model 3 Loss: 511.9560\n",
      "Epoch 458/500\n",
      "  Dynamics Model 1 Loss: 1161.5632\n",
      "  Dynamics Model 2 Loss: 1713.1752\n",
      "  Dynamics Model 3 Loss: 1226.4039\n",
      "  Behavior Model 1 Loss: 487.6252\n",
      "  Behavior Model 2 Loss: 479.6641\n",
      "  Behavior Model 3 Loss: 509.8161\n",
      "Epoch 459/500\n",
      "  Dynamics Model 1 Loss: 1161.2654\n",
      "  Dynamics Model 2 Loss: 1717.3567\n",
      "  Dynamics Model 3 Loss: 1227.1793\n",
      "  Behavior Model 1 Loss: 485.4034\n",
      "  Behavior Model 2 Loss: 476.2233\n",
      "  Behavior Model 3 Loss: 510.7105\n",
      "Epoch 460/500\n",
      "  Dynamics Model 1 Loss: 1161.0383\n",
      "  Dynamics Model 2 Loss: 1715.4800\n",
      "  Dynamics Model 3 Loss: 1227.5870\n",
      "  Behavior Model 1 Loss: 487.3488\n",
      "  Behavior Model 2 Loss: 478.3620\n",
      "  Behavior Model 3 Loss: 512.5014\n",
      "Epoch 461/500\n",
      "  Dynamics Model 1 Loss: 1162.4431\n",
      "  Dynamics Model 2 Loss: 1713.4238\n",
      "  Dynamics Model 3 Loss: 1227.9597\n",
      "  Behavior Model 1 Loss: 487.5793\n",
      "  Behavior Model 2 Loss: 479.3707\n",
      "  Behavior Model 3 Loss: 513.3828\n",
      "Epoch 462/500\n",
      "  Dynamics Model 1 Loss: 1161.7048\n",
      "  Dynamics Model 2 Loss: 1712.3300\n",
      "  Dynamics Model 3 Loss: 1226.1732\n",
      "  Behavior Model 1 Loss: 486.3778\n",
      "  Behavior Model 2 Loss: 479.3810\n",
      "  Behavior Model 3 Loss: 511.1906\n",
      "Epoch 463/500\n",
      "  Dynamics Model 1 Loss: 1162.2251\n",
      "  Dynamics Model 2 Loss: 1716.8859\n",
      "  Dynamics Model 3 Loss: 1227.1910\n",
      "  Behavior Model 1 Loss: 486.5413\n",
      "  Behavior Model 2 Loss: 480.6953\n",
      "  Behavior Model 3 Loss: 512.2187\n",
      "Epoch 464/500\n",
      "  Dynamics Model 1 Loss: 1162.8503\n",
      "  Dynamics Model 2 Loss: 1714.4801\n",
      "  Dynamics Model 3 Loss: 1227.4285\n",
      "  Behavior Model 1 Loss: 489.1300\n",
      "  Behavior Model 2 Loss: 477.5855\n",
      "  Behavior Model 3 Loss: 511.6428\n",
      "Epoch 465/500\n",
      "  Dynamics Model 1 Loss: 1164.5531\n",
      "  Dynamics Model 2 Loss: 1713.6059\n",
      "  Dynamics Model 3 Loss: 1223.9828\n",
      "  Behavior Model 1 Loss: 490.5412\n",
      "  Behavior Model 2 Loss: 478.8290\n",
      "  Behavior Model 3 Loss: 510.5655\n",
      "Epoch 466/500\n",
      "  Dynamics Model 1 Loss: 1161.6399\n",
      "  Dynamics Model 2 Loss: 1714.5856\n",
      "  Dynamics Model 3 Loss: 1226.9163\n",
      "  Behavior Model 1 Loss: 485.9138\n",
      "  Behavior Model 2 Loss: 479.0773\n",
      "  Behavior Model 3 Loss: 511.8638\n",
      "Epoch 467/500\n",
      "  Dynamics Model 1 Loss: 1162.5918\n",
      "  Dynamics Model 2 Loss: 1714.6867\n",
      "  Dynamics Model 3 Loss: 1229.4099\n",
      "  Behavior Model 1 Loss: 487.8814\n",
      "  Behavior Model 2 Loss: 478.7863\n",
      "  Behavior Model 3 Loss: 511.1065\n",
      "Epoch 468/500\n",
      "  Dynamics Model 1 Loss: 1162.7234\n",
      "  Dynamics Model 2 Loss: 1714.5872\n",
      "  Dynamics Model 3 Loss: 1224.5373\n",
      "  Behavior Model 1 Loss: 486.9693\n",
      "  Behavior Model 2 Loss: 478.3531\n",
      "  Behavior Model 3 Loss: 513.3218\n",
      "Epoch 469/500\n",
      "  Dynamics Model 1 Loss: 1162.1589\n",
      "  Dynamics Model 2 Loss: 1716.7456\n",
      "  Dynamics Model 3 Loss: 1224.0809\n",
      "  Behavior Model 1 Loss: 490.8401\n",
      "  Behavior Model 2 Loss: 480.0742\n",
      "  Behavior Model 3 Loss: 512.7694\n",
      "Epoch 470/500\n",
      "  Dynamics Model 1 Loss: 1161.7618\n",
      "  Dynamics Model 2 Loss: 1715.2079\n",
      "  Dynamics Model 3 Loss: 1225.2476\n",
      "  Behavior Model 1 Loss: 488.2283\n",
      "  Behavior Model 2 Loss: 480.4996\n",
      "  Behavior Model 3 Loss: 511.6891\n",
      "Epoch 471/500\n",
      "  Dynamics Model 1 Loss: 1163.6239\n",
      "  Dynamics Model 2 Loss: 1715.3982\n",
      "  Dynamics Model 3 Loss: 1223.3468\n",
      "  Behavior Model 1 Loss: 489.2574\n",
      "  Behavior Model 2 Loss: 477.8557\n",
      "  Behavior Model 3 Loss: 512.4520\n",
      "Epoch 472/500\n",
      "  Dynamics Model 1 Loss: 1163.8023\n",
      "  Dynamics Model 2 Loss: 1717.0514\n",
      "  Dynamics Model 3 Loss: 1225.6035\n",
      "  Behavior Model 1 Loss: 488.2637\n",
      "  Behavior Model 2 Loss: 481.3777\n",
      "  Behavior Model 3 Loss: 512.4286\n",
      "Epoch 473/500\n",
      "  Dynamics Model 1 Loss: 1160.0202\n",
      "  Dynamics Model 2 Loss: 1715.1474\n",
      "  Dynamics Model 3 Loss: 1225.7620\n",
      "  Behavior Model 1 Loss: 488.4979\n",
      "  Behavior Model 2 Loss: 479.3096\n",
      "  Behavior Model 3 Loss: 511.2659\n",
      "Epoch 474/500\n",
      "  Dynamics Model 1 Loss: 1161.9951\n",
      "  Dynamics Model 2 Loss: 1715.0836\n",
      "  Dynamics Model 3 Loss: 1228.4947\n",
      "  Behavior Model 1 Loss: 488.4349\n",
      "  Behavior Model 2 Loss: 478.9834\n",
      "  Behavior Model 3 Loss: 509.5659\n",
      "Epoch 475/500\n",
      "  Dynamics Model 1 Loss: 1162.2824\n",
      "  Dynamics Model 2 Loss: 1715.0716\n",
      "  Dynamics Model 3 Loss: 1225.5278\n",
      "  Behavior Model 1 Loss: 486.3929\n",
      "  Behavior Model 2 Loss: 478.9927\n",
      "  Behavior Model 3 Loss: 510.2601\n",
      "Epoch 476/500\n",
      "  Dynamics Model 1 Loss: 1162.3122\n",
      "  Dynamics Model 2 Loss: 1714.7787\n",
      "  Dynamics Model 3 Loss: 1225.2678\n",
      "  Behavior Model 1 Loss: 487.5478\n",
      "  Behavior Model 2 Loss: 478.1741\n",
      "  Behavior Model 3 Loss: 512.0070\n",
      "Epoch 477/500\n",
      "  Dynamics Model 1 Loss: 1163.7870\n",
      "  Dynamics Model 2 Loss: 1715.1459\n",
      "  Dynamics Model 3 Loss: 1226.6127\n",
      "  Behavior Model 1 Loss: 487.1812\n",
      "  Behavior Model 2 Loss: 480.3975\n",
      "  Behavior Model 3 Loss: 513.3143\n",
      "Epoch 478/500\n",
      "  Dynamics Model 1 Loss: 1162.7324\n",
      "  Dynamics Model 2 Loss: 1716.3967\n",
      "  Dynamics Model 3 Loss: 1225.1178\n",
      "  Behavior Model 1 Loss: 488.3908\n",
      "  Behavior Model 2 Loss: 477.4882\n",
      "  Behavior Model 3 Loss: 511.8439\n",
      "Epoch 479/500\n",
      "  Dynamics Model 1 Loss: 1163.2583\n",
      "  Dynamics Model 2 Loss: 1714.4600\n",
      "  Dynamics Model 3 Loss: 1226.0601\n",
      "  Behavior Model 1 Loss: 487.6333\n",
      "  Behavior Model 2 Loss: 477.1587\n",
      "  Behavior Model 3 Loss: 513.7536\n",
      "Epoch 480/500\n",
      "  Dynamics Model 1 Loss: 1162.2671\n",
      "  Dynamics Model 2 Loss: 1712.8084\n",
      "  Dynamics Model 3 Loss: 1226.7771\n",
      "  Behavior Model 1 Loss: 488.1918\n",
      "  Behavior Model 2 Loss: 476.9411\n",
      "  Behavior Model 3 Loss: 514.0786\n",
      "Epoch 481/500\n",
      "  Dynamics Model 1 Loss: 1160.0232\n",
      "  Dynamics Model 2 Loss: 1714.1523\n",
      "  Dynamics Model 3 Loss: 1226.2392\n",
      "  Behavior Model 1 Loss: 488.2745\n",
      "  Behavior Model 2 Loss: 477.9063\n",
      "  Behavior Model 3 Loss: 512.9961\n",
      "Epoch 482/500\n",
      "  Dynamics Model 1 Loss: 1161.5962\n",
      "  Dynamics Model 2 Loss: 1713.4942\n",
      "  Dynamics Model 3 Loss: 1226.7912\n",
      "  Behavior Model 1 Loss: 487.1028\n",
      "  Behavior Model 2 Loss: 479.4513\n",
      "  Behavior Model 3 Loss: 510.8525\n",
      "Epoch 483/500\n",
      "  Dynamics Model 1 Loss: 1163.7731\n",
      "  Dynamics Model 2 Loss: 1715.7993\n",
      "  Dynamics Model 3 Loss: 1226.1104\n",
      "  Behavior Model 1 Loss: 488.6021\n",
      "  Behavior Model 2 Loss: 479.1180\n",
      "  Behavior Model 3 Loss: 511.7582\n",
      "Epoch 484/500\n",
      "  Dynamics Model 1 Loss: 1164.3154\n",
      "  Dynamics Model 2 Loss: 1714.5536\n",
      "  Dynamics Model 3 Loss: 1224.9196\n",
      "  Behavior Model 1 Loss: 486.6405\n",
      "  Behavior Model 2 Loss: 478.4438\n",
      "  Behavior Model 3 Loss: 511.8081\n",
      "Epoch 485/500\n",
      "  Dynamics Model 1 Loss: 1164.4126\n",
      "  Dynamics Model 2 Loss: 1716.0005\n",
      "  Dynamics Model 3 Loss: 1226.0474\n",
      "  Behavior Model 1 Loss: 488.1807\n",
      "  Behavior Model 2 Loss: 478.3641\n",
      "  Behavior Model 3 Loss: 511.3176\n",
      "Epoch 486/500\n",
      "  Dynamics Model 1 Loss: 1162.8080\n",
      "  Dynamics Model 2 Loss: 1715.5777\n",
      "  Dynamics Model 3 Loss: 1226.1680\n",
      "  Behavior Model 1 Loss: 487.0080\n",
      "  Behavior Model 2 Loss: 480.0209\n",
      "  Behavior Model 3 Loss: 510.5652\n",
      "Epoch 487/500\n",
      "  Dynamics Model 1 Loss: 1163.2038\n",
      "  Dynamics Model 2 Loss: 1717.3561\n",
      "  Dynamics Model 3 Loss: 1225.9144\n",
      "  Behavior Model 1 Loss: 487.6766\n",
      "  Behavior Model 2 Loss: 478.1959\n",
      "  Behavior Model 3 Loss: 512.8388\n",
      "Epoch 488/500\n",
      "  Dynamics Model 1 Loss: 1161.5755\n",
      "  Dynamics Model 2 Loss: 1714.8981\n",
      "  Dynamics Model 3 Loss: 1225.5524\n",
      "  Behavior Model 1 Loss: 488.4635\n",
      "  Behavior Model 2 Loss: 478.3064\n",
      "  Behavior Model 3 Loss: 510.4995\n",
      "Epoch 489/500\n",
      "  Dynamics Model 1 Loss: 1161.9650\n",
      "  Dynamics Model 2 Loss: 1715.4034\n",
      "  Dynamics Model 3 Loss: 1226.3068\n",
      "  Behavior Model 1 Loss: 488.9517\n",
      "  Behavior Model 2 Loss: 478.6716\n",
      "  Behavior Model 3 Loss: 515.2702\n",
      "Epoch 490/500\n",
      "  Dynamics Model 1 Loss: 1162.4319\n",
      "  Dynamics Model 2 Loss: 1714.8657\n",
      "  Dynamics Model 3 Loss: 1226.0374\n",
      "  Behavior Model 1 Loss: 488.7164\n",
      "  Behavior Model 2 Loss: 478.9181\n",
      "  Behavior Model 3 Loss: 513.1246\n",
      "Epoch 491/500\n",
      "  Dynamics Model 1 Loss: 1162.5556\n",
      "  Dynamics Model 2 Loss: 1716.6581\n",
      "  Dynamics Model 3 Loss: 1226.8938\n",
      "  Behavior Model 1 Loss: 486.8400\n",
      "  Behavior Model 2 Loss: 481.2926\n",
      "  Behavior Model 3 Loss: 513.2496\n",
      "Epoch 492/500\n",
      "  Dynamics Model 1 Loss: 1162.5001\n",
      "  Dynamics Model 2 Loss: 1715.6088\n",
      "  Dynamics Model 3 Loss: 1226.6901\n",
      "  Behavior Model 1 Loss: 487.8267\n",
      "  Behavior Model 2 Loss: 478.6146\n",
      "  Behavior Model 3 Loss: 511.2737\n",
      "Epoch 493/500\n",
      "  Dynamics Model 1 Loss: 1161.2786\n",
      "  Dynamics Model 2 Loss: 1716.1953\n",
      "  Dynamics Model 3 Loss: 1224.8739\n",
      "  Behavior Model 1 Loss: 487.0000\n",
      "  Behavior Model 2 Loss: 479.4509\n",
      "  Behavior Model 3 Loss: 512.4113\n",
      "Epoch 494/500\n",
      "  Dynamics Model 1 Loss: 1163.6457\n",
      "  Dynamics Model 2 Loss: 1715.4359\n",
      "  Dynamics Model 3 Loss: 1225.6727\n",
      "  Behavior Model 1 Loss: 488.3042\n",
      "  Behavior Model 2 Loss: 478.6635\n",
      "  Behavior Model 3 Loss: 511.6555\n",
      "Epoch 495/500\n",
      "  Dynamics Model 1 Loss: 1163.9802\n",
      "  Dynamics Model 2 Loss: 1713.6822\n",
      "  Dynamics Model 3 Loss: 1225.6953\n",
      "  Behavior Model 1 Loss: 487.8161\n",
      "  Behavior Model 2 Loss: 477.4392\n",
      "  Behavior Model 3 Loss: 510.9825\n",
      "Epoch 496/500\n",
      "  Dynamics Model 1 Loss: 1163.3182\n",
      "  Dynamics Model 2 Loss: 1718.7436\n",
      "  Dynamics Model 3 Loss: 1226.6596\n",
      "  Behavior Model 1 Loss: 487.6032\n",
      "  Behavior Model 2 Loss: 478.6955\n",
      "  Behavior Model 3 Loss: 511.2843\n",
      "Epoch 497/500\n",
      "  Dynamics Model 1 Loss: 1163.2975\n",
      "  Dynamics Model 2 Loss: 1715.6795\n",
      "  Dynamics Model 3 Loss: 1225.1843\n",
      "  Behavior Model 1 Loss: 485.5771\n",
      "  Behavior Model 2 Loss: 477.1369\n",
      "  Behavior Model 3 Loss: 512.0667\n",
      "Epoch 498/500\n",
      "  Dynamics Model 1 Loss: 1162.4170\n",
      "  Dynamics Model 2 Loss: 1714.3949\n",
      "  Dynamics Model 3 Loss: 1225.0393\n",
      "  Behavior Model 1 Loss: 485.7341\n",
      "  Behavior Model 2 Loss: 479.4917\n",
      "  Behavior Model 3 Loss: 511.6520\n",
      "Epoch 499/500\n",
      "  Dynamics Model 1 Loss: 1162.9450\n",
      "  Dynamics Model 2 Loss: 1716.4276\n",
      "  Dynamics Model 3 Loss: 1226.3333\n",
      "  Behavior Model 1 Loss: 487.8032\n",
      "  Behavior Model 2 Loss: 477.5136\n",
      "  Behavior Model 3 Loss: 513.7820\n",
      "Epoch 500/500\n",
      "  Dynamics Model 1 Loss: 1161.5902\n",
      "  Dynamics Model 2 Loss: 1714.0804\n",
      "  Dynamics Model 3 Loss: 1224.0756\n",
      "  Behavior Model 1 Loss: 488.0354\n",
      "  Behavior Model 2 Loss: 475.9616\n",
      "  Behavior Model 3 Loss: 510.0034\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Train Dynamics Ensemble\n",
    "    dynamics_losses = [0.0 for _ in range(K1)]\n",
    "    for state, action, next_state, reward in dataloader_dynamics:\n",
    "        state = state.to(device)\n",
    "        action = action.to(device)\n",
    "        next_state = next_state.to(device)\n",
    "        inputs = torch.cat([state, action], dim=-1)\n",
    "        targets = torch.cat([next_state, reward.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "        for i, (model, optimizer) in enumerate(zip(dynamics_ensemble.models, dynamics_optimizers)):\n",
    "            model.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            _, outs_sample, _, _ = model(inputs)\n",
    "            next_state = outs_sample[:, :state_dim]\n",
    "            reward = outs_sample[:, state_dim:]\n",
    "\n",
    "            # Compute log-likelihood loss\n",
    "            loss = torch.nn.functional.mse_loss(outs_sample, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            dynamics_losses[i] += loss.item()\n",
    "\n",
    "    # Train Behavior Ensemble\n",
    "    behavior_losses = [0.0 for _ in range(K2)]\n",
    "    for state, action in dataloader_behavior:\n",
    "        state = state.to(device)\n",
    "        action = action.to(device)\n",
    "        inputs = state\n",
    "        targets = action\n",
    "\n",
    "        for i, (model, optimizer) in enumerate(zip(behavior_ensemble.models, behavior_optimizers)):\n",
    "            optimizer.zero_grad()\n",
    "            _, outs_sample, _, _ = model(inputs)\n",
    "\n",
    "            # Compute log-likelihood loss\n",
    "            loss = torch.nn.functional.mse_loss(outs_sample, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            behavior_losses[i] += loss.item()\n",
    "\n",
    "    # Print epoch losses\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    for i, loss in enumerate(dynamics_losses):\n",
    "        print(f\"  Dynamics Model {i + 1} Loss: {loss:.4f}\")\n",
    "    for i, loss in enumerate(behavior_losses):\n",
    "        print(f\"  Behavior Model {i + 1} Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5a6a249",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T03:47:11.522898Z",
     "iopub.status.busy": "2025-01-18T03:47:11.522639Z",
     "iopub.status.idle": "2025-01-18T03:47:11.621874Z",
     "shell.execute_reply": "2025-01-18T03:47:11.621102Z"
    },
    "papermill": {
     "duration": 0.143606,
     "end_time": "2025-01-18T03:47:11.623134",
     "exception": false,
     "start_time": "2025-01-18T03:47:11.479528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models saved successfully.\n"
     ]
    }
   ],
   "source": [
    "for i, (model, optimizer) in enumerate(zip(dynamics_ensemble.models, dynamics_optimizers)):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, os.path.join(DYNAMICS_MODEL, f\"dynamics_model_{i + 1}_final.pth\"))\n",
    "\n",
    "for i, (model, optimizer) in enumerate(zip(behavior_ensemble.models, behavior_optimizers)):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, os.path.join(BEHAVIOR_MODEL, f\"behavior_model_{i + 1}_final.pth\"))\n",
    "\n",
    "print(\"All models saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03909cb",
   "metadata": {
    "papermill": {
     "duration": 0.041847,
     "end_time": "2025-01-18T03:47:11.708451",
     "exception": false,
     "start_time": "2025-01-18T03:47:11.666604",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3. Value Function Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b856ac66",
   "metadata": {
    "papermill": {
     "duration": 0.041996,
     "end_time": "2025-01-18T03:47:11.792794",
     "exception": false,
     "start_time": "2025-01-18T03:47:11.750798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Q-network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "259903a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T03:47:11.879737Z",
     "iopub.status.busy": "2025-01-18T03:47:11.879455Z",
     "iopub.status.idle": "2025-01-18T03:47:11.884380Z",
     "shell.execute_reply": "2025-01-18T03:47:11.883562Z"
    },
    "papermill": {
     "duration": 0.049568,
     "end_time": "2025-01-18T03:47:11.885575",
     "exception": false,
     "start_time": "2025-01-18T03:47:11.836007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims):\n",
    "        super(QNetwork, self).__init__()\n",
    "        input_dim = state_dim + action_dim  # Combine state and action dimensions\n",
    "        layers = []\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_dim\n",
    "        layers.append(nn.Linear(input_dim, 1))  # Single output for Q-value\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)  # Concatenate state and action\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c032b86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T03:47:11.972683Z",
     "iopub.status.busy": "2025-01-18T03:47:11.972430Z",
     "iopub.status.idle": "2025-01-18T03:47:11.975709Z",
     "shell.execute_reply": "2025-01-18T03:47:11.974987Z"
    },
    "papermill": {
     "duration": 0.04868,
     "end_time": "2025-01-18T03:47:11.976817",
     "exception": false,
     "start_time": "2025-01-18T03:47:11.928137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_models = 3\n",
    "num_q_networks = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79417926",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T03:47:12.063298Z",
     "iopub.status.busy": "2025-01-18T03:47:12.063037Z",
     "iopub.status.idle": "2025-01-18T03:47:12.075047Z",
     "shell.execute_reply": "2025-01-18T03:47:12.074487Z"
    },
    "papermill": {
     "duration": 0.056819,
     "end_time": "2025-01-18T03:47:12.076294",
     "exception": false,
     "start_time": "2025-01-18T03:47:12.019475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Q-network ensemble\n",
    "hidden_dims = [500, 200, 100]  # Example hidden layers\n",
    "q_networks = [QNetwork(state_dim, action_dim, hidden_dims) for _ in range(3)]\n",
    "target_q_networks = [QNetwork(state_dim, action_dim, hidden_dims) for _ in range(3)]\n",
    "\n",
    "# Optimizers for Q-networks\n",
    "q_optimizers = [torch.optim.Adam(q_net.parameters(), lr=1e-3) for q_net in q_networks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fb7647",
   "metadata": {
    "papermill": {
     "duration": 0.042439,
     "end_time": "2025-01-18T03:47:12.172999",
     "exception": false,
     "start_time": "2025-01-18T03:47:12.130560",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- FQE training loop that iteratively updates Q-value so to minimize Bellman Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6dc1435c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T03:47:12.259094Z",
     "iopub.status.busy": "2025-01-18T03:47:12.258828Z",
     "iopub.status.idle": "2025-01-18T03:47:12.266354Z",
     "shell.execute_reply": "2025-01-18T03:47:12.265670Z"
    },
    "papermill": {
     "duration": 0.052353,
     "end_time": "2025-01-18T03:47:12.267619",
     "exception": false,
     "start_time": "2025-01-18T03:47:12.215266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_q_value_network_ensemble(q_networks, target_q_networks, behavior_policies, dataloader, optimizers, gamma=0.99, num_epochs=40, device=\"cuda\"):\n",
    "    # Move models to the correct device\n",
    "    for q_network in q_networks:\n",
    "        q_network.to(device)\n",
    "    for target_q_network in target_q_networks:\n",
    "        target_q_network.to(device)\n",
    "    for behavior_policy in behavior_policies:\n",
    "        behavior_policy.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_losses = [0.0 for _ in range(len(q_networks))]\n",
    "        for batch_idx, (state, action, next_state, reward) in enumerate(dataloader):\n",
    "            # Move data to the device\n",
    "            state = state.to(device)\n",
    "            action = action.to(device)\n",
    "            next_state = next_state.to(device)\n",
    "            reward = reward.to(device)\n",
    "\n",
    "            for i, (q_network, target_q_network, behavior_policy, optimizer) in enumerate(zip(q_networks, target_q_networks, behavior_policies, optimizers)):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Compute current Q-value\n",
    "                current_q_value = q_network(state, action)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Predict next action using the behavior policy\n",
    "                    _, next_action, _, _ = behavior_policy.predict_behavior(next_state)\n",
    "    \n",
    "                    # Compute target Q-value\n",
    "                    target_q_value = reward.unsqueeze(-1) + gamma * target_q_network(next_state, next_action)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = torch.nn.functional.mse_loss(current_q_value, target_q_value)\n",
    "\n",
    "                # Backpropagation\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_losses[i] += loss.item()\n",
    "\n",
    "        # Print losses\n",
    "        for i, loss in enumerate(epoch_losses):\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Q-Network {i + 1} Loss: {loss:.4f}\")\n",
    "\n",
    "\n",
    "    # Save Q-networks and optimizers at the end of training\n",
    "    for i, (q_network, optimizer) in enumerate(zip(q_networks, optimizers)):\n",
    "        q_net_output = os.path.join(Q_NETWORK_MODEL, f\"q_network_{i + 1}_final.pth\")\n",
    "        torch.save({\n",
    "            'model_state_dict': q_network.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, q_net_output)\n",
    "\n",
    "    print(f\"Q-networks saved successfully in directory: {Q_NETWORK_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d7d65",
   "metadata": {
    "papermill": {
     "duration": 0.042982,
     "end_time": "2025-01-18T03:47:12.353389",
     "exception": false,
     "start_time": "2025-01-18T03:47:12.310407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Compute the value function by sampling actions from the behavior policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a16869d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T03:47:12.439814Z",
     "iopub.status.busy": "2025-01-18T03:47:12.439572Z",
     "iopub.status.idle": "2025-01-18T03:47:12.443555Z",
     "shell.execute_reply": "2025-01-18T03:47:12.442942Z"
    },
    "papermill": {
     "duration": 0.048308,
     "end_time": "2025-01-18T03:47:12.444739",
     "exception": false,
     "start_time": "2025-01-18T03:47:12.396431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_value_function_ensemble(q_networks, state, behavior_policies, num_samples=10):\n",
    "    \"\"\"\n",
    "    Evaluates the value function V_b(s) using an ensemble of Q-networks and behavior policies.\n",
    "\n",
    "    Args:\n",
    "        q_networks: List of Q-value networks.\n",
    "        state: State tensor for evaluation.\n",
    "        behavior_policies: List of behavior policy models.\n",
    "        num_samples: Number of actions to sample from each behavior policy.\n",
    "\n",
    "    Returns:\n",
    "        The average value function V_b(s) over all ensembles.\n",
    "    \"\"\"\n",
    "    total_value = 0.0\n",
    "    for q_network, behavior_policy in zip(q_networks, behavior_policies):\n",
    "        for _ in range(num_samples):\n",
    "            _, action, _, _ = behavior_policy.predict_behavior(state)\n",
    "            total_value += q_network(state, action).item()\n",
    "\n",
    "    return total_value / (len(q_networks) * num_samples)  # Average over all models and samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161ea592",
   "metadata": {
    "papermill": {
     "duration": 0.042467,
     "end_time": "2025-01-18T03:47:12.529456",
     "exception": false,
     "start_time": "2025-01-18T03:47:12.486989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Max-Q operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4e5e765",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T03:47:12.615927Z",
     "iopub.status.busy": "2025-01-18T03:47:12.615668Z",
     "iopub.status.idle": "2025-01-18T03:47:12.619882Z",
     "shell.execute_reply": "2025-01-18T03:47:12.619271Z"
    },
    "papermill": {
     "duration": 0.049081,
     "end_time": "2025-01-18T03:47:12.621069",
     "exception": false,
     "start_time": "2025-01-18T03:47:12.571988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perform_max_q_operation(q_networks, behavior_policies, state, num_samples=10):\n",
    "    \"\"\"\n",
    "    Performs the max-Q operation using an ensemble of Q-networks and behavior policies.\n",
    "\n",
    "    Args:\n",
    "        q_networks: List of Q-value networks.\n",
    "        behavior_policies: List of behavior policy models.\n",
    "        state: State tensor for evaluation.\n",
    "        num_samples: Number of actions to sample from each behavior policy.\n",
    "\n",
    "    Returns:\n",
    "        Action with the maximum Q-value across the ensemble.\n",
    "    \"\"\"\n",
    "    max_q_value = float(\"-inf\")\n",
    "    best_action = None\n",
    "\n",
    "    for q_network, behavior_policy in zip(q_networks, behavior_policies):\n",
    "        for _ in range(num_samples):\n",
    "            _, action, _, _ = behavior_policy.predict_behavior(state)\n",
    "            q_value = q_network(state, action).item()\n",
    "\n",
    "            if q_value > max_q_value:\n",
    "                max_q_value = q_value\n",
    "                best_action = action\n",
    "\n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5064c314",
   "metadata": {
    "papermill": {
     "duration": 0.048959,
     "end_time": "2025-01-18T03:47:12.720202",
     "exception": false,
     "start_time": "2025-01-18T03:47:12.671243",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Define behavior policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4abf9df8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T03:47:12.807267Z",
     "iopub.status.busy": "2025-01-18T03:47:12.806909Z",
     "iopub.status.idle": "2025-01-18T03:47:12.810446Z",
     "shell.execute_reply": "2025-01-18T03:47:12.809616Z"
    },
    "papermill": {
     "duration": 0.048557,
     "end_time": "2025-01-18T03:47:12.811790",
     "exception": false,
     "start_time": "2025-01-18T03:47:12.763233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "behavior_policies = behavior_ensemble.models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b1dfeb",
   "metadata": {
    "papermill": {
     "duration": 0.042133,
     "end_time": "2025-01-18T03:47:12.898534",
     "exception": false,
     "start_time": "2025-01-18T03:47:12.856401",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Train QNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e3419c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T03:47:12.985062Z",
     "iopub.status.busy": "2025-01-18T03:47:12.984818Z",
     "iopub.status.idle": "2025-01-18T03:52:34.246778Z",
     "shell.execute_reply": "2025-01-18T03:52:34.245868Z"
    },
    "papermill": {
     "duration": 321.353941,
     "end_time": "2025-01-18T03:52:34.294752",
     "exception": false,
     "start_time": "2025-01-18T03:47:12.940811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Q-Network 1 Loss: 57.8629\n",
      "Epoch 1/40, Q-Network 2 Loss: 53.4419\n",
      "Epoch 1/40, Q-Network 3 Loss: 58.4776\n",
      "Epoch 2/40, Q-Network 1 Loss: 0.6331\n",
      "Epoch 2/40, Q-Network 2 Loss: 0.6026\n",
      "Epoch 2/40, Q-Network 3 Loss: 0.6593\n",
      "Epoch 3/40, Q-Network 1 Loss: 0.5167\n",
      "Epoch 3/40, Q-Network 2 Loss: 0.5225\n",
      "Epoch 3/40, Q-Network 3 Loss: 0.5170\n",
      "Epoch 4/40, Q-Network 1 Loss: 0.4762\n",
      "Epoch 4/40, Q-Network 2 Loss: 0.4802\n",
      "Epoch 4/40, Q-Network 3 Loss: 0.4411\n",
      "Epoch 5/40, Q-Network 1 Loss: 0.4567\n",
      "Epoch 5/40, Q-Network 2 Loss: 0.4776\n",
      "Epoch 5/40, Q-Network 3 Loss: 0.4171\n",
      "Epoch 6/40, Q-Network 1 Loss: 0.4450\n",
      "Epoch 6/40, Q-Network 2 Loss: 0.4580\n",
      "Epoch 6/40, Q-Network 3 Loss: 0.3952\n",
      "Epoch 7/40, Q-Network 1 Loss: 0.4626\n",
      "Epoch 7/40, Q-Network 2 Loss: 0.4745\n",
      "Epoch 7/40, Q-Network 3 Loss: 0.4122\n",
      "Epoch 8/40, Q-Network 1 Loss: 0.4706\n",
      "Epoch 8/40, Q-Network 2 Loss: 0.5439\n",
      "Epoch 8/40, Q-Network 3 Loss: 0.4701\n",
      "Epoch 9/40, Q-Network 1 Loss: 0.5293\n",
      "Epoch 9/40, Q-Network 2 Loss: 0.5142\n",
      "Epoch 9/40, Q-Network 3 Loss: 0.5212\n",
      "Epoch 10/40, Q-Network 1 Loss: 0.4643\n",
      "Epoch 10/40, Q-Network 2 Loss: 0.5039\n",
      "Epoch 10/40, Q-Network 3 Loss: 0.4319\n",
      "Epoch 11/40, Q-Network 1 Loss: 0.4959\n",
      "Epoch 11/40, Q-Network 2 Loss: 0.5437\n",
      "Epoch 11/40, Q-Network 3 Loss: 0.4491\n",
      "Epoch 12/40, Q-Network 1 Loss: 0.5363\n",
      "Epoch 12/40, Q-Network 2 Loss: 0.5706\n",
      "Epoch 12/40, Q-Network 3 Loss: 0.5005\n",
      "Epoch 13/40, Q-Network 1 Loss: 0.5090\n",
      "Epoch 13/40, Q-Network 2 Loss: 0.5202\n",
      "Epoch 13/40, Q-Network 3 Loss: 0.4111\n",
      "Epoch 14/40, Q-Network 1 Loss: 0.4237\n",
      "Epoch 14/40, Q-Network 2 Loss: 0.5011\n",
      "Epoch 14/40, Q-Network 3 Loss: 0.4721\n",
      "Epoch 15/40, Q-Network 1 Loss: 0.4319\n",
      "Epoch 15/40, Q-Network 2 Loss: 0.5504\n",
      "Epoch 15/40, Q-Network 3 Loss: 0.4566\n",
      "Epoch 16/40, Q-Network 1 Loss: 0.4571\n",
      "Epoch 16/40, Q-Network 2 Loss: 0.4376\n",
      "Epoch 16/40, Q-Network 3 Loss: 0.3971\n",
      "Epoch 17/40, Q-Network 1 Loss: 0.4122\n",
      "Epoch 17/40, Q-Network 2 Loss: 0.4605\n",
      "Epoch 17/40, Q-Network 3 Loss: 0.3970\n",
      "Epoch 18/40, Q-Network 1 Loss: 0.3977\n",
      "Epoch 18/40, Q-Network 2 Loss: 0.5166\n",
      "Epoch 18/40, Q-Network 3 Loss: 0.4045\n",
      "Epoch 19/40, Q-Network 1 Loss: 0.4159\n",
      "Epoch 19/40, Q-Network 2 Loss: 0.4594\n",
      "Epoch 19/40, Q-Network 3 Loss: 0.3962\n",
      "Epoch 20/40, Q-Network 1 Loss: 0.3939\n",
      "Epoch 20/40, Q-Network 2 Loss: 0.4758\n",
      "Epoch 20/40, Q-Network 3 Loss: 0.3767\n",
      "Epoch 21/40, Q-Network 1 Loss: 0.4784\n",
      "Epoch 21/40, Q-Network 2 Loss: 0.4257\n",
      "Epoch 21/40, Q-Network 3 Loss: 0.3467\n",
      "Epoch 22/40, Q-Network 1 Loss: 0.3532\n",
      "Epoch 22/40, Q-Network 2 Loss: 0.4382\n",
      "Epoch 22/40, Q-Network 3 Loss: 0.3748\n",
      "Epoch 23/40, Q-Network 1 Loss: 0.3520\n",
      "Epoch 23/40, Q-Network 2 Loss: 0.4683\n",
      "Epoch 23/40, Q-Network 3 Loss: 0.3379\n",
      "Epoch 24/40, Q-Network 1 Loss: 0.3689\n",
      "Epoch 24/40, Q-Network 2 Loss: 0.4075\n",
      "Epoch 24/40, Q-Network 3 Loss: 0.3420\n",
      "Epoch 25/40, Q-Network 1 Loss: 0.3578\n",
      "Epoch 25/40, Q-Network 2 Loss: 0.4185\n",
      "Epoch 25/40, Q-Network 3 Loss: 0.3622\n",
      "Epoch 26/40, Q-Network 1 Loss: 0.3532\n",
      "Epoch 26/40, Q-Network 2 Loss: 0.4105\n",
      "Epoch 26/40, Q-Network 3 Loss: 0.3145\n",
      "Epoch 27/40, Q-Network 1 Loss: 0.3304\n",
      "Epoch 27/40, Q-Network 2 Loss: 0.3919\n",
      "Epoch 27/40, Q-Network 3 Loss: 0.3145\n",
      "Epoch 28/40, Q-Network 1 Loss: 0.3294\n",
      "Epoch 28/40, Q-Network 2 Loss: 0.3888\n",
      "Epoch 28/40, Q-Network 3 Loss: 0.3390\n",
      "Epoch 29/40, Q-Network 1 Loss: 0.3389\n",
      "Epoch 29/40, Q-Network 2 Loss: 0.3913\n",
      "Epoch 29/40, Q-Network 3 Loss: 0.3009\n",
      "Epoch 30/40, Q-Network 1 Loss: 0.3439\n",
      "Epoch 30/40, Q-Network 2 Loss: 0.4248\n",
      "Epoch 30/40, Q-Network 3 Loss: 0.2994\n",
      "Epoch 31/40, Q-Network 1 Loss: 0.3471\n",
      "Epoch 31/40, Q-Network 2 Loss: 0.3758\n",
      "Epoch 31/40, Q-Network 3 Loss: 0.3054\n",
      "Epoch 32/40, Q-Network 1 Loss: 0.3017\n",
      "Epoch 32/40, Q-Network 2 Loss: 0.4048\n",
      "Epoch 32/40, Q-Network 3 Loss: 0.3263\n",
      "Epoch 33/40, Q-Network 1 Loss: 0.3374\n",
      "Epoch 33/40, Q-Network 2 Loss: 0.3581\n",
      "Epoch 33/40, Q-Network 3 Loss: 0.2856\n",
      "Epoch 34/40, Q-Network 1 Loss: 0.3320\n",
      "Epoch 34/40, Q-Network 2 Loss: 0.3924\n",
      "Epoch 34/40, Q-Network 3 Loss: 0.3042\n",
      "Epoch 35/40, Q-Network 1 Loss: 0.2922\n",
      "Epoch 35/40, Q-Network 2 Loss: 0.3661\n",
      "Epoch 35/40, Q-Network 3 Loss: 0.2657\n",
      "Epoch 36/40, Q-Network 1 Loss: 0.3030\n",
      "Epoch 36/40, Q-Network 2 Loss: 0.3946\n",
      "Epoch 36/40, Q-Network 3 Loss: 0.3121\n",
      "Epoch 37/40, Q-Network 1 Loss: 0.3174\n",
      "Epoch 37/40, Q-Network 2 Loss: 0.3947\n",
      "Epoch 37/40, Q-Network 3 Loss: 0.2689\n",
      "Epoch 38/40, Q-Network 1 Loss: 0.2850\n",
      "Epoch 38/40, Q-Network 2 Loss: 0.3504\n",
      "Epoch 38/40, Q-Network 3 Loss: 0.2582\n",
      "Epoch 39/40, Q-Network 1 Loss: 0.3123\n",
      "Epoch 39/40, Q-Network 2 Loss: 0.3382\n",
      "Epoch 39/40, Q-Network 3 Loss: 0.2798\n",
      "Epoch 40/40, Q-Network 1 Loss: 0.3127\n",
      "Epoch 40/40, Q-Network 2 Loss: 0.3522\n",
      "Epoch 40/40, Q-Network 3 Loss: 0.2709\n",
      "Q-networks saved successfully in directory: /kaggle/working/q_network\n"
     ]
    }
   ],
   "source": [
    "train_q_value_network_ensemble(\n",
    "    q_networks=q_networks,\n",
    "    target_q_networks=target_q_networks,\n",
    "    behavior_policies=behavior_policies,  # List of behavior policy models\n",
    "    dataloader=dataloader_dynamics,\n",
    "    optimizers=q_optimizers,\n",
    "    gamma=0.99,\n",
    "    num_epochs=40\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a34e34",
   "metadata": {
    "papermill": {
     "duration": 0.044112,
     "end_time": "2025-01-18T03:52:34.384039",
     "exception": false,
     "start_time": "2025-01-18T03:52:34.339927",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.1 MOPP CLASS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13377400",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T03:52:34.474652Z",
     "iopub.status.busy": "2025-01-18T03:52:34.474342Z",
     "iopub.status.idle": "2025-01-18T03:52:34.495085Z",
     "shell.execute_reply": "2025-01-18T03:52:34.494440Z"
    },
    "papermill": {
     "duration": 0.067825,
     "end_time": "2025-01-18T03:52:34.496376",
     "exception": false,
     "start_time": "2025-01-18T03:52:34.428551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MOPP:\n",
    "    def __init__(self, env_dataset, q_networks, behavior_ensemble, dynamics_ensemble,\n",
    "                 horizon, gamma, beta, kappa, Nm, L, num_trajectories, device, K_Q=10, batch_size=256):\n",
    "        self.env_dataset = env_dataset\n",
    "        self.q_networks = q_networks\n",
    "        self.behavior_ensemble = behavior_ensemble\n",
    "        self.dynamics_ensemble = dynamics_ensemble\n",
    "        self.horizon = horizon\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.kappa = kappa\n",
    "        self.Nm = Nm\n",
    "        self.L = L\n",
    "        self.num_trajectories = num_trajectories\n",
    "        self.device = device\n",
    "        self.K_Q = K_Q  # Number of actions sampled for V_b(s_H)\n",
    "        self.batch_size = batch_size\n",
    "        self.optimized_actions = torch.zeros(self.horizon, behavior_ensemble.models[0].output_dim, device=device)\n",
    "\n",
    "    def compute_uncertainty_matrix(self, trajectories):\n",
    "        \"\"\"\n",
    "        Compute uncertainty matrix U for the state-action pairs in the trajectories.\n",
    "        \"\"\"\n",
    "        U = []\n",
    "        for trajectory in trajectories:\n",
    "            uncertainty = []\n",
    "            for (s, a, _, _) in trajectory:\n",
    "                # Compute the discrepancy (or uncertainty) between dynamics model predictions\n",
    "                predictions = []\n",
    "                for model in self.dynamics_ensemble.models:\n",
    "                    _, dynamics_output, _, _ = model.predict_dynamics(s, a)\n",
    "                    predictions.append(dynamics_output)\n",
    "                predictions = torch.stack(predictions)\n",
    "                disc = torch.max(torch.norm(predictions - predictions.mean(dim=0), dim=-1))\n",
    "                uncertainty.append(disc)\n",
    "            U.append(uncertainty)\n",
    "        return torch.tensor(U)\n",
    "\n",
    "    def filter_trajectories(self, trajectories, uncertainty_matrix):\n",
    "        \"\"\"\n",
    "        Filter trajectories based on the uncertainty matrix.\n",
    "        \"\"\"\n",
    "        filtered_trajectories = []\n",
    "        for idx, trajectory in enumerate(trajectories):\n",
    "            uncertainties = uncertainty_matrix[idx]\n",
    "            if torch.max(uncertainties) < self.L:\n",
    "                filtered_trajectories.append(trajectory)\n",
    "        \n",
    "        # If not enough trajectories, select the ones with the lowest uncertainty\n",
    "        if len(filtered_trajectories) < self.Nm:\n",
    "            uncertainties_sum = [torch.sum(uncertainties) for uncertainties in uncertainty_matrix]\n",
    "            sorted_idx = torch.argsort(torch.tensor(uncertainties_sum))\n",
    "            for idx in sorted_idx[:self.Nm - len(filtered_trajectories)]:\n",
    "                filtered_trajectories.append(trajectories[idx])\n",
    "\n",
    "        return filtered_trajectories\n",
    "\n",
    "    def compute_cumulative_return(self, trajectory, value_b_func):\n",
    "        \"\"\"\n",
    "        Compute the cumulative return for a trajectory using the value function.\n",
    "        \"\"\"\n",
    "        R_n = 0\n",
    "        for t in range(self.horizon):\n",
    "            R_n += self.gamma ** t * trajectory[t][3]  # Add reward at timestep t\n",
    "        final_state = trajectory[-1][2]  # Terminal state\n",
    "        R_n += self.gamma ** self.horizon * value_b_func(final_state)  # Add discounted terminal value\n",
    "        return R_n\n",
    "\n",
    "    def optimize_actions(self, pruned_trajectories, returns):\n",
    "        action_dim = pruned_trajectories[0][0][1].shape[-1]\n",
    "        weighted_actions = torch.zeros(self.horizon, action_dim).to(self.device)\n",
    "\n",
    "        for t in range(self.horizon):\n",
    "            weighted_sum = torch.zeros(action_dim).to(self.device)\n",
    "            weight_sum = 0.0\n",
    "\n",
    "            for trajectory, return_value in zip(pruned_trajectories, returns):\n",
    "                return_tensor = torch.tensor(return_value, dtype=torch.float32).to(self.device)\n",
    "\n",
    "                weight = torch.exp(torch.clamp(self.kappa * return_tensor, max=20))  # Prevent overflow in exp\n",
    "                weight = weight.repeat(trajectory[t][1].shape[0], 1)\n",
    "\n",
    "                weighted_sum += (weight * trajectory[t][1]).sum(dim=0)\n",
    "                weight_sum += weight.sum()\n",
    "\n",
    "            weighted_actions[t] = weighted_sum / (weight_sum + 1e-8)\n",
    "\n",
    "        return weighted_actions\n",
    "\n",
    "\n",
    "    def compute_value_b(self, q_networks, final_state, num_samples):\n",
    "        \"\"\"\n",
    "        Compute the value function V_b(s_H) by averaging the Q-values for random action samples.\n",
    "        \n",
    "        Args:\n",
    "            q_networks: List of Q-value networks.\n",
    "            final_state: The terminal state.\n",
    "            num_samples: Number of actions to sample for Q-value computation.\n",
    "            \n",
    "        Returns:\n",
    "            The average Q-value for the terminal state.\n",
    "        \"\"\"\n",
    "        # Sample random actions using behavior policy for the final state\n",
    "        action_samples = [self.behavior_ensemble.predict_behavior(final_state)[0] for _ in range(num_samples)]\n",
    "        \n",
    "        q_values = []\n",
    "        for action in action_samples:\n",
    "            # Ensure that action is a tensor and move to the device\n",
    "            if isinstance(action, tuple):\n",
    "                action = action[0]  # Unpack if it's a tuple\n",
    "            action = action.unsqueeze(0)  # Ensure action has shape [1, action_dim]\n",
    "            \n",
    "            # Flatten the action tensor to make sure it matches the state dimension\n",
    "            action = action.view(-1, action.size(-1))  # Shape: [K_Q, action_dim]\n",
    "    \n",
    "            # Ensure final_state is also reshaped correctly\n",
    "            final_state = final_state.squeeze(0)  # Remove the extra batch dimension, shape: [state_dim]\n",
    "            final_state = final_state.unsqueeze(0).expand(action.size(0), -1)  # Shape: [K_Q, state_dim]\n",
    "            \n",
    "            # Compute Q-value using the q_networks\n",
    "            q_value = q_networks[0](final_state, action)  # Assuming q_networks is a list of Q-networks\n",
    "            q_values.append(q_value)\n",
    "        \n",
    "        # Average the Q-values\n",
    "        q_values = torch.stack(q_values)\n",
    "        value_b = q_values.mean()\n",
    "        \n",
    "        return value_b\n",
    "\n",
    "    def train(self, state):\n",
    "        dim_1 = self.horizon + 1\n",
    "        # Step 1: Initialization and train models (already done)\n",
    "        A_star = torch.zeros(dim_1, self.behavior_ensemble.models[0].output_dim, device=self.device)  # shape: [3,6]\n",
    "    \n",
    "        # Step 2: Begin infinite loop\n",
    "        for tau in range(1):\n",
    "            # Step 3: Observe and initialize\n",
    "            s_tau = state  # Use the provided state, shape: [1,17]\n",
    "            s_tau = s_tau.to(self.device)  # Ensure the state is on the correct device\n",
    "            R = []  # list to store rewards\n",
    "            T = []  # Initialize trajectory\n",
    "    \n",
    "            # Step 4: Iterate over trajectories\n",
    "            for n in range(self.num_trajectories):\n",
    "                # Step 5: Initialize quantities\n",
    "                s_0 = s_tau  # initial state for trajectory\n",
    "                R_n = 0\n",
    "                T_n = []\n",
    "\n",
    "                # Step 6: Iterate over timesteps in the trajectories\n",
    "                for t in range(self.horizon):\n",
    "                    # Step 7: Sample action a_t using behavior policy\n",
    "                    model_index_beh = torch.randint(0, len(self.behavior_ensemble.models), (1,)).item()\n",
    "                    behavior_model = self.behavior_ensemble.models[model_index_beh].to(self.device)\n",
    "                    mean_actions, _, _, _ = behavior_model.predict_behavior(s_tau)\n",
    "    \n",
    "                    sigma_a = torch.abs(mean_actions)  # Get standard deviation (uncertainty)\n",
    "                    sigma_a = torch.clamp(sigma_a, min=1e-8)  # Avoid zero or negative values\n",
    "    \n",
    "                    # Create a Normal distribution based on mean and std\n",
    "                    distr = dist.Normal(mean_actions, sigma_a)\n",
    "                    action_samples = distr.sample((self.K_Q,))  # Sample K_Q actions, shape: [K_Q, 1, action_dim]\n",
    "                    \n",
    "                    #print(\"action_samples distr\", action_samples.shape)\n",
    "                    action_samples = action_samples.squeeze(1)  # Shape: [K_Q, action_dim]\n",
    "                    #print(\"action samples squeeze\", action_samples.shape)\n",
    "                    \n",
    "                    state_expanded = s_tau.unsqueeze(0).expand(self.K_Q, -1, -1)  # Shape: [K_Q, batch_size, state_dim]\n",
    "                    #print(\"state_expanded expand\", state_expanded.shape)\n",
    "                    state_expanded = state_expanded.contiguous().view(-1, state_expanded.size(-1))  # Shape: [K_Q * batch_size, state_dim]\n",
    "                    #print(\"state_expanded view\", state_expanded.shape)\n",
    "                     \n",
    "                    # Step 8: Compute Q-values for all sampled actions\n",
    "                    q_values = []\n",
    "                    for q_network in self.q_networks:\n",
    "                        # Pass state and action separately to the Q-network\n",
    "                        q_value = q_network(state_expanded, action_samples)  # Compute Q-value\n",
    "                        q_values.append(q_value.view(self.K_Q, -1))  # Reshape back to [K_Q, batch_size]\n",
    "                    \n",
    "                    q_values = torch.stack(q_values, dim=0)  # Shape: [num_q_networks, K_Q, batch_size]\n",
    "                    #print(\"q_values\", q_values.shape)\n",
    "    \n",
    "                    # Compute the mean Q-value across networks and find the best action\n",
    "                    mean_q_values = q_values.mean(dim=0)  # Average over the Q-networks, shape: [K_Q, batch_size]\n",
    "                    _, best_action_indices = torch.max(mean_q_values, dim=0)  # Shape: [batch_size]\n",
    "                    #print(\"best_action_indices\", best_action_indices)\n",
    "                    action_t_hat = action_samples[best_action_indices, range(best_action_indices.size(0))]  # Optimal action\n",
    "                    \n",
    "                    # Compute action_t_tilde\n",
    "                    A_star_t_plus_1 = A_star[t + 1].unsqueeze(0).expand(action_t_hat.size(0), -1)  # Shape: [batch_size, action_dim]\n",
    "                    action_t_tilde = (1 - self.beta) * action_t_hat + self.beta * A_star_t_plus_1\n",
    "    \n",
    "                    # Step 9: Append (s_t, a_t) to trajectory\n",
    "                    T_n.append((s_tau, action_t_tilde))\n",
    "    \n",
    "                    # Step 10: Predict next state using dynamics model\n",
    "                    model_index_dyn = torch.randint(0, len(self.dynamics_ensemble.models), (1,)).item()\n",
    "                    dynamics_model = self.dynamics_ensemble.models[model_index_dyn].to(self.device)\n",
    "                    _, dynamics_output, _, _ = dynamics_model.predict_dynamics(s_0, action_t_tilde)\n",
    "                    next_state = dynamics_output[:, :s_0.size(-1)]\n",
    "                    reward = dynamics_output[:, s_0.size(-1):]\n",
    "    \n",
    "                    # Update trajectory and state\n",
    "                    T_n[-1] = (s_0.clone(), action_t_tilde.clone(), next_state.clone(), reward.clone())\n",
    "                    s_0 = next_state.clone()\n",
    "    \n",
    "                    # Compute the reward using all dynamics models\n",
    "                    rewards = []\n",
    "                    for dynamics_model in self.dynamics_ensemble.models:\n",
    "                        dynamics_model = dynamics_model.to(self.device)\n",
    "                        _, dynamics_output, _, _ = dynamics_model.predict_dynamics(s_0, action_t_tilde)\n",
    "                        rewards.append(dynamics_output[:, s_0.size(-1):])\n",
    "\n",
    "                    # Average the rewards across all dynamics models\n",
    "                    mean_reward = torch.stack(rewards).mean(dim=0)\n",
    "                    R_n += mean_reward.mean().item()\n",
    "    \n",
    "                # Evaluate terminal value Vb(sH)\n",
    "                final_state = T_n[-1][2]  # s_H\n",
    "                value_b = self.compute_value_b(self.q_networks, final_state, num_samples=self.K_Q)\n",
    "    \n",
    "                # Store trajectory and rewards\n",
    "                T.append(T_n)\n",
    "                R_n += value_b.item()\n",
    "                R.append(R_n)\n",
    "    \n",
    "            # Compute uncertainty matrix\n",
    "            uncertainty_matrix = self.compute_uncertainty_matrix(T)\n",
    "    \n",
    "            # Prune trajectories based on uncertainty\n",
    "            pruned_trajectories = self.filter_trajectories(T, uncertainty_matrix)\n",
    "    \n",
    "            # Optimize action sequence\n",
    "            self.optimized_actions = self.optimize_actions(pruned_trajectories, R)\n",
    "    \n",
    "        # Return optimized action for the initial state\n",
    "        return self.optimized_actions[0]\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a667a7a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T03:52:34.586131Z",
     "iopub.status.busy": "2025-01-18T03:52:34.585896Z",
     "iopub.status.idle": "2025-01-18T03:52:34.589629Z",
     "shell.execute_reply": "2025-01-18T03:52:34.588835Z"
    },
    "papermill": {
     "duration": 0.0498,
     "end_time": "2025-01-18T03:52:34.590793",
     "exception": false,
     "start_time": "2025-01-18T03:52:34.540993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mopp_policy = MOPP(\n",
    "    env_dataset=env_dataset,\n",
    "    q_networks=q_networks,  # Unused in this implementation\n",
    "    behavior_ensemble=behavior_ensemble,\n",
    "    dynamics_ensemble=dynamics_ensemble,\n",
    "    horizon=3,\n",
    "    gamma=0.99,\n",
    "    beta=0,\n",
    "    kappa=0.3,\n",
    "    Nm=20,\n",
    "    L=1,\n",
    "    num_trajectories = 100,\n",
    "    device=device,\n",
    "    batch_size=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "56fffd5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T03:52:34.681307Z",
     "iopub.status.busy": "2025-01-18T03:52:34.681029Z",
     "iopub.status.idle": "2025-01-18T03:52:34.686338Z",
     "shell.execute_reply": "2025-01-18T03:52:34.685665Z"
    },
    "papermill": {
     "duration": 0.051495,
     "end_time": "2025-01-18T03:52:34.687432",
     "exception": false,
     "start_time": "2025-01-18T03:52:34.635937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_policy_episodes_new(env, policy, n_episodes, device):\n",
    "    results = []\n",
    "\n",
    "    for i in range(n_episodes):\n",
    "        print(f\"Episode {i + 1}\")\n",
    "\n",
    "        # env.reset()\n",
    "        # Start with the initial state from the dataset\n",
    "        state = env.observations[i].to(device).unsqueeze(0)  # Ensure correct shape\n",
    "        #print(\"Initial state:\", state.shape)\n",
    "        cumulative_reward = 0.0\n",
    "\n",
    "        # Get precomputed actions for this episode\n",
    "        #optimal_actions = precomputed_actions[i]\n",
    "        #print(f\"Optimal actions shape: {optimal_actions.shape}\")\n",
    "        optimal_actions = policy.train(state)\n",
    "        #print(\"optimal_actions\", optimal_actions)\n",
    "\n",
    "        # Horizon-based loop\n",
    "        for t in range(policy.horizon):\n",
    "            # Use the precomputed action\n",
    "            action = optimal_actions[t].detach().cpu().numpy()\n",
    "            #print(f\"Step {t + 1}, Action shape: {action.shape}\")\n",
    "\n",
    "            # Step the environment\n",
    "            next_state, reward, done = env.step(action)\n",
    "            cumulative_reward += reward\n",
    "\n",
    "            # Convert next_state to a tensor and update state\n",
    "            state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode terminated early at step {t + 1}\")\n",
    "                break\n",
    "\n",
    "        results.append(cumulative_reward)\n",
    "        print(f\"Completed Episode {i + 1}! Total Reward: {cumulative_reward:.2f}\")\n",
    "\n",
    "    results = np.array(results)\n",
    "    return float(np.mean(results)), float(np.std(results)), results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b1f0df53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T03:52:34.777935Z",
     "iopub.status.busy": "2025-01-18T03:52:34.777702Z",
     "iopub.status.idle": "2025-01-18T04:58:45.508899Z",
     "shell.execute_reply": "2025-01-18T04:58:45.507948Z"
    },
    "papermill": {
     "duration": 3970.907236,
     "end_time": "2025-01-18T04:58:45.639118",
     "exception": false,
     "start_time": "2025-01-18T03:52:34.731882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-41-0221f3793b3a>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Episode 1! Total Reward: 2.63\n",
      "Episode 2\n",
      "Completed Episode 2! Total Reward: 2.26\n",
      "Episode 3\n",
      "Completed Episode 3! Total Reward: 3.06\n",
      "Episode 4\n",
      "Completed Episode 4! Total Reward: 3.93\n",
      "Episode 5\n",
      "Completed Episode 5! Total Reward: 2.90\n",
      "Episode 6\n",
      "Completed Episode 6! Total Reward: 2.59\n",
      "Episode 7\n",
      "Completed Episode 7! Total Reward: 2.79\n",
      "Episode 8\n",
      "Completed Episode 8! Total Reward: 3.37\n",
      "Episode 9\n",
      "Completed Episode 9! Total Reward: 3.73\n",
      "Episode 10\n",
      "Completed Episode 10! Total Reward: 3.80\n",
      "Episode 11\n",
      "Completed Episode 11! Total Reward: 4.06\n",
      "Episode 12\n",
      "Completed Episode 12! Total Reward: 4.35\n",
      "Episode 13\n",
      "Completed Episode 13! Total Reward: 4.75\n",
      "Episode 14\n",
      "Completed Episode 14! Total Reward: 5.01\n",
      "Episode 15\n",
      "Completed Episode 15! Total Reward: 5.23\n",
      "Episode 16\n",
      "Completed Episode 16! Total Reward: 5.42\n",
      "Episode 17\n",
      "Completed Episode 17! Total Reward: 5.42\n",
      "Episode 18\n",
      "Completed Episode 18! Total Reward: 5.50\n",
      "Episode 19\n",
      "Completed Episode 19! Total Reward: 5.75\n",
      "Episode 20\n",
      "Completed Episode 20! Total Reward: 5.92\n",
      "Episode 21\n",
      "Completed Episode 21! Total Reward: 6.33\n",
      "Episode 22\n",
      "Completed Episode 22! Total Reward: 6.55\n",
      "Episode 23\n",
      "Completed Episode 23! Total Reward: 6.88\n",
      "Episode 24\n",
      "Completed Episode 24! Total Reward: 7.44\n",
      "Episode 25\n",
      "Completed Episode 25! Total Reward: 7.46\n",
      "Episode 26\n",
      "Completed Episode 26! Total Reward: 7.53\n",
      "Episode 27\n",
      "Completed Episode 27! Total Reward: 7.77\n",
      "Episode 28\n",
      "Completed Episode 28! Total Reward: 7.65\n",
      "Episode 29\n",
      "Completed Episode 29! Total Reward: 7.73\n",
      "Episode 30\n",
      "Completed Episode 30! Total Reward: 8.07\n",
      "Episode 31\n",
      "Completed Episode 31! Total Reward: 7.78\n",
      "Episode 32\n",
      "Completed Episode 32! Total Reward: 7.38\n",
      "Episode 33\n",
      "Completed Episode 33! Total Reward: 7.23\n",
      "Episode 34\n",
      "Completed Episode 34! Total Reward: 6.87\n",
      "Episode 35\n",
      "Completed Episode 35! Total Reward: 6.75\n",
      "Episode 36\n",
      "Completed Episode 36! Total Reward: 6.64\n",
      "Episode 37\n",
      "Completed Episode 37! Total Reward: 7.03\n",
      "Episode 38\n",
      "Completed Episode 38! Total Reward: 7.36\n",
      "Episode 39\n",
      "Completed Episode 39! Total Reward: 7.35\n",
      "Episode 40\n",
      "Completed Episode 40! Total Reward: 7.12\n",
      "Episode 41\n",
      "Completed Episode 41! Total Reward: 7.20\n",
      "Episode 42\n",
      "Completed Episode 42! Total Reward: 7.38\n",
      "Episode 43\n",
      "Completed Episode 43! Total Reward: 7.48\n",
      "Episode 44\n",
      "Completed Episode 44! Total Reward: 7.52\n",
      "Episode 45\n",
      "Completed Episode 45! Total Reward: 7.45\n",
      "Episode 46\n",
      "Completed Episode 46! Total Reward: 7.23\n",
      "Episode 47\n",
      "Completed Episode 47! Total Reward: 7.01\n",
      "Episode 48\n",
      "Completed Episode 48! Total Reward: 7.04\n",
      "Episode 49\n",
      "Completed Episode 49! Total Reward: 7.21\n",
      "Episode 50\n",
      "Completed Episode 50! Total Reward: 7.03\n",
      "Episode 51\n",
      "Completed Episode 51! Total Reward: 6.78\n",
      "Episode 52\n",
      "Completed Episode 52! Total Reward: 6.83\n",
      "Episode 53\n",
      "Completed Episode 53! Total Reward: 8.29\n",
      "Episode 54\n",
      "Completed Episode 54! Total Reward: 9.97\n",
      "Episode 55\n",
      "Completed Episode 55! Total Reward: 10.53\n",
      "Episode 56\n",
      "Completed Episode 56! Total Reward: 10.55\n",
      "Episode 57\n",
      "Completed Episode 57! Total Reward: 10.22\n",
      "Episode 58\n",
      "Completed Episode 58! Total Reward: 9.93\n",
      "Episode 59\n",
      "Completed Episode 59! Total Reward: 9.90\n",
      "Episode 60\n",
      "Completed Episode 60! Total Reward: 9.41\n",
      "Episode 61\n",
      "Completed Episode 61! Total Reward: 9.03\n",
      "Episode 62\n",
      "Completed Episode 62! Total Reward: 8.52\n",
      "Episode 63\n",
      "Completed Episode 63! Total Reward: 8.02\n",
      "Episode 64\n",
      "Completed Episode 64! Total Reward: 8.28\n",
      "Episode 65\n",
      "Completed Episode 65! Total Reward: 8.72\n",
      "Episode 66\n",
      "Completed Episode 66! Total Reward: 8.89\n",
      "Episode 67\n",
      "Completed Episode 67! Total Reward: 8.96\n",
      "Episode 68\n",
      "Completed Episode 68! Total Reward: 9.04\n",
      "Episode 69\n",
      "Completed Episode 69! Total Reward: 8.98\n",
      "Episode 70\n",
      "Completed Episode 70! Total Reward: 9.03\n",
      "Episode 71\n",
      "Completed Episode 71! Total Reward: 9.19\n",
      "Episode 72\n",
      "Completed Episode 72! Total Reward: 9.20\n",
      "Episode 73\n",
      "Completed Episode 73! Total Reward: 9.18\n",
      "Episode 74\n",
      "Completed Episode 74! Total Reward: 9.07\n",
      "Episode 75\n",
      "Completed Episode 75! Total Reward: 8.51\n",
      "Episode 76\n",
      "Completed Episode 76! Total Reward: 8.55\n",
      "Episode 77\n",
      "Completed Episode 77! Total Reward: 8.52\n",
      "Episode 78\n",
      "Completed Episode 78! Total Reward: 8.57\n",
      "Episode 79\n",
      "Completed Episode 79! Total Reward: 8.63\n",
      "Episode 80\n",
      "Completed Episode 80! Total Reward: 8.67\n",
      "Episode 81\n",
      "Completed Episode 81! Total Reward: 8.79\n",
      "Episode 82\n",
      "Completed Episode 82! Total Reward: 10.94\n",
      "Episode 83\n",
      "Completed Episode 83! Total Reward: 11.99\n",
      "Episode 84\n",
      "Completed Episode 84! Total Reward: 11.64\n",
      "Episode 85\n",
      "Completed Episode 85! Total Reward: 11.31\n",
      "Episode 86\n",
      "Completed Episode 86! Total Reward: 11.46\n",
      "Episode 87\n",
      "Completed Episode 87! Total Reward: 10.93\n",
      "Episode 88\n",
      "Completed Episode 88! Total Reward: 10.51\n",
      "Episode 89\n",
      "Completed Episode 89! Total Reward: 9.77\n",
      "Episode 90\n",
      "Completed Episode 90! Total Reward: 9.42\n",
      "Episode 91\n",
      "Completed Episode 91! Total Reward: 9.75\n",
      "Episode 92\n",
      "Completed Episode 92! Total Reward: 9.99\n",
      "Episode 93\n",
      "Completed Episode 93! Total Reward: 10.23\n",
      "Episode 94\n",
      "Completed Episode 94! Total Reward: 10.37\n",
      "Episode 95\n",
      "Completed Episode 95! Total Reward: 10.42\n",
      "Episode 96\n",
      "Completed Episode 96! Total Reward: 10.16\n",
      "Episode 97\n",
      "Completed Episode 97! Total Reward: 10.24\n",
      "Episode 98\n",
      "Completed Episode 98! Total Reward: 10.38\n",
      "Episode 99\n",
      "Completed Episode 99! Total Reward: 10.45\n",
      "Episode 100\n",
      "Completed Episode 100! Total Reward: 10.49\n",
      "Mean Reward: 7.771778106689453, Std Reward: 2.2894442081451416\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the policy\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mean_reward, std_reward, rewards = eval_policy_episodes_new(\n",
    "    env=env_dataset, \n",
    "    policy=mopp_policy, \n",
    "    n_episodes=100, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Mean Reward: {mean_reward}, Std Reward: {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "72288bfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T04:58:45.743062Z",
     "iopub.status.busy": "2025-01-18T04:58:45.742743Z",
     "iopub.status.idle": "2025-01-18T04:58:45.747919Z",
     "shell.execute_reply": "2025-01-18T04:58:45.747098Z"
    },
    "papermill": {
     "duration": 0.058213,
     "end_time": "2025-01-18T04:58:45.749095",
     "exception": false,
     "start_time": "2025-01-18T04:58:45.690882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_results(results, save_dir, save_format=\"json\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "    #timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"results.{save_format}\"\n",
    "    file_path = os.path.join(save_dir, filename)\n",
    "\n",
    "    if save_format == \"json\":\n",
    "        # Convert numpy array to list for JSON serialization\n",
    "        results_data = {\n",
    "            \"mean_reward\": float(np.mean(results)),\n",
    "            \"std_reward\": float(np.std(results)),\n",
    "            \"rewards\": results.tolist(),\n",
    "        }\n",
    "        with open(file_path, \"w\") as f:\n",
    "            json.dump(results_data, f, indent=4)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid save format. Use 'json' or 'pickle'.\")\n",
    "\n",
    "    print(f\"Results saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "632efbc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T04:58:45.852419Z",
     "iopub.status.busy": "2025-01-18T04:58:45.852054Z",
     "iopub.status.idle": "2025-01-18T04:58:45.857441Z",
     "shell.execute_reply": "2025-01-18T04:58:45.856660Z"
    },
    "papermill": {
     "duration": 0.05836,
     "end_time": "2025-01-18T04:58:45.858749",
     "exception": false,
     "start_time": "2025-01-18T04:58:45.800389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /kaggle/working/results/results.json\n"
     ]
    }
   ],
   "source": [
    "save_results(rewards, RESULTS, save_format=\"json\")  # To save in JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f0a7915",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T04:58:45.962907Z",
     "iopub.status.busy": "2025-01-18T04:58:45.962624Z",
     "iopub.status.idle": "2025-01-18T04:58:45.966857Z",
     "shell.execute_reply": "2025-01-18T04:58:45.966202Z"
    },
    "papermill": {
     "duration": 0.058417,
     "end_time": "2025-01-18T04:58:45.968198",
     "exception": false,
     "start_time": "2025-01-18T04:58:45.909781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/working/results/results.json\", \"r\") as f:\n",
    "    results_data = json.load(f)\n",
    "\n",
    "mean_reward_results = results_data[\"mean_reward\"]\n",
    "std_reward_results = results_data[\"std_reward\"]\n",
    "rewards_results = results_data[\"rewards\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad8d75f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T04:58:46.074788Z",
     "iopub.status.busy": "2025-01-18T04:58:46.074505Z",
     "iopub.status.idle": "2025-01-18T04:58:46.353676Z",
     "shell.execute_reply": "2025-01-18T04:58:46.352795Z"
    },
    "papermill": {
     "duration": 0.33298,
     "end_time": "2025-01-18T04:58:46.354964",
     "exception": false,
     "start_time": "2025-01-18T04:58:46.021984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot individual episode rewards\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rewards_results, marker='o', linestyle='-', label=\"Episode Rewards\")\n",
    "plt.axhline(mean_reward_results, color='red', linestyle='--', label=f\"Mean Reward: {mean_reward_results:.2f}\")\n",
    "plt.fill_between(range(len(rewards_results)), mean_reward_results - std_reward_results, mean_reward_results + std_reward_results, \n",
    "                 color='red', alpha=0.2, label=f\"±1 STD ({std_reward_results:.2f})\")\n",
    "\n",
    "plt.title(\"Episode Rewards\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abc84a4",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e7c26",
   "metadata": {},
   "source": [
    "In order to test the model with various datasets, you need to modify the environment and the MOPP calling function. Here we provide the instructions in order to test the model with Half Cheetah and Walker2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cf693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "# Walker2D random \n",
    "env_dataset = Environment(env_name=\"walker2d-random-v2\", normalization_states=False, normalization_rewards=False)\n",
    "mopp_policy = MOPP(\n",
    "    env_dataset=env_dataset,\n",
    "    q_networks=q_networks,  # Unused in this implementation\n",
    "    behavior_ensemble=behavior_ensemble,\n",
    "    dynamics_ensemble=dynamics_ensemble,\n",
    "    horizon=3,\n",
    "    gamma=0.99,\n",
    "    beta=0,\n",
    "    kappa=0.3,\n",
    "    Nm=20,\n",
    "    L=8,\n",
    "    num_trajectories = 100,\n",
    "    device=device,\n",
    "    batch_size=256\n",
    ")\n",
    "'''\n",
    "\n",
    "''' \n",
    "# Half Cheetah random \n",
    "env_dataset = Environment(env_name=\"halfcheetah-random-v2\", normalization_states=False, normalization_rewards=False)\n",
    "mopp_policy = MOPP(\n",
    "    env_dataset=env_dataset,\n",
    "    q_networks=q_networks,  # Unused in this implementation\n",
    "    behavior_ensemble=behavior_ensemble,\n",
    "    dynamics_ensemble=dynamics_ensemble,\n",
    "    horizon=3,\n",
    "    gamma=0.99,\n",
    "    beta=0,\n",
    "    kappa=3,\n",
    "    Nm=20,\n",
    "    L=4,\n",
    "    num_trajectories = 100,\n",
    "    device=device,\n",
    "    batch_size=256\n",
    ")\n",
    "'''\n",
    "\n",
    "''' \n",
    "# Hopper random \n",
    "env_dataset = Environment(env_name=\"hopper-random-v2\", normalization_states=False, normalization_rewards=False)\n",
    "mopp_policy = MOPP(\n",
    "    env_dataset=env_dataset,\n",
    "    q_networks=q_networks,  # Unused in this implementation\n",
    "    behavior_ensemble=behavior_ensemble,\n",
    "    dynamics_ensemble=dynamics_ensemble,\n",
    "    horizon=4,\n",
    "    gamma=0.99,\n",
    "    beta=0,\n",
    "    kappa=10,\n",
    "    Nm=20,\n",
    "    L=0.5,\n",
    "    num_trajectories = 100,\n",
    "    device=device,\n",
    "    batch_size=256\n",
    "'''\n",
    "\n",
    "\n",
    "''' \n",
    "# Walker2D medium \n",
    "env_dataset = Environment(env_name=\"walker2d-medium-v2\", normalization_states=False, normalization_rewards=False)\n",
    "mopp_policy = MOPP(\n",
    "    env_dataset=env_dataset,\n",
    "    q_networks=q_networks,  # Unused in this implementation\n",
    "    behavior_ensemble=behavior_ensemble,\n",
    "    dynamics_ensemble=dynamics_ensemble,\n",
    "    horizon=2,\n",
    "    gamma=0.99,\n",
    "    beta=0,\n",
    "    kappa=0.1,\n",
    "    Nm=20,\n",
    "    L=7,\n",
    "    num_trajectories = 100,\n",
    "    device=device,\n",
    "    batch_size=256\n",
    ")\n",
    "''' \n",
    "\n",
    "''' \n",
    "# Half Cheetah medium \n",
    "env_dataset = Environment(env_name=\"halfcheetah-medium-v2\", normalization_states=False, normalization_rewards=False)\n",
    "mopp_policy = MOPP(\n",
    "    env_dataset=env_dataset,\n",
    "    q_networks=q_networks,  # Unused in this implementation\n",
    "    behavior_ensemble=behavior_ensemble,\n",
    "    dynamics_ensemble=dynamics_ensemble,\n",
    "    horizon=2,\n",
    "    gamma=0.99,\n",
    "    beta=0,\n",
    "    kappa=3,\n",
    "    Nm=20,\n",
    "    L=5,\n",
    "    num_trajectories = 100,\n",
    "    device=device,\n",
    "    batch_size=256\n",
    ")\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6501412,
     "sourceId": 10500660,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26996.439643,
   "end_time": "2025-01-18T04:58:48.941095",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-17T21:28:52.501452",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
